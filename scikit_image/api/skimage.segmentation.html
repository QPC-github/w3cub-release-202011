
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Segmentation - Scikit-image - W3cubDocs</title>
  
  <meta name="description" content=" Random walker algorithm for segmentation from markers. ">
  <meta name="keywords" content="module, segmentation, scikit-image, scikit_image">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/scikit_image/api/skimage.segmentation.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/scikit_image.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_image/" class="_nav-link" title="" style="margin-left:0;">scikit-image</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="module-segmentation">Module: segmentation</h1> <table class="longtable docutils" id="module-skimage.segmentation">   <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.random_walker" title="skimage.segmentation.random_walker"><code>skimage.segmentation.random_walker</code></a>(data, labels)</td> <td>Random walker algorithm for segmentation from markers.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.active_contour" title="skimage.segmentation.active_contour"><code>skimage.segmentation.active_contour</code></a>(image, snake)</td> <td>Active contour model.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.felzenszwalb" title="skimage.segmentation.felzenszwalb"><code>skimage.segmentation.felzenszwalb</code></a>(image[, …])</td> <td>Computes Felsenszwalb’s efficient graph based image segmentation.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.slic" title="skimage.segmentation.slic"><code>skimage.segmentation.slic</code></a>(image[, …])</td> <td>Segments image using k-means clustering in Color-(x,y,z) space.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.quickshift" title="skimage.segmentation.quickshift"><code>skimage.segmentation.quickshift</code></a>(image[, …])</td> <td>Segments image using quickshift clustering in Color-(x,y) space.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.find_boundaries" title="skimage.segmentation.find_boundaries"><code>skimage.segmentation.find_boundaries</code></a>(label_img)</td> <td>Return bool array where boundaries between labeled regions are True.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.mark_boundaries" title="skimage.segmentation.mark_boundaries"><code>skimage.segmentation.mark_boundaries</code></a>(image, …)</td> <td>Return image with boundaries between labeled regions highlighted.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.clear_border" title="skimage.segmentation.clear_border"><code>skimage.segmentation.clear_border</code></a>(labels[, …])</td> <td>Clear objects connected to the label image border.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.join_segmentations" title="skimage.segmentation.join_segmentations"><code>skimage.segmentation.join_segmentations</code></a>(s1, s2)</td> <td>Return the join of the two input segmentations.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.relabel_from_one" title="skimage.segmentation.relabel_from_one"><code>skimage.segmentation.relabel_from_one</code></a>(…)</td> <td>
<strong>Deprecated function</strong>.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.relabel_sequential" title="skimage.segmentation.relabel_sequential"><code>skimage.segmentation.relabel_sequential</code></a>(…)</td> <td>Relabel arbitrary labels to {<code>offset</code>, …</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.watershed" title="skimage.segmentation.watershed"><code>skimage.segmentation.watershed</code></a>(image, markers)</td> <td>Find watershed basins in <code>image</code> flooded from given <code>markers</code>.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.chan_vese" title="skimage.segmentation.chan_vese"><code>skimage.segmentation.chan_vese</code></a>(image[, mu, …])</td> <td>Chan-Vese segmentation algorithm.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.morphological_geodesic_active_contour" title="skimage.segmentation.morphological_geodesic_active_contour"><code>skimage.segmentation.morphological_geodesic_active_contour</code></a>(…)</td> <td>Morphological Geodesic Active Contours (MorphGAC).</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.morphological_chan_vese" title="skimage.segmentation.morphological_chan_vese"><code>skimage.segmentation.morphological_chan_vese</code></a>(…)</td> <td>Morphological Active Contours without Edges (MorphACWE)</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.inverse_gaussian_gradient" title="skimage.segmentation.inverse_gaussian_gradient"><code>skimage.segmentation.inverse_gaussian_gradient</code></a>(image)</td> <td>Inverse of gradient magnitude.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.circle_level_set" title="skimage.segmentation.circle_level_set"><code>skimage.segmentation.circle_level_set</code></a>(…[, …])</td> <td>Create a circle level set with binary values.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.segmentation.checkerboard_level_set" title="skimage.segmentation.checkerboard_level_set"><code>skimage.segmentation.checkerboard_level_set</code></a>(…)</td> <td>Create a checkerboard level set with binary values.</td> </tr>  </table>  <h2 id="random-walker">random_walker</h2> <dl class="function"> <dt id="skimage.segmentation.random_walker">
<code>skimage.segmentation.random_walker(data, labels, beta=130, mode='bf', tol=0.001, copy=True, multichannel=False, return_full_prob=False, spacing=None)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/random_walker_segmentation.py#L215"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Random walker algorithm for segmentation from markers.</p> <p>Random walker algorithm is implemented for gray-level or multichannel images.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>data : array_like</code> </dt> <dd>
<p class="first last">Image to be segmented in phases. Gray-level <code>data</code> can be two- or three-dimensional; multichannel data can be three- or four- dimensional (multichannel=True) with the highest dimension denoting channels. Data spacing is assumed isotropic unless the <code>spacing</code> keyword argument is used.</p> </dd> <dt>
<code>labels : array of ints, of same shape as data without channels dimension</code> </dt> <dd>
<p class="first last">Array of seed markers labeled with different positive integers for different phases. Zero-labeled pixels are unlabeled pixels. Negative labels correspond to inactive pixels that are not taken into account (they are removed from the graph). If labels are not consecutive integers, the labels array will be transformed so that labels are consecutive. In the multichannel case, <code>labels</code> should have the same shape as a single channel of <code>data</code>, i.e. without the final dimension denoting channels.</p> </dd> <dt>
<code>beta : float</code> </dt> <dd>
<p class="first last">Penalization coefficient for the random walker motion (the greater <code>beta</code>, the more difficult the diffusion).</p> </dd> <dt>
<code>mode : string, available options {‘cg_mg’, ‘cg’, ‘bf’}</code> </dt> <dd>
<p class="first">Mode for solving the linear system in the random walker algorithm. If no preference given, automatically attempt to use the fastest option available (‘cg_mg’ from pyamg &gt;&gt; ‘cg’ with UMFPACK &gt; ‘bf’).</p> <ul class="last simple"> <li>‘bf’ (brute force): an LU factorization of the Laplacian is computed. This is fast for small images (&lt;1024x1024), but very slow and memory-intensive for large images (e.g., 3-D volumes).</li> <li>‘cg’ (conjugate gradient): the linear system is solved iteratively using the Conjugate Gradient method from scipy.sparse.linalg. This is less memory-consuming than the brute force method for large images, but it is quite slow.</li> <li>‘cg_mg’ (conjugate gradient with multigrid preconditioner): a preconditioner is computed using a multigrid solver, then the solution is computed with the Conjugate Gradient method. This mode requires that the pyamg module (<a class="reference external" href="http://pyamg.org/">http://pyamg.org/</a>) is installed. For images of size &gt; 512x512, this is the recommended (fastest) mode.</li> </ul> </dd> <dt>
<code>tol : float</code> </dt> <dd>
<p class="first last">tolerance to achieve when solving the linear system, in cg’ and ‘cg_mg’ modes.</p> </dd> <dt>
<code>copy : bool</code> </dt> <dd>
<p class="first last">If copy is False, the <code>labels</code> array will be overwritten with the result of the segmentation. Use copy=False if you want to save on memory.</p> </dd> <dt>
<code>multichannel : bool, default False</code> </dt> <dd>
<p class="first last">If True, input data is parsed as multichannel data (see ‘data’ above for proper input format in this case)</p> </dd> <dt>
<code>return_full_prob : bool, default False</code> </dt> <dd>
<p class="first last">If True, the probability that a pixel belongs to each of the labels will be returned, instead of only the most likely label.</p> </dd> <dt>
<code>spacing : iterable of floats</code> </dt> <dd>
<p class="first last">Spacing between voxels in each spatial dimension. If <code>None</code>, then the spacing between pixels/voxels in each dimension is assumed 1.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>output : ndarray</code> </dt> <dd>
<ul class="first last simple"> <li>If <code>return_full_prob</code> is False, array of ints of same shape as <code>data</code>, in which each pixel has been labeled according to the marker that reached the pixel first by anisotropic diffusion.</li> <li>If <code>return_full_prob</code> is True, array of floats of shape <code>(nlabels, data.shape)</code>. <code>output[label_nb, i, j]</code> is the probability that label <code>label_nb</code> reaches the pixel <code>(i, j)</code> first.</li> </ul> </dd> </dl> </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <dl class="last docutils"> <dt>
 <a class="reference internal" href="skimage.morphology#skimage.morphology.watershed" title="skimage.morphology.watershed"><code>skimage.morphology.watershed</code></a>
</dt> <dd>watershed segmentation A segmentation algorithm based on mathematical morphology and “flooding” of regions from markers.</dd> </dl> </div> <h4 class="rubric">Notes</h4> <p>Multichannel inputs are scaled with all channel data combined. Ensure all channels are separately normalized prior to running this algorithm.</p> <p>The <code>spacing</code> argument is specifically for anisotropic datasets, where data points are spaced differently in one or more spatial dimensions. Anisotropic data is commonly encountered in medical imaging.</p> <p>The algorithm was first proposed in <em>Random walks for image segmentation</em>, Leo Grady, IEEE Trans Pattern Anal Mach Intell. 2006 Nov;28(11):1768-83.</p> <p>The algorithm solves the diffusion equation at infinite times for sources placed on markers of each phase in turn. A pixel is labeled with the phase that has the greatest probability to diffuse first to the pixel.</p> <p>The diffusion equation is solved by minimizing x.T L x for each phase, where L is the Laplacian of the weighted graph of the image, and x is the probability that a marker of the given phase arrives first at a pixel by diffusion (x=1 on markers of the phase, x=0 on the other markers, and the other coefficients are looked for). Each pixel is attributed the label for which it has a maximal value of x. The Laplacian L of the image is defined as:</p>  <ul class="simple"> <li>L_ii = d_i, the number of neighbors of pixel i (the degree of i)</li> <li>L_ij = -w_ij if i and j are adjacent pixels</li> </ul>  <p>The weight w_ij is a decreasing function of the norm of the local gradient. This ensures that diffusion is easier between pixels of similar values.</p> <p>When the Laplacian is decomposed into blocks of marked and unmarked pixels:</p> <pre data-language="python">L = M B.T
    B A
</pre> <p>with first indices corresponding to marked pixels, and then to unmarked pixels, minimizing x.T L x for one phase amount to solving:</p> <pre data-language="python">A x = - B x_m
</pre> <p>where x_m = 1 on markers of the given phase, and 0 on other markers. This linear system is solved in the algorithm using a direct method for small images, and an iterative method for larger images.</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; np.random.seed(0)
&gt;&gt;&gt; a = np.zeros((10, 10)) + 0.2 * np.random.rand(10, 10)
&gt;&gt;&gt; a[5:8, 5:8] += 1
&gt;&gt;&gt; b = np.zeros_like(a)
&gt;&gt;&gt; b[3, 3] = 1  # Marker for first phase
&gt;&gt;&gt; b[6, 6] = 2  # Marker for second phase
&gt;&gt;&gt; random_walker(a, b)
array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)
</pre> </dd>
</dl>   <h2 id="active-contour">active_contour</h2> <dl class="function"> <dt id="skimage.segmentation.active_contour">
<code>skimage.segmentation.active_contour(image, snake, alpha=0.01, beta=0.1, w_line=0, w_edge=1, gamma=0.01, bc='periodic', max_px_move=1.0, max_iterations=2500, convergence=0.1)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/active_contour_model.py#L8"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Active contour model.</p> <p>Active contours by fitting snakes to features of images. Supports single and multichannel 2D images. Snakes can be periodic (for segmentation) or have fixed and/or free ends. The output snake has the same length as the input boundary. As the number of points is constant, make sure that the initial snake has enough points to capture the details of the final contour.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (N, M) or (N, M, 3) ndarray</code> </dt> <dd>
<p class="first last">Input image.</p> </dd> <dt>
<code>snake : (N, 2) ndarray</code> </dt> <dd>
<p class="first last">Initialisation coordinates of snake. For periodic snakes, it should not include duplicate endpoints.</p> </dd> <dt>
<code>alpha : float, optional</code> </dt> <dd>
<p class="first last">Snake length shape parameter. Higher values makes snake contract faster.</p> </dd> <dt>
<code>beta : float, optional</code> </dt> <dd>
<p class="first last">Snake smoothness shape parameter. Higher values makes snake smoother.</p> </dd> <dt>
<code>w_line : float, optional</code> </dt> <dd>
<p class="first last">Controls attraction to brightness. Use negative values to attract to dark regions.</p> </dd> <dt>
<code>w_edge : float, optional</code> </dt> <dd>
<p class="first last">Controls attraction to edges. Use negative values to repel snake from edges.</p> </dd> <dt>
<code>gamma : float, optional</code> </dt> <dd>
<p class="first last">Explicit time stepping parameter.</p> </dd> <dt>
<code>bc : {‘periodic’, ‘free’, ‘fixed’}, optional</code> </dt> <dd>
<p class="first last">Boundary conditions for worm. ‘periodic’ attaches the two ends of the snake, ‘fixed’ holds the end-points in place, and’free’ allows free movement of the ends. ‘fixed’ and ‘free’ can be combined by parsing ‘fixed-free’, ‘free-fixed’. Parsing ‘fixed-fixed’ or ‘free-free’ yields same behaviour as ‘fixed’ and ‘free’, respectively.</p> </dd> <dt>
<code>max_px_move : float, optional</code> </dt> <dd>
<p class="first last">Maximum pixel distance to move per iteration.</p> </dd> <dt>
<code>max_iterations : int, optional</code> </dt> <dd>
<p class="first last">Maximum iterations to optimize snake shape.</p> </dd> <dt><strong>convergence: float, optional</strong></dt> <dd>
<p class="first last">Convergence criteria.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>snake : (N, 2) ndarray</code> </dt> <dd>
<p class="first last">Optimised snake, same shape as input parameter.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r094916d7e45d-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>Kass, M.; Witkin, A.; Terzopoulos, D. “Snakes: Active contour models”. International Journal of Computer Vision 1 (4): 321 (1988).</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.draw import circle_perimeter
&gt;&gt;&gt; from skimage.filters import gaussian
</pre> <p>Create and smooth image:</p> <pre data-language="python">&gt;&gt;&gt; img = np.zeros((100, 100))
&gt;&gt;&gt; rr, cc = circle_perimeter(35, 45, 25)
&gt;&gt;&gt; img[rr, cc] = 1
&gt;&gt;&gt; img = gaussian(img, 2)
</pre> <p>Initiliaze spline:</p> <pre data-language="python">&gt;&gt;&gt; s = np.linspace(0, 2*np.pi,100)
&gt;&gt;&gt; init = 50*np.array([np.cos(s), np.sin(s)]).T+50
</pre> <p>Fit spline to image:</p> <pre data-language="python">&gt;&gt;&gt; snake = active_contour(img, init, w_edge=0, w_line=1) #doctest: +SKIP
&gt;&gt;&gt; dist = np.sqrt((45-snake[:, 0])**2 +(35-snake[:, 1])**2) #doctest: +SKIP
&gt;&gt;&gt; int(np.mean(dist)) #doctest: +SKIP
25
</pre> </dd>
</dl>   <h2 id="felzenszwalb">felzenszwalb</h2> <dl class="function"> <dt id="skimage.segmentation.felzenszwalb">
<code>skimage.segmentation.felzenszwalb(image, scale=1, sigma=0.8, min_size=20, multichannel=True)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/_felzenszwalb.py#L6"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes Felsenszwalb’s efficient graph based image segmentation.</p> <p>Produces an oversegmentation of a multichannel (i.e. RGB) image using a fast, minimum spanning tree based clustering on the image grid. The parameter <code>scale</code> sets an observation level. Higher scale means less and larger segments. <code>sigma</code> is the diameter of a Gaussian kernel, used for smoothing the image prior to segmentation.</p> <p>The number of produced segments as well as their size can only be controlled indirectly through <code>scale</code>. Segment size within an image can vary greatly depending on local contrast.</p> <p>For RGB images, the algorithm uses the euclidean distance between pixels in color space.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (width, height, 3) or (width, height) ndarray</code> </dt> <dd>
<p class="first last">Input image.</p> </dd> <dt>
<code>scale : float</code> </dt> <dd>
<p class="first last">Free parameter. Higher means larger clusters.</p> </dd> <dt>
<code>sigma : float</code> </dt> <dd>
<p class="first last">Width of Gaussian kernel used in preprocessing.</p> </dd> <dt>
<code>min_size : int</code> </dt> <dd>
<p class="first last">Minimum component size. Enforced using postprocessing.</p> </dd> <dt>
<code>multichannel : bool, optional (default: True)</code> </dt> <dd>
<p class="first last">Whether the last axis of the image is to be interpreted as multiple channels. A value of False, for a 3D image, is not currently supported.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>segment_mask : (width, height) ndarray</code> </dt> <dd>
<p class="first last">Integer mask indicating segment labels.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r89864356f083-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id2">[1]</a></td>
<td>Efficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.segmentation import felzenszwalb
&gt;&gt;&gt; from skimage.data import coffee
&gt;&gt;&gt; img = coffee()
&gt;&gt;&gt; segments = felzenszwalb(img, scale=3.0, sigma=0.95, min_size=5)
</pre> </dd>
</dl>   <h2 id="slic">slic</h2> <dl class="function"> <dt id="skimage.segmentation.slic">
<code>skimage.segmentation.slic(image, n_segments=100, compactness=10.0, max_iter=10, sigma=0, spacing=None, multichannel=True, convert2lab=None, enforce_connectivity=True, min_size_factor=0.5, max_size_factor=3, slic_zero=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/slic_superpixels.py#L14"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Segments image using k-means clustering in Color-(x,y,z) space.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : 2D, 3D or 4D ndarray</code> </dt> <dd>
<p class="first last">Input image, which can be 2D or 3D, and grayscale or multichannel (see <code>multichannel</code> parameter).</p> </dd> <dt>
<code>n_segments : int, optional</code> </dt> <dd>
<p class="first last">The (approximate) number of labels in the segmented output image.</p> </dd> <dt>
<code>compactness : float, optional</code> </dt> <dd>
<p class="first last">Balances color proximity and space proximity. Higher values give more weight to space proximity, making superpixel shapes more square/cubic. In SLICO mode, this is the initial compactness. This parameter depends strongly on image contrast and on the shapes of objects in the image. We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10, 100, before refining around a chosen value.</p> </dd> <dt>
<code>max_iter : int, optional</code> </dt> <dd>
<p class="first last">Maximum number of iterations of k-means.</p> </dd> <dt>
<code>sigma : float or (3,) array-like of floats, optional</code> </dt> <dd>
<p class="first last">Width of Gaussian smoothing kernel for pre-processing for each dimension of the image. The same sigma is applied to each dimension in case of a scalar value. Zero means no smoothing. Note, that <code>sigma</code> is automatically scaled if it is scalar and a manual voxel spacing is provided (see Notes section).</p> </dd> <dt>
<code>spacing : (3,) array-like of floats, optional</code> </dt> <dd>
<p class="first last">The voxel spacing along each image dimension. By default, <code>slic</code> assumes uniform spacing (same voxel resolution along z, y and x). This parameter controls the weights of the distances along z, y, and x during k-means clustering.</p> </dd> <dt>
<code>multichannel : bool, optional</code> </dt> <dd>
<p class="first last">Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.</p> </dd> <dt>
<code>convert2lab : bool, optional</code> </dt> <dd>
<p class="first last">Whether the input should be converted to Lab colorspace prior to segmentation. The input image <em>must</em> be RGB. Highly recommended. This option defaults to <code>True</code> when <code>multichannel=True</code> <em>and</em> <code>image.shape[-1] == 3</code>.</p> </dd> <dt><strong>enforce_connectivity: bool, optional</strong></dt> <dd>
<p class="first last">Whether the generated segments are connected or not</p> </dd> <dt><strong>min_size_factor: float, optional</strong></dt> <dd>
<p class="first last">Proportion of the minimum segment size to be removed with respect to the supposed segment size <code>`depth*width*height/n_segments`</code></p> </dd> <dt><strong>max_size_factor: float, optional</strong></dt> <dd>
<p class="first last">Proportion of the maximum connected segment size. A value of 3 works in most of the cases.</p> </dd> <dt><strong>slic_zero: bool, optional</strong></dt> <dd>
<p class="first last">Run SLIC-zero, the zero-parameter mode of SLIC. <a class="reference internal" href="#rbeb231216055-2" id="id3">[2]</a></p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>labels : 2D or 3D array</code> </dt> <dd>
<p class="first last">Integer mask indicating segment labels.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Raises:</th>
<td class="field-body">
<dl class="first last docutils"> <dt><strong>ValueError</strong></dt> <dd>
<p class="first last">If <code>convert2lab</code> is set to <code>True</code> but the last array dimension is not of length 3.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <ul class="simple"> <li>If <code>sigma &gt; 0</code>, the image is smoothed using a Gaussian kernel prior to segmentation.</li> <li>If <code>sigma</code> is scalar and <code>spacing</code> is provided, the kernel width is divided along each dimension by the spacing. For example, if <code>sigma=1</code> and <code>spacing=[5, 1, 1]</code>, the effective <code>sigma</code> is <code>[0.2, 1, 1]</code>. This ensures sensible smoothing for anisotropic images.</li> <li>The image is rescaled to be in [0, 1] prior to processing.</li> <li>Images of shape (M, N, 3) are interpreted as 2D RGB images by default. To interpret them as 3D with the last dimension having length 3, use <code>multichannel=False</code>.</li> </ul> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rbeb231216055-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id4">[1]</a></td>
<td>Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel Methods, TPAMI, May 2012.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rbeb231216055-2" rules="none">   <tr>
<td class="label">[2]</td>
<td>
<em>(<a class="fn-backref" href="#id3">1</a>, <a class="fn-backref" href="#id5">2</a>)</em> <a class="reference external" href="http://ivrg.epfl.ch/research/superpixels#SLICO">http://ivrg.epfl.ch/research/superpixels#SLICO</a>
</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.segmentation import slic
&gt;&gt;&gt; from skimage.data import astronaut
&gt;&gt;&gt; img = astronaut()
&gt;&gt;&gt; segments = slic(img, n_segments=100, compactness=10)
</pre> <p>Increasing the compactness parameter yields more square regions:</p> <pre data-language="python">&gt;&gt;&gt; segments = slic(img, n_segments=100, compactness=20)
</pre> </dd>
</dl>   <h2 id="quickshift">quickshift</h2> <dl class="function"> <dt id="skimage.segmentation.quickshift">
<code>skimage.segmentation.quickshift(image, ratio=1.0, kernel_size=5, max_dist=10, return_tree=False, sigma=0, convert2lab=True, random_seed=42)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/_quickshift.py#L11"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Segments image using quickshift clustering in Color-(x,y) space.</p> <p>Produces an oversegmentation of the image using the quickshift mode-seeking algorithm.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (width, height, channels) ndarray</code> </dt> <dd>
<p class="first last">Input image.</p> </dd> <dt>
<code>ratio : float, optional, between 0 and 1</code> </dt> <dd>
<p class="first last">Balances color-space proximity and image-space proximity. Higher values give more weight to color-space.</p> </dd> <dt>
<code>kernel_size : float, optional</code> </dt> <dd>
<p class="first last">Width of Gaussian kernel used in smoothing the sample density. Higher means fewer clusters.</p> </dd> <dt>
<code>max_dist : float, optional</code> </dt> <dd>
<p class="first last">Cut-off point for data distances. Higher means fewer clusters.</p> </dd> <dt>
<code>return_tree : bool, optional</code> </dt> <dd>
<p class="first last">Whether to return the full segmentation hierarchy tree and distances.</p> </dd> <dt>
<code>sigma : float, optional</code> </dt> <dd>
<p class="first last">Width for Gaussian smoothing as preprocessing. Zero means no smoothing.</p> </dd> <dt>
<code>convert2lab : bool, optional</code> </dt> <dd>
<p class="first last">Whether the input should be converted to Lab colorspace prior to segmentation. For this purpose, the input is assumed to be RGB.</p> </dd> <dt>
<code>random_seed : int, optional</code> </dt> <dd>
<p class="first last">Random seed used for breaking ties.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>segment_mask : (width, height) ndarray</code> </dt> <dd>
<p class="first last">Integer mask indicating segment labels.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The authors advocate to convert the image to Lab color space prior to segmentation, though this is not strictly necessary. For this to work, the image must be given in RGB format.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r7604e01a2c85-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id6">[1]</a></td>
<td>Quick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008</td>
</tr>  </table> </dd>
</dl>   <h2 id="find-boundaries">find_boundaries</h2> <dl class="function"> <dt id="skimage.segmentation.find_boundaries">
<code>skimage.segmentation.find_boundaries(label_img, connectivity=1, mode='thick', background=0)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/boundaries.py#L49"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return bool array where boundaries between labeled regions are True.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>label_img : array of int or bool</code> </dt> <dd>
<p class="first last">An array in which different regions are labeled with either different integers or boolean values.</p> </dd> <dt><strong>connectivity: int in {1, …, `label_img.ndim`}, optional</strong></dt> <dd>
<p class="first last">A pixel is considered a boundary pixel if any of its neighbors has a different label. <code>connectivity</code> controls which pixels are considered neighbors. A connectivity of 1 (default) means pixels sharing an edge (in 2D) or a face (in 3D) will be considered neighbors. A connectivity of <code>label_img.ndim</code> means pixels sharing a corner will be considered neighbors.</p> </dd> <dt><strong>mode: string in {‘thick’, ‘inner’, ‘outer’, ‘subpixel’}</strong></dt> <dd>
<p class="first">How to mark the boundaries:</p> <ul class="last simple"> <li>thick: any pixel not completely surrounded by pixels of the same label (defined by <code>connectivity</code>) is marked as a boundary. This results in boundaries that are 2 pixels thick.</li> <li>inner: outline the pixels <em>just inside</em> of objects, leaving background pixels untouched.</li> <li>outer: outline pixels in the background around object boundaries. When two objects touch, their boundary is also marked.</li> <li>subpixel: return a doubled image, with pixels <em>between</em> the original pixels marked as boundary where appropriate.</li> </ul> </dd> <dt><strong>background: int, optional</strong></dt> <dd>
<p class="first last">For modes ‘inner’ and ‘outer’, a definition of a background label is required. See <code>mode</code> for descriptions of these two.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>boundaries : array of bool, same shape as label_img</code> </dt> <dd>
<p class="first last">A bool image where <code>True</code> represents a boundary pixel. For <code>mode</code> equal to ‘subpixel’, <code>boundaries.shape[i]</code> is equal to <code>2 * label_img.shape[i] - 1</code> for all <code>i</code> (a pixel is inserted in between all other pairs of pixels).</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; labels = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
...                    [0, 0, 0, 0, 0, 5, 5, 5, 0, 0],
...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],
...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],
...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],
...                    [0, 0, 0, 0, 0, 5, 5, 5, 0, 0],
...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=np.uint8)
&gt;&gt;&gt; find_boundaries(labels, mode='thick').astype(np.uint8)
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],
       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],
       [0, 1, 1, 0, 1, 1, 0, 1, 1, 0],
       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],
       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; find_boundaries(labels, mode='inner').astype(np.uint8)
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],
       [0, 0, 1, 0, 1, 1, 0, 1, 0, 0],
       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; find_boundaries(labels, mode='outer').astype(np.uint8)
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],
       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],
       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],
       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],
       [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; labels_small = labels[::2, ::3]
&gt;&gt;&gt; labels_small
array([[0, 0, 0, 0],
       [0, 0, 5, 0],
       [0, 1, 5, 0],
       [0, 0, 5, 0],
       [0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; find_boundaries(labels_small, mode='subpixel').astype(np.uint8)
array([[0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 1, 1, 1, 0],
       [0, 0, 0, 1, 0, 1, 0],
       [0, 1, 1, 1, 0, 1, 0],
       [0, 1, 0, 1, 0, 1, 0],
       [0, 1, 1, 1, 0, 1, 0],
       [0, 0, 0, 1, 0, 1, 0],
       [0, 0, 0, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; bool_image = np.array([[False, False, False, False, False],
...                        [False, False, False, False, False],
...                        [False, False,  True,  True,  True],
...                        [False, False,  True,  True,  True],
...                        [False, False,  True,  True,  True]], dtype=np.bool)
&gt;&gt;&gt; find_boundaries(bool_image)
array([[False, False, False, False, False],
       [False, False,  True,  True,  True],
       [False,  True,  True,  True,  True],
       [False,  True,  True, False, False],
       [False,  True,  True, False, False]], dtype=bool)
</pre> </dd>
</dl>   <h2 id="mark-boundaries">mark_boundaries</h2> <dl class="function"> <dt id="skimage.segmentation.mark_boundaries">
<code>skimage.segmentation.mark_boundaries(image, label_img, color=(1, 1, 0), outline_color=None, mode='outer', background_label=0)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/boundaries.py#L184"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return image with boundaries between labeled regions highlighted.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (M, N[, 3]) array</code> </dt> <dd>
<p class="first last">Grayscale or RGB image.</p> </dd> <dt>
<code>label_img : (M, N) array of int</code> </dt> <dd>
<p class="first last">Label array where regions are marked by different integer values.</p> </dd> <dt>
<code>color : length-3 sequence, optional</code> </dt> <dd>
<p class="first last">RGB color of boundaries in the output image.</p> </dd> <dt>
<code>outline_color : length-3 sequence, optional</code> </dt> <dd>
<p class="first last">RGB color surrounding boundaries in the output image. If None, no outline is drawn.</p> </dd> <dt>
<code>mode : string in {‘thick’, ‘inner’, ‘outer’, ‘subpixel’}, optional</code> </dt> <dd>
<p class="first last">The mode for finding boundaries.</p> </dd> <dt>
<code>background_label : int, optional</code> </dt> <dd>
<p class="first last">Which label to consider background (this is only useful for modes <code>inner</code> and <code>outer</code>).</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>marked : (M, N, 3) array of float</code> </dt> <dd>
<p class="first last">An image in which the boundaries between labels are superimposed on the original image.</p> </dd> </dl> </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="#skimage.segmentation.find_boundaries" title="skimage.segmentation.find_boundaries"><code>find_boundaries</code></a></p> </div> </dd>
</dl>   <h2 id="clear-border">clear_border</h2> <dl class="function"> <dt id="skimage.segmentation.clear_border">
<code>skimage.segmentation.clear_border(labels, buffer_size=0, bgval=0, in_place=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/_clear_border.py#L5"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Clear objects connected to the label image border.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>labels : (M[, N[, …, P]]) array of int or bool</code> </dt> <dd>
<p class="first last">Imaging data labels.</p> </dd> <dt>
<code>buffer_size : int, optional</code> </dt> <dd>
<p class="first last">The width of the border examined. By default, only objects that touch the outside of the image are removed.</p> </dd> <dt>
<code>bgval : float or int, optional</code> </dt> <dd>
<p class="first last">Cleared objects are set to this value.</p> </dd> <dt>
<code>in_place : bool, optional</code> </dt> <dd>
<p class="first last">Whether or not to manipulate the labels array in-place.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>out : (M[, N[, …, P]]) array</code> </dt> <dd>
<p class="first last">Imaging data labels with cleared borders</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from skimage.segmentation import clear_border
&gt;&gt;&gt; labels = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0],
...                    [0, 0, 0, 0, 1, 0, 0, 0, 0],
...                    [1, 0, 0, 1, 0, 1, 0, 0, 0],
...                    [0, 0, 1, 1, 1, 1, 1, 0, 0],
...                    [0, 1, 1, 1, 1, 1, 1, 1, 0],
...                    [0, 0, 0, 0, 0, 0, 0, 0, 0]])
&gt;&gt;&gt; clear_border(labels)
array([[0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0],
       [0, 0, 0, 1, 0, 1, 0, 0, 0],
       [0, 0, 1, 1, 1, 1, 1, 0, 0],
       [0, 1, 1, 1, 1, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0]])
</pre> </dd>
</dl>   <h2 id="join-segmentations">join_segmentations</h2> <dl class="function"> <dt id="skimage.segmentation.join_segmentations">
<code>skimage.segmentation.join_segmentations(s1, s2)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/_join.py#L5"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the join of the two input segmentations.</p> <p>The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in <em>both</em> S1 and S2.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>s1, s2 : numpy arrays</code> </dt> <dd>
<p class="first last">s1 and s2 are label fields of the same shape.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>j : numpy array</code> </dt> <dd>
<p class="first last">The join segmentation of s1 and s2.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.segmentation import join_segmentations
&gt;&gt;&gt; s1 = np.array([[0, 0, 1, 1],
...                [0, 2, 1, 1],
...                [2, 2, 2, 1]])
&gt;&gt;&gt; s2 = np.array([[0, 1, 1, 0],
...                [0, 1, 1, 0],
...                [0, 1, 1, 1]])
&gt;&gt;&gt; join_segmentations(s1, s2)
array([[0, 1, 3, 2],
       [0, 5, 3, 2],
       [4, 5, 5, 3]])
</pre> </dd>
</dl>   <h2 id="relabel-from-one">relabel_from_one</h2> <dl class="function"> <dt id="skimage.segmentation.relabel_from_one">
<code>skimage.segmentation.relabel_from_one(label_field)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/_shared/utils.py#L59"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p><strong>Deprecated function</strong>. Use <code>relabel_sequential</code> instead.</p> <p>Convert labels in an arbitrary label field to {1, … number_of_labels}.</p> <p>This function is deprecated, see <code>relabel_sequential</code> for more.</p> </dd>
</dl>   <h2 id="relabel-sequential">relabel_sequential</h2> <dl class="function"> <dt id="skimage.segmentation.relabel_sequential">
<code>skimage.segmentation.relabel_sequential(label_field, offset=1)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/_join.py#L55"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Relabel arbitrary labels to {<code>offset</code>, … <code>offset</code> + number_of_labels}.</p> <p>This function also returns the forward map (mapping the original labels to the reduced labels) and the inverse map (mapping the reduced labels back to the original ones).</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>label_field : numpy array of int, arbitrary shape</code> </dt> <dd>
<p class="first last">An array of labels.</p> </dd> <dt>
<code>offset : int, optional</code> </dt> <dd>
<p class="first last">The return labels will start at <code>offset</code>, which should be strictly positive.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>relabeled : numpy array of int, same shape as label_field</code> </dt> <dd>
<p class="first last">The input label field with labels mapped to {offset, …, number_of_labels + offset - 1}.</p> </dd> <dt>
<code>forward_map : numpy array of int, shape (label_field.max() + 1,)</code> </dt> <dd>
<p class="first last">The map from the original label space to the returned label space. Can be used to re-apply the same mapping. See examples for usage.</p> </dd> <dt>
<code>inverse_map : 1D numpy array of int, of length offset + number of labels</code> </dt> <dd>
<p class="first last">The map from the new label space to the original space. This can be used to reconstruct the original label field from the relabeled one.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The label 0 is assumed to denote the background and is never remapped.</p> <p>The forward map can be extremely big for some inputs, since its length is given by the maximum of the label field. However, in most situations, <code>label_field.max()</code> is much smaller than <code>label_field.size</code>, and in these cases the forward map is guaranteed to be smaller than either the input or output images.</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.segmentation import relabel_sequential
&gt;&gt;&gt; label_field = np.array([1, 1, 5, 5, 8, 99, 42])
&gt;&gt;&gt; relab, fw, inv = relabel_sequential(label_field)
&gt;&gt;&gt; relab
array([1, 1, 2, 2, 3, 5, 4])
&gt;&gt;&gt; fw
array([0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 5])
&gt;&gt;&gt; inv
array([ 0,  1,  5,  8, 42, 99])
&gt;&gt;&gt; (fw[label_field] == relab).all()
True
&gt;&gt;&gt; (inv[relab] == label_field).all()
True
&gt;&gt;&gt; relab, fw, inv = relabel_sequential(label_field, offset=5)
&gt;&gt;&gt; relab
array([5, 5, 6, 6, 7, 9, 8])
</pre> </dd>
</dl>   <h2 id="watershed">watershed</h2> <dl class="function"> <dt id="skimage.segmentation.watershed">
<code>skimage.segmentation.watershed(image, markers, connectivity=1, offset=None, mask=None, compactness=0, watershed_line=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/morphology/watershed.py#L138"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Find watershed basins in <code>image</code> flooded from given <code>markers</code>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt><strong>image: ndarray (2-D, 3-D, …) of integers</strong></dt> <dd>
<p class="first last">Data array where the lowest value points are labeled first.</p> </dd> <dt><strong>markers: int, or ndarray of int, same shape as `image`</strong></dt> <dd>
<p class="first last">The desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker.</p> </dd> <dt><strong>connectivity: ndarray, optional</strong></dt> <dd>
<p class="first last">An array with the same number of dimensions as <code>image</code> whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.</p> </dd> <dt><strong>offset: array_like of shape image.ndim, optional</strong></dt> <dd>
<p class="first last">offset of the connectivity (one offset per dimension)</p> </dd> <dt><strong>mask: ndarray of bools or 0s and 1s, optional</strong></dt> <dd>
<p class="first last">Array of same shape as <code>image</code>. Only points at which mask == True will be labeled.</p> </dd> <dt>
<code>compactness : float, optional</code> </dt> <dd>
<p class="first last">Use compact watershed <a class="reference internal" href="#rc8002e235889-3" id="id7">[3]</a> with given compactness parameter. Higher values result in more regularly-shaped watershed basins.</p> </dd> <dt>
<code>watershed_line : bool, optional</code> </dt> <dd>
<p class="first last">If watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt><strong>out: ndarray</strong></dt> <dd>
<p class="first last">A labeled matrix of the same type and shape as markers</p> </dd> </dl> </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <dl class="last docutils"> <dt>
 <a class="reference internal" href="#skimage.segmentation.random_walker" title="skimage.segmentation.random_walker"><code>skimage.segmentation.random_walker</code></a>
</dt> <dd>random walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.</dd> </dl> </div> <h4 class="rubric">Notes</h4> <p>This function implements a watershed algorithm <a class="reference internal" href="#rc8002e235889-1" id="id8">[1]</a> <a class="reference internal" href="#rc8002e235889-2" id="id9">[2]</a> that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker.</p> <p>Some ideas taken from Soille, “Automated Basin Delineation from Digital Elevation Models Using Mathematical Morphology”, Signal Processing 20 (1990) 171-182</p> <p>The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides.</p> <p>This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm.</p> <p>Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example).</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rc8002e235889-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id8">1</a>, <a class="fn-backref" href="#id10">2</a>)</em> <a class="reference external" href="http://en.wikipedia.org/wiki/Watershed_%28image_processing%29">http://en.wikipedia.org/wiki/Watershed_%28image_processing%29</a>
</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rc8002e235889-2" rules="none">   <tr>
<td class="label">[2]</td>
<td>
<em>(<a class="fn-backref" href="#id9">1</a>, <a class="fn-backref" href="#id11">2</a>)</em> <a class="reference external" href="http://cmm.ensmp.fr/~beucher/wtshed.html">http://cmm.ensmp.fr/~beucher/wtshed.html</a>
</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rc8002e235889-3" rules="none">   <tr>
<td class="label">[3]</td>
<td>
<em>(<a class="fn-backref" href="#id7">1</a>, <a class="fn-backref" href="#id12">2</a>)</em> Peer Neubert &amp; Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/forschung/rsrc/cws_pSLIC_ICPR.pdf</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <p>The watershed algorithm is useful to separate overlapping objects.</p> <p>We first generate an initial image with two overlapping circles:</p> <pre data-language="python">&gt;&gt;&gt; x, y = np.indices((80, 80))
&gt;&gt;&gt; x1, y1, x2, y2 = 28, 28, 44, 52
&gt;&gt;&gt; r1, r2 = 16, 20
&gt;&gt;&gt; mask_circle1 = (x - x1)**2 + (y - y1)**2 &lt; r1**2
&gt;&gt;&gt; mask_circle2 = (x - x2)**2 + (y - y2)**2 &lt; r2**2
&gt;&gt;&gt; image = np.logical_or(mask_circle1, mask_circle2)
</pre> <p>Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background:</p> <pre data-language="python">&gt;&gt;&gt; from scipy import ndimage as ndi
&gt;&gt;&gt; distance = ndi.distance_transform_edt(image)
&gt;&gt;&gt; from skimage.feature import peak_local_max
&gt;&gt;&gt; local_maxi = peak_local_max(distance, labels=image,
...                             footprint=np.ones((3, 3)),
...                             indices=False)
&gt;&gt;&gt; markers = ndi.label(local_maxi)[0]
</pre> <p>Finally, we run the watershed on the image and markers:</p> <pre data-language="python">&gt;&gt;&gt; labels = watershed(-distance, markers, mask=image)
</pre> <p>The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres.</p> </dd>
</dl>   <h2 id="chan-vese">chan_vese</h2> <dl class="function"> <dt id="skimage.segmentation.chan_vese">
<code>skimage.segmentation.chan_vese(image, mu=0.25, lambda1=1.0, lambda2=1.0, tol=0.001, max_iter=500, dt=0.5, init_level_set='checkerboard', extended_output=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/_chan_vese.py#L170"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Chan-Vese segmentation algorithm.</p> <p>Active contour model by evolving a level set. Can be used to segment objects without clearly defined boundaries.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (M, N) ndarray</code> </dt> <dd>
<p class="first last">Grayscale image to be segmented.</p> </dd> <dt>
<code>mu : float, optional</code> </dt> <dd>
<p class="first last">‘edge length’ weight parameter. Higher <code>mu</code> values will produce a ‘round’ edge, while values closer to zero will detect smaller objects.</p> </dd> <dt>
<code>lambda1 : float, optional</code> </dt> <dd>
<p class="first last">‘difference from average’ weight parameter for the output region with value ‘True’. If it is lower than <code>lambda2</code>, this region will have a larger range of values than the other.</p> </dd> <dt>
<code>lambda2 : float, optional</code> </dt> <dd>
<p class="first last">‘difference from average’ weight parameter for the output region with value ‘False’. If it is lower than <code>lambda1</code>, this region will have a larger range of values than the other.</p> </dd> <dt>
<code>tol : float, positive, optional</code> </dt> <dd>
<p class="first last">Level set variation tolerance between iterations. If the L2 norm difference between the level sets of successive iterations normalized by the area of the image is below this value, the algorithm will assume that the solution was reached.</p> </dd> <dt>
<code>max_iter : uint, optional</code> </dt> <dd>
<p class="first last">Maximum number of iterations allowed before the algorithm interrupts itself.</p> </dd> <dt>
<code>dt : float, optional</code> </dt> <dd>
<p class="first last">A multiplication factor applied at calculations for each step, serves to accelerate the algorithm. While higher values may speed up the algorithm, they may also lead to convergence problems.</p> </dd> <dt>
<code>init_level_set : str or (M, N) ndarray, optional</code> </dt> <dd>
<p class="first">Defines the starting level set used by the algorithm. If a string is inputted, a level set that matches the image size will automatically be generated. Alternatively, it is possible to define a custom level set, which should be an array of float values, with the same shape as ‘image’. Accepted string values are as follows.</p> <dl class="last docutils"> <dt>‘checkerboard’</dt> <dd>
<p class="first last">the starting level set is defined as sin(x/5*pi)*sin(y/5*pi), where x and y are pixel coordinates. This level set has fast convergence, but may fail to detect implicit edges.</p> </dd> <dt>‘disk’</dt> <dd>
<p class="first last">the starting level set is defined as the opposite of the distance from the center of the image minus half of the minimum value between image width and image height. This is somewhat slower, but is more likely to properly detect implicit edges.</p> </dd> <dt>‘small disk’</dt> <dd>
<p class="first last">the starting level set is defined as the opposite of the distance from the center of the image minus a quarter of the minimum value between image width and image height.</p> </dd> </dl> </dd> <dt>
<code>extended_output : bool, optional</code> </dt> <dd>
<p class="first last">If set to True, the return value will be a tuple containing the three return values (see below). If set to False which is the default value, only the ‘segmentation’ array will be returned.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>segmentation : (M, N) ndarray, bool</code> </dt> <dd>
<p class="first last">Segmentation produced by the algorithm.</p> </dd> <dt>
<code>phi : (M, N) ndarray of floats</code> </dt> <dd>
<p class="first last">Final level set computed by the algorithm.</p> </dd> <dt>
<code>energies : list of floats</code> </dt> <dd>
<p class="first last">Shows the evolution of the ‘energy’ for each step of the algorithm. This should allow to check whether the algorithm converged.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The Chan-Vese Algorithm is designed to segment objects without clearly defined boundaries. This algorithm is based on level sets that are evolved iteratively to minimize an energy, which is defined by weighted values corresponding to the sum of differences intensity from the average value outside the segmented region, the sum of differences from the average value inside the segmented region, and a term which is dependent on the length of the boundary of the segmented region.</p> <p>This algorithm was first proposed by Tony Chan and Luminita Vese, in a publication entitled “An Active Countour Model Without Edges” <a class="reference internal" href="#rb5da2c114fc8-1" id="id13">[1]</a>.</p> <p>This implementation of the algorithm is somewhat simplified in the sense that the area factor ‘nu’ described in the original paper is not implemented, and is only suitable for grayscale images.</p> <p>Typical values for <code>lambda1</code> and <code>lambda2</code> are 1. If the ‘background’ is very different from the segmented object in terms of distribution (for example, a uniform black image with figures of varying intensity), then these values should be different from each other.</p> <p>Typical values for mu are between 0 and 1, though higher values can be used when dealing with shapes with very ill-defined contours.</p> <p>The ‘energy’ which this algorithm tries to minimize is defined as the sum of the differences from the average within the region squared and weighed by the ‘lambda’ factors to which is added the length of the contour multiplied by the ‘mu’ factor.</p> <p>Supports 2D grayscale images only, and does not implement the area term described in the original article.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rb5da2c114fc8-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id13">1</a>, <a class="fn-backref" href="#id14">2</a>)</em> An Active Contour Model without Edges, Tony Chan and Luminita Vese, Scale-Space Theories in Computer Vision, 1999, DOI:10.1007/3-540-48236-9_13</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rb5da2c114fc8-2" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id15">[2]</a></td>
<td>Chan-Vese Segmentation, Pascal Getreuer Image Processing On Line, 2 (2012), pp. 214-224, DOI:10.5201/ipol.2012.g-cv</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rb5da2c114fc8-3" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id16">[3]</a></td>
<td>The Chan-Vese Algorithm - Project Report, Rami Cohen, <a class="reference external" href="http://arxiv.org/abs/1107.2782">http://arxiv.org/abs/1107.2782</a>, 2011</td>
</tr>  </table> </dd>
</dl>   <h2 id="morphological-geodesic-active-contour">morphological_geodesic_active_contour</h2> <dl class="function"> <dt id="skimage.segmentation.morphological_geodesic_active_contour">
<code>skimage.segmentation.morphological_geodesic_active_contour(gimage, iterations, init_level_set='circle', smoothing=1, threshold='auto', balloon=0, iter_callback=&lt;function &lt;lambda&gt;&gt;)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/morphsnakes.py#L318"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Morphological Geodesic Active Contours (MorphGAC).</p> <p>Geodesic active contours implemented with morphological operators. It can be used to segment objects with visible but noisy, cluttered, broken borders.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>gimage : (M, N) or (L, M, N) array</code> </dt> <dd>
<p class="first last">Preprocessed image or volume to be segmented. This is very rarely the original image. Instead, this is usually a preprocessed version of the original image that enhances and highlights the borders (or other structures) of the object to segment. <code>morphological_geodesic_active_contour</code> will try to stop the contour evolution in areas where <code>gimage</code> is small. See <code>morphsnakes.inverse_gaussian_gradient</code> as an example function to perform this preprocessing. Note that the quality of <code>morphological_geodesic_active_contour</code> might greatly depend on this preprocessing.</p> </dd> <dt>
<code>iterations : uint</code> </dt> <dd>
<p class="first last">Number of iterations to run.</p> </dd> <dt>
<code>init_level_set : str, (M, N) array, or (L, M, N) array</code> </dt> <dd>
<p class="first last">Initial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the <code>image</code>. Accepted values are ‘checkerboard’ and ‘circle’. See the documentation of <code>checkerboard_level_set</code> and <code>circle_level_set</code> respectively for details about how these level sets are created.</p> </dd> <dt>
<code>smoothing : uint, optional</code> </dt> <dd>
<p class="first last">Number of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.</p> </dd> <dt>
<code>threshold : float, optional</code> </dt> <dd>
<p class="first last">Areas of the image with a value smaller than this threshold will be considered borders. The evolution of the contour will stop in this areas.</p> </dd> <dt>
<code>balloon : float, optional</code> </dt> <dd>
<p class="first last">Balloon force to guide the contour in non-informative areas of the image, i.e., areas where the gradient of the image is too small to push the contour towards a border. A negative value will shrink the contour, while a positive value will expand the contour in these areas. Setting this to zero will disable the balloon force.</p> </dd> <dt>
<code>iter_callback : function, optional</code> </dt> <dd>
<p class="first last">If given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>out : (M, N) or (L, M, N) array</code> </dt> <dd>
<p class="first last">Final segmentation (i.e., the final level set)</p> </dd> </dl> </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="#skimage.segmentation.inverse_gaussian_gradient" title="skimage.segmentation.inverse_gaussian_gradient"><code>inverse_gaussian_gradient</code></a>, <a class="reference internal" href="#skimage.segmentation.circle_level_set" title="skimage.segmentation.circle_level_set"><code>circle_level_set</code></a>, <a class="reference internal" href="#skimage.segmentation.checkerboard_level_set" title="skimage.segmentation.checkerboard_level_set"><code>checkerboard_level_set</code></a></p> </div> <h4 class="rubric">Notes</h4> <p>This is a version of the Geodesic Active Contours (GAC) algorithm that uses morphological operators instead of solving partial differential equations (PDEs) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the GAC PDEs (see <a class="reference internal" href="#rb6daaf5d7730-1" id="id17">[1]</a>). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (e.g., it is not necessary to find the right time step for the evolution), and are computationally faster.</p> <p>The algorithm and its theoretical derivation are described in <a class="reference internal" href="#rb6daaf5d7730-1" id="id18">[1]</a>.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rb6daaf5d7730-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id17">1</a>, <a class="fn-backref" href="#id18">2</a>, <a class="fn-backref" href="#id19">3</a>)</em> A Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo Márquez-Neila, Luis Baumela, Luis Álvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI 10.1109/TPAMI.2013.106</td>
</tr>  </table> </dd>
</dl>   <h2 id="morphological-chan-vese">morphological_chan_vese</h2> <dl class="function"> <dt id="skimage.segmentation.morphological_chan_vese">
<code>skimage.segmentation.morphological_chan_vese(image, iterations, init_level_set='checkerboard', smoothing=1, lambda1=1, lambda2=1, iter_callback=&lt;function &lt;lambda&gt;&gt;)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/morphsnakes.py#L214"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Morphological Active Contours without Edges (MorphACWE)</p> <p>Active contours without edges implemented with morphological operators. It can be used to segment objects in images and volumes without well defined borders. It is required that the inside of the object looks different on average than the outside (i.e., the inner area of the object should be darker or lighter than the outer area on average).</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (M, N) or (L, M, N) array</code> </dt> <dd>
<p class="first last">Grayscale image or volume to be segmented.</p> </dd> <dt>
<code>iterations : uint</code> </dt> <dd>
<p class="first last">Number of iterations to run</p> </dd> <dt>
<code>init_level_set : str, (M, N) array, or (L, M, N) array</code> </dt> <dd>
<p class="first last">Initial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the <code>image</code>. Accepted values are ‘checkerboard’ and ‘circle’. See the documentation of <code>checkerboard_level_set</code> and <code>circle_level_set</code> respectively for details about how these level sets are created.</p> </dd> <dt>
<code>smoothing : uint, optional</code> </dt> <dd>
<p class="first last">Number of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.</p> </dd> <dt>
<code>lambda1 : float, optional</code> </dt> <dd>
<p class="first last">Weight parameter for the outer region. If <code>lambda1</code> is larger than <code>lambda2</code>, the outer region will contain a larger range of values than the inner region.</p> </dd> <dt>
<code>lambda2 : float, optional</code> </dt> <dd>
<p class="first last">Weight parameter for the inner region. If <code>lambda2</code> is larger than <code>lambda1</code>, the inner region will contain a larger range of values than the outer region.</p> </dd> <dt>
<code>iter_callback : function, optional</code> </dt> <dd>
<p class="first last">If given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>out : (M, N) or (L, M, N) array</code> </dt> <dd>
<p class="first last">Final segmentation (i.e., the final level set)</p> </dd> </dl> </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="#skimage.segmentation.circle_level_set" title="skimage.segmentation.circle_level_set"><code>circle_level_set</code></a>, <a class="reference internal" href="#skimage.segmentation.checkerboard_level_set" title="skimage.segmentation.checkerboard_level_set"><code>checkerboard_level_set</code></a></p> </div> <h4 class="rubric">Notes</h4> <p>This is a version of the Chan-Vese algorithm that uses morphological operators instead of solving a partial differential equation (PDE) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the Chan-Vese PDE (see <a class="reference internal" href="#r81c856a3d0d3-1" id="id20">[1]</a>). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (it is not necessary to find the right time step for the evolution), and are computationally faster.</p> <p>The algorithm and its theoretical derivation are described in <a class="reference internal" href="#r81c856a3d0d3-1" id="id21">[1]</a>.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r81c856a3d0d3-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id20">1</a>, <a class="fn-backref" href="#id21">2</a>, <a class="fn-backref" href="#id22">3</a>)</em> A Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo Márquez-Neila, Luis Baumela, Luis Álvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI 10.1109/TPAMI.2013.106</td>
</tr>  </table> </dd>
</dl>   <h2 id="inverse-gaussian-gradient">inverse_gaussian_gradient</h2> <dl class="function"> <dt id="skimage.segmentation.inverse_gaussian_gradient">
<code>skimage.segmentation.inverse_gaussian_gradient(image, alpha=100.0, sigma=5.0)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/morphsnakes.py#L182"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Inverse of gradient magnitude.</p> <p>Compute the magnitude of the gradients in the image and then inverts the result in the range [0, 1]. Flat areas are assigned values close to 1, while areas close to borders are assigned values close to 0.</p> <p>This function or a similar one defined by the user should be applied over the image as a preprocessing step before calling <code>morphological_geodesic_active_contour</code>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (M, N) or (L, M, N) array</code> </dt> <dd>
<p class="first last">Grayscale image or volume.</p> </dd> <dt>
<code>alpha : float, optional</code> </dt> <dd>
<p class="first last">Controls the steepness of the inversion. A larger value will make the transition between the flat areas and border areas steeper in the resulting array.</p> </dd> <dt>
<code>sigma : float, optional</code> </dt> <dd>
<p class="first last">Standard deviation of the Gaussian filter applied over the image.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>gimage : (M, N) or (L, M, N) array</code> </dt> <dd>
<p class="first last">Preprocessed image (or volume) suitable for <code>morphological_geodesic_active_contour</code>.</p> </dd> </dl> </td> </tr>  </table> </dd>
</dl>   <h2 id="circle-level-set">circle_level_set</h2> <dl class="function"> <dt id="skimage.segmentation.circle_level_set">
<code>skimage.segmentation.circle_level_set(image_shape, center=None, radius=None)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/morphsnakes.py#L114"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a circle level set with binary values.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image_shape : tuple of positive integers</code> </dt> <dd>
<p class="first last">Shape of the image</p> </dd> <dt>
<code>center : tuple of positive integers, optional</code> </dt> <dd>
<p class="first last">Coordinates of the center of the circle given in (row, column). If not given, it defaults to the center of the image.</p> </dd> <dt>
<code>radius : float, optional</code> </dt> <dd>
<p class="first last">Radius of the circle. If not given, it is set to the 75% of the smallest image dimension.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>out : array with shape image_shape</code> </dt> <dd>
<p class="first last">Binary level set of the circle with the given <code>radius</code> and <code>center</code>.</p> </dd> </dl> </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="#skimage.segmentation.checkerboard_level_set" title="skimage.segmentation.checkerboard_level_set"><code>checkerboard_level_set</code></a></p> </div> </dd>
</dl>   <h2 id="checkerboard-level-set">checkerboard_level_set</h2> <dl class="function"> <dt id="skimage.segmentation.checkerboard_level_set">
<code>skimage.segmentation.checkerboard_level_set(image_shape, square_size=5)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/segmentation/morphsnakes.py#L151"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a checkerboard level set with binary values.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image_shape : tuple of positive integers</code> </dt> <dd>
<p class="first last">Shape of the image.</p> </dd> <dt>
<code>square_size : int, optional</code> </dt> <dd>
<p class="first last">Size of the squares of the checkerboard. It defaults to 5.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>out : array with shape image_shape</code> </dt> <dd>
<p class="first last">Binary level set of the checkerboard.</p> </dd> </dl> </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="#skimage.segmentation.circle_level_set" title="skimage.segmentation.circle_level_set"><code>circle_level_set</code></a></p> </div> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2011 the scikit-image team<br>Licensed under the BSD 3-clause License.<br>
    <a href="http://scikit-image.org/docs/0.14.x/api/skimage.segmentation.html" class="_attribution-link">http://scikit-image.org/docs/0.14.x/api/skimage.segmentation.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
