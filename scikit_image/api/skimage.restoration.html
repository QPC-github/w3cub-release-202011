
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Restoration - Scikit-image - W3cubDocs</title>
  
  <meta name="description" content=" Image restoration module. ">
  <meta name="keywords" content="module, restoration, scikit-image, scikit_image">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/scikit_image/api/skimage.restoration.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/scikit_image.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_image/" class="_nav-link" title="" style="margin-left:0;">scikit-image</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="module-restoration">Module: restoration</h1> <p id="module-skimage.restoration">Image restoration module.</p> <table class="longtable docutils">   <tr>
<td>
<a class="reference internal" href="#skimage.restoration.wiener" title="skimage.restoration.wiener"><code>skimage.restoration.wiener</code></a>(image, psf, balance)</td> <td>Wiener-Hunt deconvolution</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.unsupervised_wiener" title="skimage.restoration.unsupervised_wiener"><code>skimage.restoration.unsupervised_wiener</code></a>(…)</td> <td>Unsupervised Wiener-Hunt deconvolution.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.richardson_lucy" title="skimage.restoration.richardson_lucy"><code>skimage.restoration.richardson_lucy</code></a>(image, psf)</td> <td>Richardson-Lucy deconvolution.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.unwrap_phase" title="skimage.restoration.unwrap_phase"><code>skimage.restoration.unwrap_phase</code></a>(image[, …])</td> <td>Recover the original from a wrapped phase image.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.denoise_tv_bregman" title="skimage.restoration.denoise_tv_bregman"><code>skimage.restoration.denoise_tv_bregman</code></a>(…)</td> <td>Perform total-variation denoising using split-Bregman optimization.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.denoise_tv_chambolle" title="skimage.restoration.denoise_tv_chambolle"><code>skimage.restoration.denoise_tv_chambolle</code></a>(image)</td> <td>Perform total-variation denoising on n-dimensional images.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.denoise_bilateral" title="skimage.restoration.denoise_bilateral"><code>skimage.restoration.denoise_bilateral</code></a>(image)</td> <td>Denoise image using bilateral filter.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.denoise_wavelet" title="skimage.restoration.denoise_wavelet"><code>skimage.restoration.denoise_wavelet</code></a>(image[, …])</td> <td>Perform wavelet denoising on an image.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.denoise_nl_means" title="skimage.restoration.denoise_nl_means"><code>skimage.restoration.denoise_nl_means</code></a>(image)</td> <td>Perform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB images.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.estimate_sigma" title="skimage.restoration.estimate_sigma"><code>skimage.restoration.estimate_sigma</code></a>(image[, …])</td> <td>Robust wavelet-based estimator of the (Gaussian) noise standard deviation.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.inpaint_biharmonic" title="skimage.restoration.inpaint_biharmonic"><code>skimage.restoration.inpaint_biharmonic</code></a>(…)</td> <td>Inpaint masked points in image with biharmonic equations.</td> </tr> <tr>
<td>
<a class="reference internal" href="#skimage.restoration.cycle_spin" title="skimage.restoration.cycle_spin"><code>skimage.restoration.cycle_spin</code></a>(x, func, …)</td> <td>Cycle spinning (repeatedly apply func to shifted versions of x).</td> </tr>  </table>  <h2 id="wiener">wiener</h2> <dl class="function"> <dt id="skimage.restoration.wiener">
<code>skimage.restoration.wiener(image, psf, balance, reg=None, is_real=True, clip=True)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/deconvolution.py#L17"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wiener-Hunt deconvolution</p> <p>Return the deconvolution with a Wiener-Hunt approach (i.e. with Fourier diagonalisation).</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (M, N) ndarray</code> </dt> <dd>
<p class="first last">Input degraded image</p> </dd> <dt>
<code>psf : ndarray</code> </dt> <dd>
<p class="first last">Point Spread Function. This is assumed to be the impulse response (input image space) if the data-type is real, or the transfer function (Fourier space) if the data-type is complex. There is no constraints on the shape of the impulse response. The transfer function must be of shape <code>(M, N)</code> if <code>is_real is True</code>, <code>(M, N // 2 + 1)</code> otherwise (see <code>np.fft.rfftn</code>).</p> </dd> <dt>
<code>balance : float</code> </dt> <dd>
<p class="first last">The regularisation parameter value that tunes the balance between the data adequacy that improve frequency restoration and the prior adequacy that reduce frequency restoration (to avoid noise artifacts).</p> </dd> <dt>
<code>reg : ndarray, optional</code> </dt> <dd>
<p class="first last">The regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf. Shape constraint is the same as for the <code>psf</code> parameter.</p> </dd> <dt>
<code>is_real : boolean, optional</code> </dt> <dd>
<p class="first last">True by default. Specify if <code>psf</code> and <code>reg</code> are provided with hermitian hypothesis, that is only half of the frequency plane is provided (due to the redundancy of Fourier transform of real signal). It’s apply only if <code>psf</code> and/or <code>reg</code> are provided as transfer function. For the hermitian property see <code>uft</code> module or <code>np.fft.rfftn</code>.</p> </dd> <dt>
<code>clip : boolean, optional</code> </dt> <dd>
<p class="first last">True by default. If True, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>im_deconv : (M, N) ndarray</code> </dt> <dd>
<p class="first last">The deconvolved image.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>This function applies the Wiener filter to a noisy and degraded image by an impulse response (or PSF). If the data model is</p> <div class="math notranslate nohighlight"> \[y = Hx + n\]</div> <p>where <span class="math notranslate nohighlight">\(n\)</span> is noise, <span class="math notranslate nohighlight">\(H\)</span> the PSF and <span class="math notranslate nohighlight">\(x\)</span> the unknown original image, the Wiener filter is</p> <div class="math notranslate nohighlight"> \[\hat x = F^\dagger (|\Lambda_H|^2 + \lambda |\Lambda_D|^2) \Lambda_H^\dagger F y\]</div> <p>where <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(F^\dagger\)</span> are the Fourier and inverse Fourier transfroms respectively, <span class="math notranslate nohighlight">\(\Lambda_H\)</span> the transfer function (or the Fourier transfrom of the PSF, see [Hunt] below) and <span class="math notranslate nohighlight">\(\Lambda_D\)</span> the filter to penalize the restored image frequencies (Laplacian by default, that is penalization of high frequency). The parameter <span class="math notranslate nohighlight">\(\lambda\)</span> tunes the balance between the data (that tends to increase high frequency, even those coming from noise), and the regularization.</p> <p>These methods are then specific to a prior model. Consequently, the application or the true image nature must corresponds to the prior model. By default, the prior model (Laplacian) introduce image smoothness or pixel correlation. It can also be interpreted as high-frequency penalization to compensate the instability of the solution with respect to the data (sometimes called noise amplification or “explosive” solution).</p> <p>Finally, the use of Fourier space implies a circulant property of <span class="math notranslate nohighlight">\(H\)</span>, see [Hunt].</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r78add0113d5e-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>
<p class="first">François Orieux, Jean-François Giovannelli, and Thomas Rodet, “Bayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution”, J. Opt. Soc. Am. A 27, 1593-1607 (2010)</p> <p><a class="reference external" href="http://www.opticsinfobase.org/josaa/abstract.cfm?URI=josaa-27-7-1593">http://www.opticsinfobase.org/josaa/abstract.cfm?URI=josaa-27-7-1593</a></p> <p class="last"><a class="reference external" href="http://research.orieux.fr/files/papers/OGR-JOSA10.pdf">http://research.orieux.fr/files/papers/OGR-JOSA10.pdf</a></p> </td>
</tr>  </table> <table class="docutils citation" frame="void" id="r78add0113d5e-2" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id2">[2]</a></td>
<td>B. R. Hunt “A matrix theory proof of the discrete convolution theorem”, IEEE Trans. on Audio and Electroacoustics, vol. au-19, no. 4, pp. 285-288, dec. 1971</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage import color, data, restoration
&gt;&gt;&gt; img = color.rgb2gray(data.astronaut())
&gt;&gt;&gt; from scipy.signal import convolve2d
&gt;&gt;&gt; psf = np.ones((5, 5)) / 25
&gt;&gt;&gt; img = convolve2d(img, psf, 'same')
&gt;&gt;&gt; img += 0.1 * img.std() * np.random.standard_normal(img.shape)
&gt;&gt;&gt; deconvolved_img = restoration.wiener(img, psf, 1100)
</pre> </dd>
</dl>   <h2 id="unsupervised-wiener">unsupervised_wiener</h2> <dl class="function"> <dt id="skimage.restoration.unsupervised_wiener">
<code>skimage.restoration.unsupervised_wiener(image, psf, reg=None, user_params=None, is_real=True, clip=True)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/deconvolution.py#L144"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Unsupervised Wiener-Hunt deconvolution.</p> <p>Return the deconvolution with a Wiener-Hunt approach, where the hyperparameters are automatically estimated. The algorithm is a stochastic iterative process (Gibbs sampler) described in the reference below. See also <code>wiener</code> function.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (M, N) ndarray</code> </dt> <dd>
<p class="first last">The input degraded image.</p> </dd> <dt>
<code>psf : ndarray</code> </dt> <dd>
<p class="first last">The impulse response (input image’s space) or the transfer function (Fourier space). Both are accepted. The transfer function is automatically recognized as being complex (<code>np.iscomplexobj(psf)</code>).</p> </dd> <dt>
<code>reg : ndarray, optional</code> </dt> <dd>
<p class="first last">The regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf.</p> </dd> <dt>
<code>user_params : dict</code> </dt> <dd>
<p class="first last">Dictionary of parameters for the Gibbs sampler. See below.</p> </dd> <dt>
<code>clip : boolean, optional</code> </dt> <dd>
<p class="first last">True by default. If true, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>x_postmean : (M, N) ndarray</code> </dt> <dd>
<p class="first last">The deconvolved image (the posterior mean).</p> </dd> <dt>
<code>chains : dict</code> </dt> <dd>
<p class="first last">The keys <code>noise</code> and <code>prior</code> contain the chain list of noise and prior precision respectively.</p> </dd> </dl> </td> </tr> <tr><th class="field-name" colspan="2">Other Parameters:</th></tr> <tr>
<td> </td>
<td class="field-body">
<dl class="first last docutils"> <dt><strong>The keys of ``user_params`` are:</strong></dt>  <dt>
<code>threshold : float</code> </dt> <dd>
<p class="first last">The stopping criterion: the norm of the difference between to successive approximated solution (empirical mean of object samples, see Notes section). 1e-4 by default.</p> </dd> <dt>
<code>burnin : int</code> </dt> <dd>
<p class="first last">The number of sample to ignore to start computation of the mean. 15 by default.</p> </dd> <dt>
<code>min_iter : int</code> </dt> <dd>
<p class="first last">The minimum number of iterations. 30 by default.</p> </dd> <dt>
<code>max_iter : int</code> </dt> <dd>
<p class="first last">The maximum number of iterations if <code>threshold</code> is not satisfied. 200 by default.</p> </dd> <dt>
<code>callback : callable (None by default)</code> </dt> <dd>
<p class="first last">A user provided callable to which is passed, if the function exists, the current image sample for whatever purpose. The user can store the sample, or compute other moments than the mean. It has no influence on the algorithm execution and is only for inspection.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The estimated image is design as the posterior mean of a probability law (from a Bayesian analysis). The mean is defined as a sum over all the possible images weighted by their respective probability. Given the size of the problem, the exact sum is not tractable. This algorithm use of MCMC to draw image under the posterior law. The practical idea is to only draw highly probable images since they have the biggest contribution to the mean. At the opposite, the less probable images are drawn less often since their contribution is low. Finally the empirical mean of these samples give us an estimation of the mean, and an exact computation with an infinite sample set.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rc01bcdcadf9b-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id3">[1]</a></td>
<td>
<p class="first">François Orieux, Jean-François Giovannelli, and Thomas Rodet, “Bayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution”, J. Opt. Soc. Am. A 27, 1593-1607 (2010)</p> <p><a class="reference external" href="http://www.opticsinfobase.org/josaa/abstract.cfm?URI=josaa-27-7-1593">http://www.opticsinfobase.org/josaa/abstract.cfm?URI=josaa-27-7-1593</a></p> <p class="last"><a class="reference external" href="http://research.orieux.fr/files/papers/OGR-JOSA10.pdf">http://research.orieux.fr/files/papers/OGR-JOSA10.pdf</a></p> </td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage import color, data, restoration
&gt;&gt;&gt; img = color.rgb2gray(data.astronaut())
&gt;&gt;&gt; from scipy.signal import convolve2d
&gt;&gt;&gt; psf = np.ones((5, 5)) / 25
&gt;&gt;&gt; img = convolve2d(img, psf, 'same')
&gt;&gt;&gt; img += 0.1 * img.std() * np.random.standard_normal(img.shape)
&gt;&gt;&gt; deconvolved_img = restoration.unsupervised_wiener(img, psf)
</pre> </dd>
</dl>   <h2 id="richardson-lucy">richardson_lucy</h2> <dl class="function"> <dt id="skimage.restoration.richardson_lucy">
<code>skimage.restoration.richardson_lucy(image, psf, iterations=50, clip=True)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/deconvolution.py#L333"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Richardson-Lucy deconvolution.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : ndarray</code> </dt> <dd>
<p class="first last">Input degraded image (can be N dimensional).</p> </dd> <dt>
<code>psf : ndarray</code> </dt> <dd>
<p class="first last">The point spread function.</p> </dd> <dt>
<code>iterations : int</code> </dt> <dd>
<p class="first last">Number of iterations. This parameter plays the role of regularisation.</p> </dd> <dt>
<code>clip : boolean, optional</code> </dt> <dd>
<p class="first last">True by default. If true, pixel value of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>im_deconv : ndarray</code> </dt> <dd>
<p class="first last">The deconvolved image.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rbba3d9c89116-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id4">[1]</a></td>
<td><a class="reference external" href="http://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution">http://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution</a></td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage import color, data, restoration
&gt;&gt;&gt; camera = color.rgb2gray(data.camera())
&gt;&gt;&gt; from scipy.signal import convolve2d
&gt;&gt;&gt; psf = np.ones((5, 5)) / 25
&gt;&gt;&gt; camera = convolve2d(camera, psf, 'same')
&gt;&gt;&gt; camera += 0.1 * camera.std() * np.random.standard_normal(camera.shape)
&gt;&gt;&gt; deconvolved = restoration.richardson_lucy(camera, psf, 5)
</pre> </dd>
</dl>   <h2 id="unwrap-phase">unwrap_phase</h2> <dl class="function"> <dt id="skimage.restoration.unwrap_phase">
<code>skimage.restoration.unwrap_phase(image, wrap_around=False, seed=None)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/unwrap.py#L11"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Recover the original from a wrapped phase image.</p> <p>From an image wrapped to lie in the interval [-pi, pi), recover the original, unwrapped image.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : 1D, 2D or 3D ndarray of floats, optionally a masked array</code> </dt> <dd>
<p class="first last">The values should be in the range [-pi, pi). If a masked array is provided, the masked entries will not be changed, and their values will not be used to guide the unwrapping of neighboring, unmasked values. Masked 1D arrays are not allowed, and will raise a <code>ValueError</code>.</p> </dd> <dt>
<code>wrap_around : bool or sequence of bool, optional</code> </dt> <dd>
<p class="first last">When an element of the sequence is <code>True</code>, the unwrapping process will regard the edges along the corresponding axis of the image to be connected and use this connectivity to guide the phase unwrapping process. If only a single boolean is given, it will apply to all axes. Wrap around is not supported for 1D arrays.</p> </dd> <dt>
<code>seed : int, optional</code> </dt> <dd>
<p class="first last">Unwrapping 2D or 3D images uses random initialization. This sets the seed of the PRNG to achieve deterministic behavior.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image_unwrapped : array_like, double</code> </dt> <dd>
<p class="first last">Unwrapped image of the same shape as the input. If the input <code>image</code> was a masked array, the mask will be preserved.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Raises:</th>
<td class="field-body">
<dl class="first last docutils"> <dt><strong>ValueError</strong></dt> <dd>
<p class="first last">If called with a masked 1D array or called with a 1D array and <code>wrap_around=True</code>.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r160444e59583-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id5">[1]</a></td>
<td>Miguel Arevallilo Herraez, David R. Burton, Michael J. Lalor, and Munther A. Gdeisat, “Fast two-dimensional phase-unwrapping algorithm based on sorting by reliability following a noncontinuous path”, Journal Applied Optics, Vol. 41, No. 35 (2002) 7437,</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r160444e59583-2" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id6">[2]</a></td>
<td>Abdul-Rahman, H., Gdeisat, M., Burton, D., &amp; Lalor, M., “Fast three-dimensional phase-unwrapping algorithm based on sorting by reliability following a non-continuous path. In W. Osten, C. Gorecki, &amp; E. L. Novak (Eds.), Optical Metrology (2005) 32–40, International Society for Optics and Photonics.</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; c0, c1 = np.ogrid[-1:1:128j, -1:1:128j]
&gt;&gt;&gt; image = 12 * np.pi * np.exp(-(c0**2 + c1**2))
&gt;&gt;&gt; image_wrapped = np.angle(np.exp(1j * image))
&gt;&gt;&gt; image_unwrapped = unwrap_phase(image_wrapped)
&gt;&gt;&gt; np.std(image_unwrapped - image) &lt; 1e-6   # A constant offset is normal
True
</pre> </dd>
</dl>   <h2 id="denoise-tv-bregman">denoise_tv_bregman</h2> <dl class="function"> <dt id="skimage.restoration.denoise_tv_bregman">
<code>skimage.restoration.denoise_tv_bregman(image, weight, max_iter=100, eps=0.001, isotropic=True)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/_denoise.py#L120"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Perform total-variation denoising using split-Bregman optimization.</p> <p>Total-variation denoising (also know as total-variation regularization) tries to find an image with less total-variation under the constraint of being similar to the input image, which is controlled by the regularization parameter (<a class="reference internal" href="#rc0e3588f2bc3-1" id="id7">[1]</a>, <a class="reference internal" href="#rc0e3588f2bc3-2" id="id8">[2]</a>, <a class="reference internal" href="#rc0e3588f2bc3-3" id="id9">[3]</a>, <a class="reference internal" href="#rc0e3588f2bc3-4" id="id10">[4]</a>).</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : ndarray</code> </dt> <dd>
<p class="first last">Input data to be denoised (converted using img_as_float`).</p> </dd> <dt>
<code>weight : float</code> </dt> <dd>
<p class="first last">Denoising weight. The smaller the <code>weight</code>, the more denoising (at the expense of less similarity to the <code>input</code>). The regularization parameter <code>lambda</code> is chosen as <code>2 * weight</code>.</p> </dd> <dt>
<code>eps : float, optional</code> </dt> <dd>
<p class="first">Relative difference of the value of the cost function that determines the stop criterion. The algorithm stops when:</p> <pre data-language="python">SUM((u(n) - u(n-1))**2) &lt; eps
</pre> </dd> <dt>
<code>max_iter : int, optional</code> </dt> <dd>
<p class="first last">Maximal number of iterations used for the optimization.</p> </dd> <dt>
<code>isotropic : boolean, optional</code> </dt> <dd>
<p class="first last">Switch between isotropic and anisotropic TV denoising.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>u : ndarray</code> </dt> <dd>
<p class="first last">Denoised image.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rc0e3588f2bc3-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id7">1</a>, <a class="fn-backref" href="#id11">2</a>)</em> <a class="reference external" href="http://en.wikipedia.org/wiki/Total_variation_denoising">http://en.wikipedia.org/wiki/Total_variation_denoising</a>
</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rc0e3588f2bc3-2" rules="none">   <tr>
<td class="label">[2]</td>
<td>
<em>(<a class="fn-backref" href="#id8">1</a>, <a class="fn-backref" href="#id12">2</a>)</em> Tom Goldstein and Stanley Osher, “The Split Bregman Method For L1 Regularized Problems”, <a class="reference external" href="ftp://ftp.math.ucla.edu/pub/camreport/cam08-29.pdf">ftp://ftp.math.ucla.edu/pub/camreport/cam08-29.pdf</a>
</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rc0e3588f2bc3-3" rules="none">   <tr>
<td class="label">[3]</td>
<td>
<em>(<a class="fn-backref" href="#id9">1</a>, <a class="fn-backref" href="#id13">2</a>)</em> Pascal Getreuer, “Rudin–Osher–Fatemi Total Variation Denoising using Split Bregman” in Image Processing On Line on 2012–05–19, <a class="reference external" href="http://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf">http://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf</a>
</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rc0e3588f2bc3-4" rules="none">   <tr>
<td class="label">[4]</td>
<td>
<em>(<a class="fn-backref" href="#id10">1</a>, <a class="fn-backref" href="#id14">2</a>)</em> <a class="reference external" href="http://www.math.ucsb.edu/~cgarcia/UGProjects/BregmanAlgorithms_JacquelineBush.pdf">http://www.math.ucsb.edu/~cgarcia/UGProjects/BregmanAlgorithms_JacquelineBush.pdf</a>
</td>
</tr>  </table> </dd>
</dl>   <h2 id="denoise-tv-chambolle">denoise_tv_chambolle</h2> <dl class="function"> <dt id="skimage.restoration.denoise_tv_chambolle">
<code>skimage.restoration.denoise_tv_chambolle(image, weight=0.1, eps=0.0002, n_iter_max=200, multichannel=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/_denoise.py#L249"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Perform total-variation denoising on n-dimensional images.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : ndarray of ints, uints or floats</code> </dt> <dd>
<p class="first last">Input data to be denoised. <code>image</code> can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.</p> </dd> <dt>
<code>weight : float, optional</code> </dt> <dd>
<p class="first last">Denoising weight. The greater <code>weight</code>, the more denoising (at the expense of fidelity to <code>input</code>).</p> </dd> <dt>
<code>eps : float, optional</code> </dt> <dd>
<p class="first">Relative difference of the value of the cost function that determines the stop criterion. The algorithm stops when:</p>  <p>(E_(n-1) - E_n) &lt; eps * E_0</p>  </dd> <dt>
<code>n_iter_max : int, optional</code> </dt> <dd>
<p class="first last">Maximal number of iterations used for the optimization.</p> </dd> <dt>
<code>multichannel : bool, optional</code> </dt> <dd>
<p class="first last">Apply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>out : ndarray</code> </dt> <dd>
<p class="first last">Denoised image.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>Make sure to set the multichannel parameter appropriately for color images.</p> <p>The principle of total variation denoising is explained in <a class="reference external" href="http://en.wikipedia.org/wiki/Total_variation_denoising">http://en.wikipedia.org/wiki/Total_variation_denoising</a></p> <p>The principle of total variation denoising is to minimize the total variation of the image, which can be roughly described as the integral of the norm of the image gradient. Total variation denoising tends to produce “cartoon-like” images, that is, piecewise-constant images.</p> <p>This code is an implementation of the algorithm of Rudin, Fatemi and Osher that was proposed by Chambolle in <a class="reference internal" href="#r3f46bb237e10-1" id="id15">[1]</a>.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r3f46bb237e10-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id15">1</a>, <a class="fn-backref" href="#id16">2</a>)</em> A. Chambolle, An algorithm for total variation minimization and applications, Journal of Mathematical Imaging and Vision, Springer, 2004, 20, 89-97.</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <p>2D example on astronaut image:</p> <pre data-language="python">&gt;&gt;&gt; from skimage import color, data
&gt;&gt;&gt; img = color.rgb2gray(data.astronaut())[:50, :50]
&gt;&gt;&gt; img += 0.5 * img.std() * np.random.randn(*img.shape)
&gt;&gt;&gt; denoised_img = denoise_tv_chambolle(img, weight=60)
</pre> <p>3D example on synthetic data:</p> <pre data-language="python">&gt;&gt;&gt; x, y, z = np.ogrid[0:20, 0:20, 0:20]
&gt;&gt;&gt; mask = (x - 22)**2 + (y - 20)**2 + (z - 17)**2 &lt; 8**2
&gt;&gt;&gt; mask = mask.astype(np.float)
&gt;&gt;&gt; mask += 0.2*np.random.randn(*mask.shape)
&gt;&gt;&gt; res = denoise_tv_chambolle(mask, weight=100)
</pre> </dd>
</dl>   <h2 id="denoise-bilateral">denoise_bilateral</h2> <dl class="function"> <dt id="skimage.restoration.denoise_bilateral">
<code>skimage.restoration.denoise_bilateral(image, win_size=None, sigma_color=None, sigma_spatial=1, bins=10000, mode='constant', cval=0, multichannel=None)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/_denoise.py#L13"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Denoise image using bilateral filter.</p> <p>This is an edge-preserving, denoising filter. It averages pixels based on their spatial closeness and radiometric similarity <a class="reference internal" href="#rb832e60bc162-1" id="id17">[1]</a>.</p> <p>Spatial closeness is measured by the Gaussian function of the Euclidean distance between two pixels and a certain standard deviation (<code>sigma_spatial</code>).</p> <p>Radiometric similarity is measured by the Gaussian function of the Euclidean distance between two color values and a certain standard deviation (<code>sigma_color</code>).</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : ndarray, shape (M, N[, 3])</code> </dt> <dd>
<p class="first last">Input image, 2D grayscale or RGB.</p> </dd> <dt>
<code>win_size : int</code> </dt> <dd>
<p class="first last">Window size for filtering. If win_size is not specified, it is calculated as <code>max(5, 2 * ceil(3 * sigma_spatial) + 1)</code>.</p> </dd> <dt>
<code>sigma_color : float</code> </dt> <dd>
<p class="first last">Standard deviation for grayvalue/color distance (radiometric similarity). A larger value results in averaging of pixels with larger radiometric differences. Note, that the image will be converted using the <code>img_as_float</code> function and thus the standard deviation is in respect to the range <code>[0, 1]</code>. If the value is <code>None</code> the standard deviation of the <code>image</code> will be used.</p> </dd> <dt>
<code>sigma_spatial : float</code> </dt> <dd>
<p class="first last">Standard deviation for range distance. A larger value results in averaging of pixels with larger spatial differences.</p> </dd> <dt>
<code>bins : int</code> </dt> <dd>
<p class="first last">Number of discrete values for Gaussian weights of color filtering. A larger value results in improved accuracy.</p> </dd> <dt>
<code>mode : {‘constant’, ‘edge’, ‘symmetric’, ‘reflect’, ‘wrap’}</code> </dt> <dd>
<p class="first last">How to handle values outside the image borders. See <code>numpy.pad</code> for detail.</p> </dd> <dt>
<code>cval : string</code> </dt> <dd>
<p class="first last">Used in conjunction with mode ‘constant’, the value outside the image boundaries.</p> </dd> <dt>
<code>multichannel : bool</code> </dt> <dd>
<p class="first last">Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>denoised : ndarray</code> </dt> <dd>
<p class="first last">Denoised image.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rb832e60bc162-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id17">1</a>, <a class="fn-backref" href="#id18">2</a>)</em> <a class="reference external" href="http://users.soe.ucsc.edu/~manduchi/Papers/ICCV98.pdf">http://users.soe.ucsc.edu/~manduchi/Papers/ICCV98.pdf</a>
</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage import data, img_as_float
&gt;&gt;&gt; astro = img_as_float(data.astronaut())
&gt;&gt;&gt; astro = astro[220:300, 220:320]
&gt;&gt;&gt; noisy = astro + 0.6 * astro.std() * np.random.random(astro.shape)
&gt;&gt;&gt; noisy = np.clip(noisy, 0, 1)
&gt;&gt;&gt; denoised = denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15)
</pre> </dd>
</dl>   <h2 id="denoise-wavelet">denoise_wavelet</h2> <dl class="function"> <dt id="skimage.restoration.denoise_wavelet">
<code>skimage.restoration.denoise_wavelet(image, sigma=None, wavelet='db1', mode='soft', wavelet_levels=None, multichannel=False, convert2ycbcr=False, method='BayesShrink')</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/_denoise.py#L494"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Perform wavelet denoising on an image.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : ndarray ([M[, N[, …P]][, C]) of ints, uints or floats</code> </dt> <dd>
<p class="first last">Input data to be denoised. <code>image</code> can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.</p> </dd> <dt>
<code>sigma : float or list, optional</code> </dt> <dd>
<p class="first last">The noise standard deviation used when computing the wavelet detail coefficient threshold(s). When None (default), the noise standard deviation is estimated via the method in <a class="reference internal" href="#r3b8ec6d23a4e-2" id="id19">[2]</a>.</p> </dd> <dt>
<code>wavelet : string, optional</code> </dt> <dd>
<p class="first last">The type of wavelet to perform and can be any of the options <code>pywt.wavelist</code> outputs. The default is <code>‘db1’</code>. For example, <code>wavelet</code> can be any of <code>{'db2', 'haar', 'sym9'}</code> and many more.</p> </dd> <dt>
<code>mode : {‘soft’, ‘hard’}, optional</code> </dt> <dd>
<p class="first last">An optional argument to choose the type of denoising performed. It noted that choosing soft thresholding given additive noise finds the best approximation of the original image.</p> </dd> <dt>
<code>wavelet_levels : int or None, optional</code> </dt> <dd>
<p class="first last">The number of wavelet decomposition levels to use. The default is three less than the maximum number of possible decomposition levels.</p> </dd> <dt>
<code>multichannel : bool, optional</code> </dt> <dd>
<p class="first last">Apply wavelet denoising separately for each channel (where channels correspond to the final axis of the array).</p> </dd> <dt>
<code>convert2ycbcr : bool, optional</code> </dt> <dd>
<p class="first last">If True and multichannel True, do the wavelet denoising in the YCbCr colorspace instead of the RGB color space. This typically results in better performance for RGB images.</p> </dd> <dt>
<code>method : {‘BayesShrink’, ‘VisuShrink’}, optional</code> </dt> <dd>
<p class="first last">Thresholding method to be used. The currently supported methods are “BayesShrink” <a class="reference internal" href="#r3b8ec6d23a4e-1" id="id20">[1]</a> and “VisuShrink” <a class="reference internal" href="#r3b8ec6d23a4e-2" id="id21">[2]</a>. Defaults to “BayesShrink”.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>out : ndarray</code> </dt> <dd>
<p class="first last">Denoised image.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The wavelet domain is a sparse representation of the image, and can be thought of similarly to the frequency domain of the Fourier transform. Sparse representations have most values zero or near-zero and truly random noise is (usually) represented by many small values in the wavelet domain. Setting all values below some threshold to 0 reduces the noise in the image, but larger thresholds also decrease the detail present in the image.</p> <p>If the input is 3D, this function performs wavelet denoising on each color plane separately. The output image is clipped between either [-1, 1] and [0, 1] depending on the input image range.</p> <p>When YCbCr conversion is done, every color channel is scaled between 0 and 1, and <code>sigma</code> values are applied to these scaled color channels.</p> <p>Many wavelet coefficient thresholding approaches have been proposed. By default, <code>denoise_wavelet</code> applies BayesShrink, which is an adaptive thresholding method that computes separate thresholds for each wavelet sub-band as described in <a class="reference internal" href="#r3b8ec6d23a4e-1" id="id22">[1]</a>.</p> <p>If <code>method == "VisuShrink"</code>, a single “universal threshold” is applied to all wavelet detail coefficients as described in <a class="reference internal" href="#r3b8ec6d23a4e-2" id="id23">[2]</a>. This threshold is designed to remove all Gaussian noise at a given <code>sigma</code> with high probability, but tends to produce images that appear overly smooth.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r3b8ec6d23a4e-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id20">1</a>, <a class="fn-backref" href="#id22">2</a>, <a class="fn-backref" href="#id24">3</a>)</em> Chang, S. Grace, Bin Yu, and Martin Vetterli. “Adaptive wavelet thresholding for image denoising and compression.” Image Processing, IEEE Transactions on 9.9 (2000): 1532-1546. DOI: 10.1109/83.862633</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r3b8ec6d23a4e-2" rules="none">   <tr>
<td class="label">[2]</td>
<td>
<em>(<a class="fn-backref" href="#id19">1</a>, <a class="fn-backref" href="#id21">2</a>, <a class="fn-backref" href="#id23">3</a>, <a class="fn-backref" href="#id25">4</a>)</em> D. L. Donoho and I. M. Johnstone. “Ideal spatial adaptation by wavelet shrinkage.” Biometrika 81.3 (1994): 425-455. DOI: 10.1093/biomet/81.3.425</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage import color, data
&gt;&gt;&gt; img = img_as_float(data.astronaut())
&gt;&gt;&gt; img = color.rgb2gray(img)
&gt;&gt;&gt; img += 0.1 * np.random.randn(*img.shape)
&gt;&gt;&gt; img = np.clip(img, 0, 1)
&gt;&gt;&gt; denoised_img = denoise_wavelet(img, sigma=0.1)
</pre> </dd>
</dl>   <h2 id="denoise-nl-means">denoise_nl_means</h2> <dl class="function"> <dt id="skimage.restoration.denoise_nl_means">
<code>skimage.restoration.denoise_nl_means(image, patch_size=7, patch_distance=11, h=0.1, multichannel=None, fast_mode=True, sigma=0.0)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/non_local_means.py#L10"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Perform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB images.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : 2D or 3D ndarray</code> </dt> <dd>
<p class="first last">Input image to be denoised, which can be 2D or 3D, and grayscale or RGB (for 2D images only, see <code>multichannel</code> parameter).</p> </dd> <dt>
<code>patch_size : int, optional</code> </dt> <dd>
<p class="first last">Size of patches used for denoising.</p> </dd> <dt>
<code>patch_distance : int, optional</code> </dt> <dd>
<p class="first last">Maximal distance in pixels where to search patches used for denoising.</p> </dd> <dt>
<code>h : float, optional</code> </dt> <dd>
<p class="first last">Cut-off distance (in gray levels). The higher h, the more permissive one is in accepting patches. A higher h results in a smoother image, at the expense of blurring features. For a Gaussian noise of standard deviation sigma, a rule of thumb is to choose the value of h to be sigma of slightly less.</p> </dd> <dt>
<code>multichannel : bool, optional</code> </dt> <dd>
<p class="first last">Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension. Set to <code>False</code> for 3-D images.</p> </dd> <dt>
<code>fast_mode : bool, optional</code> </dt> <dd>
<p class="first last">If True (default value), a fast version of the non-local means algorithm is used. If False, the original version of non-local means is used. See the Notes section for more details about the algorithms.</p> </dd> <dt>
<code>sigma : float, optional</code> </dt> <dd>
<p class="first last">The standard deviation of the (Gaussian) noise. If provided, a more robust computation of patch weights is computed that takes the expected noise variance into account (see Notes below).</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>result : ndarray</code> </dt> <dd>
<p class="first last">Denoised image, of same shape as <code>image</code>.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The non-local means algorithm is well suited for denoising images with specific textures. The principle of the algorithm is to average the value of a given pixel with values of other pixels in a limited neighbourhood, provided that the <em>patches</em> centered on the other pixels are similar enough to the patch centered on the pixel of interest.</p> <p>In the original version of the algorithm <a class="reference internal" href="#rc9b3919da938-1" id="id26">[1]</a>, corresponding to <code>fast=False</code>, the computational complexity is:</p> <pre data-language="python">image.size * patch_size ** image.ndim * patch_distance ** image.ndim
</pre> <p>Hence, changing the size of patches or their maximal distance has a strong effect on computing times, especially for 3-D images.</p> <p>However, the default behavior corresponds to <code>fast_mode=True</code>, for which another version of non-local means <a class="reference internal" href="#rc9b3919da938-2" id="id27">[2]</a> is used, corresponding to a complexity of:</p> <pre data-language="python">image.size * patch_distance ** image.ndim
</pre> <p>The computing time depends only weakly on the patch size, thanks to the computation of the integral of patches distances for a given shift, that reduces the number of operations <a class="reference internal" href="#rc9b3919da938-1" id="id28">[1]</a>. Therefore, this algorithm executes faster than the classic algorith (<code>fast_mode=False</code>), at the expense of using twice as much memory. This implementation has been proven to be more efficient compared to other alternatives, see e.g. <a class="reference internal" href="#rc9b3919da938-3" id="id29">[3]</a>.</p> <p>Compared to the classic algorithm, all pixels of a patch contribute to the distance to another patch with the same weight, no matter their distance to the center of the patch. This coarser computation of the distance can result in a slightly poorer denoising performance. Moreover, for small images (images with a linear size that is only a few times the patch size), the classic algorithm can be faster due to boundary effects.</p> <p>The image is padded using the <code>reflect</code> mode of <code>skimage.util.pad</code> before denoising.</p> <p>If the noise standard deviation, <code>sigma</code>, is provided a more robust computation of patch weights is used. Subtracting the known noise variance from the computed patch distances improves the estimates of patch similarity, giving a moderate improvement to denoising performance <a class="reference internal" href="#rc9b3919da938-4" id="id30">[4]</a>. It was also mentioned as an option for the fast variant of the algorithm in <a class="reference internal" href="#rc9b3919da938-3" id="id31">[3]</a>.</p> <p>When <code>sigma</code> is provided, a smaller <code>h</code> should typically be used to avoid oversmoothing. The optimal value for <code>h</code> depends on the image content and noise level, but a reasonable starting point is <code>h = 0.8 * sigma</code> when <code>fast_mode</code> is <code>True</code>, or <code>h = 0.6 * sigma</code> when <code>fast_mode</code> is <code>False</code>.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rc9b3919da938-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id26">1</a>, <a class="fn-backref" href="#id28">2</a>, <a class="fn-backref" href="#id32">3</a>)</em> A. Buades, B. Coll, &amp; J-M. Morel. A non-local algorithm for image denoising. In CVPR 2005, Vol. 2, pp. 60-65, IEEE. DOI: 10.1109/CVPR.2005.38</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rc9b3919da938-2" rules="none">   <tr>
<td class="label">[2]</td>
<td>
<em>(<a class="fn-backref" href="#id27">1</a>, <a class="fn-backref" href="#id33">2</a>)</em> J. Darbon, A. Cunha, T.F. Chan, S. Osher, and G.J. Jensen, Fast nonlocal filtering applied to electron cryomicroscopy, in 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 2008, pp. 1331-1334. DOI: 10.1109/ISBI.2008.4541250</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rc9b3919da938-3" rules="none">   <tr>
<td class="label">[3]</td>
<td>
<em>(<a class="fn-backref" href="#id29">1</a>, <a class="fn-backref" href="#id31">2</a>, <a class="fn-backref" href="#id34">3</a>)</em> Jacques Froment. Parameter-Free Fast Pixelwise Non-Local Means Denoising. Image Processing On Line, 2014, vol. 4, pp. 300-326. DOI: 10.5201/ipol.2014.120</td>
</tr>  </table> <table class="docutils citation" frame="void" id="rc9b3919da938-4" rules="none">   <tr>
<td class="label">[4]</td>
<td>
<em>(<a class="fn-backref" href="#id30">1</a>, <a class="fn-backref" href="#id35">2</a>)</em> A. Buades, B. Coll, &amp; J-M. Morel. Non-Local Means Denoising. Image Processing On Line, 2011, vol. 1, pp. 208-212. DOI: 10.5201/ipol.2011.bcm_nlm</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; a = np.zeros((40, 40))
&gt;&gt;&gt; a[10:-10, 10:-10] = 1.
&gt;&gt;&gt; a += 0.3 * np.random.randn(*a.shape)
&gt;&gt;&gt; denoised_a = denoise_nl_means(a, 7, 5, 0.1)
</pre> </dd>
</dl>   <h2 id="estimate-sigma">estimate_sigma</h2> <dl class="function"> <dt id="skimage.restoration.estimate_sigma">
<code>skimage.restoration.estimate_sigma(image, average_sigmas=False, multichannel=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/_denoise.py#L626"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Robust wavelet-based estimator of the (Gaussian) noise standard deviation.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : ndarray</code> </dt> <dd>
<p class="first last">Image for which to estimate the noise standard deviation.</p> </dd> <dt>
<code>average_sigmas : bool, optional</code> </dt> <dd>
<p class="first last">If true, average the channel estimates of <code>sigma</code>. Otherwise return a list of sigmas corresponding to each channel.</p> </dd> <dt>
<code>multichannel : bool</code> </dt> <dd>
<p class="first last">Estimate sigma separately for each channel.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>sigma : float or list</code> </dt> <dd>
<p class="first last">Estimated noise standard deviation(s). If <code>multichannel</code> is True and <code>average_sigmas</code> is False, a separate noise estimate for each channel is returned. Otherwise, the average of the individual channel estimates is returned.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>This function assumes the noise follows a Gaussian distribution. The estimation algorithm is based on the median absolute deviation of the wavelet detail coefficients as described in section 4.2 of <a class="reference internal" href="#rbc448ac95825-1" id="id36">[1]</a>.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="rbc448ac95825-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id36">1</a>, <a class="fn-backref" href="#id37">2</a>)</em> D. L. Donoho and I. M. Johnstone. “Ideal spatial adaptation by wavelet shrinkage.” Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import skimage.data
&gt;&gt;&gt; from skimage import img_as_float
&gt;&gt;&gt; img = img_as_float(skimage.data.camera())
&gt;&gt;&gt; sigma = 0.1
&gt;&gt;&gt; img = img + sigma * np.random.standard_normal(img.shape)
&gt;&gt;&gt; sigma_hat = estimate_sigma(img, multichannel=False)
</pre> </dd>
</dl>   <h2 id="inpaint-biharmonic">inpaint_biharmonic</h2> <dl class="function"> <dt id="skimage.restoration.inpaint_biharmonic">
<code>skimage.restoration.inpaint_biharmonic(image, mask, multichannel=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/inpaint.py#L77"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Inpaint masked points in image with biharmonic equations.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>image : (M[, N[, …, P]][, C]) ndarray</code> </dt> <dd>
<p class="first last">Input image.</p> </dd> <dt>
<code>mask : (M[, N[, …, P]]) ndarray</code> </dt> <dd>
<p class="first last">Array of pixels to be inpainted. Have to be the same shape as one of the ‘image’ channels. Unknown pixels have to be represented with 1, known pixels - with 0.</p> </dd> <dt>
<code>multichannel : boolean, optional</code> </dt> <dd>
<p class="first last">If True, the last <code>image</code> dimension is considered as a color channel, otherwise as spatial.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>out : (M[, N[, …, P]][, C]) ndarray</code> </dt> <dd>
<p class="first last">Input image with masked pixels inpainted.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r2b3da8fbc807-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id38">[1]</a></td>
<td>N.S.Hoang, S.B.Damelin, “On surface completion and image inpainting by biharmonic functions: numerical aspects”, <a class="reference external" href="https://arxiv.org/abs/1707.06567">https://arxiv.org/abs/1707.06567</a>
</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r2b3da8fbc807-2" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id39">[2]</a></td>
<td>C. K. Chui and H. N. Mhaskar, MRA Contextual-Recovery Extension of Smooth Functions on Manifolds, Appl. and Comp. Harmonic Anal., 28 (2010), 104-113, DOI: 10.1016/j.acha.2009.04.004</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; img = np.tile(np.square(np.linspace(0, 1, 5)), (5, 1))
&gt;&gt;&gt; mask = np.zeros_like(img)
&gt;&gt;&gt; mask[2, 2:] = 1
&gt;&gt;&gt; mask[1, 3:] = 1
&gt;&gt;&gt; mask[0, 4:] = 1
&gt;&gt;&gt; out = inpaint_biharmonic(img, mask)
</pre> </dd>
</dl>   <h2 id="cycle-spin">cycle_spin</h2> <dl class="function"> <dt id="skimage.restoration.cycle_spin">
<code>skimage.restoration.cycle_spin(x, func, max_shifts, shift_steps=1, num_workers=None, multichannel=False, func_kw={})</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.14.1/skimage/restoration/_cycle_spin.py#L71"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Cycle spinning (repeatedly apply func to shifted versions of x).</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>x : array-like</code> </dt> <dd>
<p class="first last">Data for input to <code>func</code>.</p> </dd> <dt>
<code>func : function</code> </dt> <dd>
<p class="first last">A function to apply to circularly shifted versions of <code>x</code>. Should take <code>x</code> as its first argument. Any additional arguments can be supplied via <code>func_kw</code>.</p> </dd> <dt>
<code>max_shifts : int or tuple</code> </dt> <dd>
<p class="first last">If an integer, shifts in <code>range(0, max_shifts+1)</code> will be used along each axis of <code>x</code>. If a tuple, <code>range(0, max_shifts[i]+1)</code> will be along axis i.</p> </dd> <dt>
<code>shift_steps : int or tuple, optional</code> </dt> <dd>
<p class="first last">The step size for the shifts applied along axis, i, are:: <code>range((0, max_shifts[i]+1, shift_steps[i]))</code>. If an integer is provided, the same step size is used for all axes.</p> </dd> <dt>
<code>num_workers : int or None, optional</code> </dt> <dd>
<p class="first last">The number of parallel threads to use during cycle spinning. If set to <code>None</code>, the full set of available cores are used.</p> </dd> <dt>
<code>multichannel : bool, optional</code> </dt> <dd>
<p class="first last">Whether to treat the final axis as channels (no cycle shifts are performed over the channels axis).</p> </dd> <dt>
<code>func_kw : dict, optional</code> </dt> <dd>
<p class="first last">Additional keyword arguments to supply to <code>func</code>.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>avg_y : np.ndarray</code> </dt> <dd>
<p class="first last">The output of <code>func(x, **func_kw)</code> averaged over all combinations of the specified axis shifts.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>Cycle spinning was proposed as a way to approach shift-invariance via performing several circular shifts of a shift-variant transform <a class="reference internal" href="#r67eed921dbd3-1" id="id40">[1]</a>.</p> <p>For a n-level discrete wavelet transforms, one may wish to perform all shifts up to <code>max_shifts = 2**n - 1</code>. In practice, much of the benefit can often be realized with only a small number of shifts per axis.</p> <p>For transforms such as the blockwise discrete cosine transform, one may wish to evaluate shifts up to the block size used by the transform.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r67eed921dbd3-1" rules="none">   <tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="#id40">1</a>, <a class="fn-backref" href="#id41">2</a>)</em> R.R. Coifman and D.L. Donoho. “Translation-Invariant De-Noising”. Wavelets and Statistics, Lecture Notes in Statistics, vol.103. Springer, New York, 1995, pp.125-150. DOI:10.1007/978-1-4612-2544-7_9</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import skimage.data
&gt;&gt;&gt; from skimage import img_as_float
&gt;&gt;&gt; from skimage.restoration import denoise_wavelet, cycle_spin
&gt;&gt;&gt; img = img_as_float(skimage.data.camera())
&gt;&gt;&gt; sigma = 0.1
&gt;&gt;&gt; img = img + sigma * np.random.standard_normal(img.shape)
&gt;&gt;&gt; denoised = cycle_spin(img, func=denoise_wavelet, max_shifts=3)
</pre> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2011 the scikit-image team<br>Licensed under the BSD 3-clause License.<br>
    <a href="http://scikit-image.org/docs/0.14.x/api/skimage.restoration.html" class="_attribution-link">http://scikit-image.org/docs/0.14.x/api/skimage.restoration.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
