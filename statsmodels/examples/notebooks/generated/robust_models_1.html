
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Example&#58; M-Estimators for Robust Linear Modeling - Statsmodels - W3cubDocs</title>
  
  <meta name="description" content=" &#36;&#36;Q(e_i, &#92;rho) &#61; &#92;sum_i~&#92;rho &#92;left (&#92;frac{e_i}{s}&#92;right )&#36;&#36; ">
  <meta name="keywords" content="m-estimators, for, robust, linear, modeling, example, statsmodels">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/statsmodels/examples/notebooks/generated/robust_models_1.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/statsmodels.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/statsmodels/" class="_nav-link" title="" style="margin-left:0;">Statsmodels</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="M-Estimators-for-Robust-Linear-Modeling">M-Estimators for Robust Linear Modeling</h1>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">%matplotlib inline

from __future__ import print_function
from statsmodels.compat import lmap
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

import statsmodels.api as sm
</pre>   </div>     <ul> <li>An M-estimator minimizes the function </li> </ul> <p>$$Q(e_i, \rho) = \sum_i~\rho \left (\frac{e_i}{s}\right )$$</p> <p>where $\rho$ is a symmetric function of the residuals</p> <ul> <li>The effect of $\rho$ is to reduce the influence of outliers</li> <li>$s$ is an estimate of scale. </li> <li>The robust estimates $\hat{\beta}$ are computed by the iteratively re-weighted least squares algorithm</li> </ul>       <ul> <li>We have several choices available for the weighting functions to be used</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">norms = sm.robust.norms
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">def plot_weights(support, weights_func, xlabels, xticks):
    fig = plt.figure(figsize=(12,8))
    ax = fig.add_subplot(111)
    ax.plot(support, weights_func(support))
    ax.set_xticks(xticks)
    ax.set_xticklabels(xlabels, fontsize=16)
    ax.set_ylim(-.1, 1.1)
    return ax
</pre>   </div>     <h3 id="Andrew's-Wave">Andrew's Wave</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">help(norms.AndrewWave.weights)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">a = 1.339
support = np.linspace(-np.pi*a, np.pi*a, 100)
andrew = norms.AndrewWave(a=a)
plot_weights(support, andrew.weights, ['$-\pi*a$', '0', '$\pi*a$'], [-np.pi*a, 0, np.pi*a]);
</pre>   </div>     <h3 id="Hampel's-17A">Hampel's 17A</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">help(norms.Hampel.weights)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">c = 8
support = np.linspace(-3*c, 3*c, 1000)
hampel = norms.Hampel(a=2., b=4., c=c)
plot_weights(support, hampel.weights, ['3*c', '0', '3*c'], [-3*c, 0, 3*c]);
</pre>   </div>     <h3 id="Huber's-t">Huber's t</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">help(norms.HuberT.weights)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">t = 1.345
support = np.linspace(-3*t, 3*t, 1000)
huber = norms.HuberT(t=t)
plot_weights(support, huber.weights, ['-3*t', '0', '3*t'], [-3*t, 0, 3*t]);
</pre>   </div>     <h3 id="Least-Squares">Least Squares</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">help(norms.LeastSquares.weights)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">support = np.linspace(-3, 3, 1000)
lst_sq = norms.LeastSquares()
plot_weights(support, lst_sq.weights, ['-3', '0', '3'], [-3, 0, 3]);
</pre>   </div>     <h3 id="Ramsay's-Ea">Ramsay's Ea</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">help(norms.RamsayE.weights)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">a = .3
support = np.linspace(-3*a, 3*a, 1000)
ramsay = norms.RamsayE(a=a)
plot_weights(support, ramsay.weights, ['-3*a', '0', '3*a'], [-3*a, 0, 3*a]);
</pre>   </div>     <h3 id="Trimmed-Mean">Trimmed Mean</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">help(norms.TrimmedMean.weights)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">c = 2
support = np.linspace(-3*c, 3*c, 1000)
trimmed = norms.TrimmedMean(c=c)
plot_weights(support, trimmed.weights, ['-3*c', '0', '3*c'], [-3*c, 0, 3*c]);
</pre>   </div>     <h3 id="Tukey's-Biweight">Tukey's Biweight</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">help(norms.TukeyBiweight.weights)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">c = 4.685
support = np.linspace(-3*c, 3*c, 1000)
tukey = norms.TukeyBiweight(c=c)
plot_weights(support, tukey.weights, ['-3*c', '0', '3*c'], [-3*c, 0, 3*c]);
</pre>   </div>     <h3 id="Scale-Estimators">Scale Estimators</h3>       <ul> <li>Robust estimates of the location</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">x = np.array([1, 2, 3, 4, 500])
</pre>   </div>     <ul> <li>The mean is not a robust estimator of location</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">x.mean()
</pre>   </div>     <ul> <li>The median, on the other hand, is a robust estimator with a breakdown point of 50%</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">np.median(x)
</pre>   </div>     <ul> <li>Analagously for the scale</li> <li>The standard deviation is not robust</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">x.std()
</pre>   </div>     <p>Median Absolute Deviation</p> <p>$$ median_i |X_i - median_j(X_j)|) $$</p>       <p>Standardized Median Absolute Deviation is a consistent estimator for $\hat{\sigma}$</p> <p>$$\hat{\sigma}=K \cdot MAD$$</p> <p>where $K$ depends on the distribution. For the normal distribution for example,</p> <p>$$K = \Phi^{-1}(.75)$$</p>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">stats.norm.ppf(.75)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(x)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">sm.robust.scale.mad(x)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">np.array([1,2,3,4,5.]).std()
</pre>   </div>     <ul> <li>The default for Robust Linear Models is MAD</li> <li>another popular choice is Huber's proposal 2</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">np.random.seed(12345)
fat_tails = stats.t(6).rvs(40)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">kde = sm.nonparametric.KDEUnivariate(fat_tails)
kde.fit()
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
ax.plot(kde.support, kde.density);
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(fat_tails.mean(), fat_tails.std())
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(stats.norm.fit(fat_tails))
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(stats.t.fit(fat_tails, f0=6))
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">huber = sm.robust.scale.Huber()
loc, scale = huber(fat_tails)
print(loc, scale)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">sm.robust.mad(fat_tails)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">sm.robust.mad(fat_tails, c=stats.t(6).ppf(.75))
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">sm.robust.scale.mad(fat_tails)
</pre>   </div>     <h3 id="Duncan's-Occupational-Prestige-data---M-estimation-for-outliers">Duncan's Occupational Prestige data - M-estimation for outliers</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">from statsmodels.graphics.api import abline_plot
from statsmodels.formula.api import ols, rlm
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">prestige = sm.datasets.get_rdataset("Duncan", "car", cache=True).data
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(prestige.head(10))
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">fig = plt.figure(figsize=(12,12))
ax1 = fig.add_subplot(211, xlabel='Income', ylabel='Prestige')
ax1.scatter(prestige.income, prestige.prestige)
xy_outlier = prestige.loc['minister', ['income','prestige']]
ax1.annotate('Minister', xy_outlier, xy_outlier+1, fontsize=16)
ax2 = fig.add_subplot(212, xlabel='Education',
                           ylabel='Prestige')
ax2.scatter(prestige.education, prestige.prestige);
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">ols_model = ols('prestige ~ income + education', prestige).fit()
print(ols_model.summary())
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">infl = ols_model.get_influence()
student = infl.summary_frame()['student_resid']
print(student)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(student.loc[np.abs(student) &gt; 2])
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(infl.summary_frame().loc['minister'])
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">sidak = ols_model.outlier_test('sidak')
sidak.sort_values('unadj_p', inplace=True)
print(sidak)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">fdr = ols_model.outlier_test('fdr_bh')
fdr.sort_values('unadj_p', inplace=True)
print(fdr)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">rlm_model = rlm('prestige ~ income + education', prestige).fit()
print(rlm_model.summary())
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(rlm_model.weights)
</pre>   </div>     <h3 id="Hertzprung-Russell-data-for-Star-Cluster-CYG-0B1---Leverage-Points">Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points</h3>       <ul> <li>Data is on the luminosity and temperature of 47 stars in the direction of Cygnus.</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">dta = sm.datasets.get_rdataset("starsCYG", "robustbase", cache=True).data
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">from matplotlib.patches import Ellipse
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111, xlabel='log(Temp)', ylabel='log(Light)', title='Hertzsprung-Russell Diagram of Star Cluster CYG OB1')
ax.scatter(*dta.values.T)
# highlight outliers
e = Ellipse((3.5, 6), .2, 1, alpha=.25, color='r')
ax.add_patch(e);
ax.annotate('Red giants', xy=(3.6, 6), xytext=(3.8, 6),
            arrowprops=dict(facecolor='black', shrink=0.05, width=2),
            horizontalalignment='left', verticalalignment='bottom',
            clip_on=True, # clip to the axes bounding box
            fontsize=16,
     )
# annotate these with their index
for i,row in dta.loc[dta['log.Te'] &lt; 3.8].iterrows():
    ax.annotate(i, row, row + .01, fontsize=14)
xlim, ylim = ax.get_xlim(), ax.get_ylim()
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">from IPython.display import Image
Image(filename='star_diagram.png')
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">y = dta['log.light']
X = sm.add_constant(dta['log.Te'], prepend=True)
ols_model = sm.OLS(y, X).fit()
abline_plot(model_results=ols_model, ax=ax)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">rlm_mod = sm.RLM(y, X, sm.robust.norms.TrimmedMean(.5)).fit()
abline_plot(model_results=rlm_mod, ax=ax, color='red')
</pre>   </div>     <ul> <li>Why? Because M-estimators are not robust to leverage points.</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">infl = ols_model.get_influence()
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">h_bar = 2*(ols_model.df_model + 1 )/ols_model.nobs
hat_diag = infl.summary_frame()['hat_diag']
hat_diag.loc[hat_diag &gt; h_bar]
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">sidak2 = ols_model.outlier_test('sidak')
sidak2.sort_values('unadj_p', inplace=True)
print(sidak2)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">fdr2 = ols_model.outlier_test('fdr_bh')
fdr2.sort_values('unadj_p', inplace=True)
print(fdr2)
</pre>   </div>     <ul> <li>Let's delete that line</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">l =  ax.lines[-1]
l.remove()
del l
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">weights = np.ones(len(X))
weights[X[X['log.Te'] &lt; 3.8].index.values - 1] = 0
wls_model = sm.WLS(y, X, weights=weights).fit()
abline_plot(model_results=wls_model, ax=ax, color='green')
</pre>   </div>     <ul> <li>MM estimators are good for this type of problem, unfortunately, we don't yet have these yet. </li> <li>It's being worked on, but it gives a good excuse to look at the R cell magics in the notebook.</li> </ul>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">yy = y.values[:,None]
xx = X['log.Te'].values[:,None]
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">%load_ext rpy2.ipython
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">%R library(robustbase)
%Rpush yy xx
%R mod &lt;- lmrob(yy ~ xx);
%R params &lt;- mod$coefficients;
%Rpull params
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">%R print(mod)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">print(params)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">abline_plot(intercept=params[0], slope=params[1], ax=ax, color='red')
</pre>   </div>     <h3 id="Exercise:-Breakdown-points-of-M-estimator">Exercise: Breakdown points of M-estimator</h3>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">np.random.seed(12345)
nobs = 200
beta_true = np.array([3, 1, 2.5, 3, -4])
X = np.random.uniform(-20,20, size=(nobs, len(beta_true)-1))
# stack a constant in front
X = sm.add_constant(X, prepend=True) # np.c_[np.ones(nobs), X]
mc_iter = 500
contaminate = .25 # percentage of response variables to contaminate
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">all_betas = []
for i in range(mc_iter):
    y = np.dot(X, beta_true) + np.random.normal(size=200)
    random_idx = np.random.randint(0, nobs, size=int(contaminate * nobs))
    y[random_idx] = np.random.uniform(-750, 750)
    beta_hat = sm.RLM(y, X).fit().params
    all_betas.append(beta_hat)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">all_betas = np.asarray(all_betas)
se_loss = lambda x : np.linalg.norm(x, ord=2)**2
se_beta = lmap(se_loss, all_betas - beta_true)
</pre>   </div>     <h4 id="Squared-error-loss">Squared error loss</h4>     <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">np.array(se_beta).mean()
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">all_betas.mean(0)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">beta_true
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [ ]:</div>   <pre data-language="python">se_loss(all_betas.mean(0) - beta_true)
</pre>   </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2009–2012 Statsmodels Developers<br>© 2006–2008 Scipy Developers<br>© 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/examples/notebooks/generated/robust_models_1.html" class="_attribution-link">http://www.statsmodels.org/stable/examples/notebooks/generated/robust_models_1.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
