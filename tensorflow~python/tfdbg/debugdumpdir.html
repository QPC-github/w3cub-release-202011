
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tfdbg.DebugDumpDir - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Defined in tensorflow&#47;python&#47;debug&#47;lib&#47;debug_data.py. ">
  <meta name="keywords" content="tfdbg, debugdumpdir, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/tfdbg/debugdumpdir.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/tensorflow~python.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> tfdbg.DebugDumpDir </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tfdbg.DebugDumpDir"> <meta itemprop="path" content="r1.8"> <meta itemprop="property" content="core_metadata"> <meta itemprop="property" content="dumped_tensor_data"> <meta itemprop="property" content="python_graph"> <meta itemprop="property" content="run_feed_keys_info"> <meta itemprop="property" content="run_fetches_info"> <meta itemprop="property" content="size"> <meta itemprop="property" content="t0"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="debug_watch_keys"> <meta itemprop="property" content="devices"> <meta itemprop="property" content="find"> <meta itemprop="property" content="find_some_path"> <meta itemprop="property" content="get_dump_sizes_bytes"> <meta itemprop="property" content="get_rel_timestamps"> <meta itemprop="property" content="get_tensor_file_paths"> <meta itemprop="property" content="get_tensors"> <meta itemprop="property" content="loaded_partition_graphs"> <meta itemprop="property" content="node_attributes"> <meta itemprop="property" content="node_device"> <meta itemprop="property" content="node_exists"> <meta itemprop="property" content="node_inputs"> <meta itemprop="property" content="node_op_type"> <meta itemprop="property" content="node_recipients"> <meta itemprop="property" content="node_traceback"> <meta itemprop="property" content="nodes"> <meta itemprop="property" content="partition_graphs"> <meta itemprop="property" content="reconstructed_non_debug_partition_graphs"> <meta itemprop="property" content="set_python_graph"> <meta itemprop="property" content="transitive_inputs"> <meta itemprop="property" content="watch_key_to_data"> </div> <h2 id="class_debugdumpdir">Class <code>DebugDumpDir</code>
</h2> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/debug/lib/debug_data.py"><code>tensorflow/python/debug/lib/debug_data.py</code></a>.</p> <p>See the guide: <a href="https://www.tensorflow.org/api_guides/python/tfdbg#Classes_for_debug_dump_data_and_directories">TensorFlow Debugger &gt; Classes for debug-dump data and directories</a></p> <p>Data set from a debug-dump directory on filesystem.</p> <p>An instance of <code>DebugDumpDir</code> contains all <code>DebugTensorDatum</code> instances in a tfdbg dump root directory.</p> <h2 id="properties">Properties</h2> <h3 id="core_metadata"><code>core_metadata</code></h3> <p>Metadata about the <code>Session.run()</code> call from the core runtime.</p> <p>Of the three counters available in the return value, <code>global_step</code> is supplied by the caller of the debugged <code>Session.run()</code>, while <code>session_run_index</code> and <code>executor_step_index</code> are determined by the state of the core runtime, automatically. For the same fetch list, feed keys and debug tensor watch options, the same executor will be used and <code>executor_step_index</code> should increase by one at a time. However, runs with different fetch lists, feed keys and debug_tensor watch options that all share the same <code>Session</code> object can lead to gaps in <code>session_run_index</code>.</p> <h4 id="returns">Returns:</h4> <p>If core metadata are loaded, a <code>namedtuple</code> with the fields: <code>global_step</code>: A global step count supplied by the caller of <code>Session.run()</code>. It is optional to the caller. If the caller did not supply this parameter, its value will be -1. <code>session_run_index</code>: A sorted index for Run() calls to the underlying TensorFlow <code>Session</code> object. <code>executor_step_index</code>: A counter for invocations of a given runtime executor. The same executor is re-used for the same fetched tensors, target nodes, input feed keys and debug tensor watch options. <code>input_names</code>: Names of the input (feed) Tensors. <code>output_names</code>: Names of the output (fetched) Tensors. <code>target_nodes</code>: Names of the target nodes. If the core metadata have not been loaded, <code>None</code>. If more than one core metadata files exist, return a list of the <code>nametuple</code> described above.</p> <h3 id="dumped_tensor_data"><code>dumped_tensor_data</code></h3> <p>Retrieve dumped tensor data.</p> <h3 id="python_graph"><code>python_graph</code></h3> <p>Get the Python graph.</p> <h4 id="returns_1">Returns:</h4> <p>If the Python graph has been set, returns a <a href="../tf/graph"><code>tf.Graph</code></a> object. Otherwise, returns None.</p> <h3 id="run_feed_keys_info"><code>run_feed_keys_info</code></h3> <p>Get a str representation of the feed_dict used in the Session.run() call.</p> <h4 id="returns_2">Returns:</h4> <p>If the information is available from one <code>Session.run</code> call, a <code>str</code> obtained from <code>repr(feed_dict)</code>. If the information is available from multiple <code>Session.run</code> calls, a <code>list</code> of <code>str</code> obtained from <code>repr(feed_dict)</code>. If the information is not available, <code>None</code>.</p> <h3 id="run_fetches_info"><code>run_fetches_info</code></h3> <p>Get a str representation of the fetches used in the Session.run() call.</p> <h4 id="returns_3">Returns:</h4> <p>If the information is available from one <code>Session.run</code> call, a <code>str</code> obtained from <code>repr(fetches)</code>. If the information is available from multiple <code>Session.run</code> calls, a <code>list</code> of <code>str</code> from <code>repr(fetches)</code>. If the information is not available, <code>None</code>.</p> <h3 id="size"><code>size</code></h3> <p>Total number of dumped tensors in the dump root directory.</p> <h4 id="returns_4">Returns:</h4> <p>(<code>int</code>) The total number of dumped tensors in the dump root directory.</p> <h3 id="t0"><code>t0</code></h3> <p>Absolute timestamp of the first dumped tensor across all devices.</p> <h4 id="returns_5">Returns:</h4> <p>(<code>int</code>) absolute timestamp of the first dumped tensor, in microseconds.</p> <h2 id="methods">Methods</h2> <h3 id="__init__"><code>__init__</code></h3> <pre class="prettyprint lang-python" data-language="python">__init__(
    dump_root,
    partition_graphs=None,
    validate=True
)
</pre> <p><code>DebugDumpDir</code> constructor.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>dump_root</code></b>: (<code>str</code>) path to the dump root directory.</li> <li>
<b><code>partition_graphs</code></b>: A repeated field of GraphDefs representing the partition graphs executed by the TensorFlow runtime.</li> <li>
<b><code>validate</code></b>: (<code>bool</code>) whether the dump files are to be validated against the partition graphs.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>IOError</code></b>: If dump_root does not exist as a directory.</li> <li>
<b><code>ValueError</code></b>: If more than one core metadata file is found under the dump root directory.</li> </ul> <h3 id="debug_watch_keys"><code>debug_watch_keys</code></h3> <pre class="prettyprint lang-python" data-language="python">debug_watch_keys(
    node_name,
    device_name=None
)
</pre> <p>Get all tensor watch keys of given node according to partition graphs.</p> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if node_name exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_6">Returns:</h4> <p>(<code>list</code> of <code>str</code>) all debug tensor watch keys. Returns an empty list if the node name does not correspond to any debug watch keys.</p> <h4 id="raises_1">Raises:</h4> <p><code>LookupError</code>: If debug watch information has not been loaded from partition graphs yet.</p> <h3 id="devices"><code>devices</code></h3> <pre class="prettyprint lang-python" data-language="python">devices()
</pre> <p>Get the list of device names.</p> <h4 id="returns_7">Returns:</h4> <p>(<code>list</code> of <code>str</code>) names of the devices.</p> <h3 id="find"><code>find</code></h3> <pre class="prettyprint lang-python" data-language="python">find(
    predicate,
    first_n=0,
    device_name=None,
    exclude_node_names=None
)
</pre> <p>Find dumped tensor data by a certain predicate.</p> <h4 id="args_2">Args:</h4> <ul> <li> <p><b><code>predicate</code></b>: A callable that takes two input arguments:</p> <p><code>python def predicate(debug_tensor_datum, tensor): # returns a bool</code></p> <p>where <code>debug_tensor_datum</code> is an instance of <code>DebugTensorDatum</code>, which carries the metadata, such as the <code>Tensor</code>'s node name, output slot timestamp, debug op name, etc.; and <code>tensor</code> is the dumped tensor value as a <code>numpy.ndarray</code>. <em> <b><code>first_n</code></b>: (<code>int</code>) return only the first n <code>DebugTensotDatum</code> instances (in time order) for which the predicate returns True. To return all the <code>DebugTensotDatum</code> instances, let first_n be &lt;= 0. </em> <b><code>device_name</code></b>: optional device name. * <b><code>exclude_node_names</code></b>: Optional regular expression to exclude nodes with names matching the regular expression.</p> </li> </ul> <h4 id="returns_8">Returns:</h4> <p>A list of all <code>DebugTensorDatum</code> objects in this <code>DebugDumpDir</code> object for which predicate returns True, sorted in ascending order of the timestamp.</p> <h3 id="find_some_path"><code>find_some_path</code></h3> <pre class="prettyprint lang-python" data-language="python">find_some_path(
    src_node_name,
    dst_node_name,
    include_control=True,
    include_reversed_ref=False,
    device_name=None
)
</pre> <p>Find a path between a source node and a destination node.</p> <p>Limitation: the source and destination are required to be on the same device, i.e., this method does not yet take into account Send/Recv nodes across devices.</p> <p>TODO(cais): Make this method work across device edges by tracing Send/Recv nodes.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>src_node_name</code></b>: (<code>str</code>) name of the source node or name of an output tensor of the node.</li> <li>
<b><code>dst_node_name</code></b>: (<code>str</code>) name of the destination node or name of an output tensor of the node.</li> <li>
<b><code>include_control</code></b>: (<code>bool</code>) whrther control edges are considered in the graph tracing.</li> <li>
<b><code>include_reversed_ref</code></b>: Whether a ref input, say from A to B, is to be also considered as an input from B to A. The rationale is that ref inputs generally let the recipient (e.g., B in this case) mutate the value of the source (e.g., A in this case). So the reverse direction of the ref edge reflects the direction of information flow.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if node_name exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_9">Returns:</h4> <p>A path from the src_node_name to dst_node_name, as a <code>list</code> of <code>str</code>, if it exists. The list includes src_node_name as the first item and dst_node_name as the last. If such a path does not exist, <code>None</code>.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If the source and destination nodes are not on the same device.</li> </ul> <h3 id="get_dump_sizes_bytes"><code>get_dump_sizes_bytes</code></h3> <pre class="prettyprint lang-python" data-language="python">get_dump_sizes_bytes(
    node_name,
    output_slot,
    debug_op,
    device_name=None
)
</pre> <p>Get the sizes of the dump files for a debug-dumped tensor.</p> <p>Unit of the file size: byte.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node that the tensor is produced by.</li> <li>
<b><code>output_slot</code></b>: (<code>int</code>) output slot index of tensor.</li> <li>
<b><code>debug_op</code></b>: (<code>str</code>) name of the debug op.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if the specified debug_watch_key exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_10">Returns:</h4> <p>(<code>list</code> of <code>int</code>): list of dump file sizes in bytes.</p> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code>WatchKeyDoesNotExistInDebugDumpDirError</code></b>: If the tensor watch key does not exist in the debug dump data.</li> </ul> <h3 id="get_rel_timestamps"><code>get_rel_timestamps</code></h3> <pre class="prettyprint lang-python" data-language="python">get_rel_timestamps(
    node_name,
    output_slot,
    debug_op,
    device_name=None
)
</pre> <p>Get the relative timestamp from for a debug-dumped tensor.</p> <p>Relative timestamp means (absolute timestamp - <code>t0</code>), where <code>t0</code> is the absolute timestamp of the first dumped tensor in the dump root. The tensor may be dumped multiple times in the dump root directory, so a list of relative timestamps (<code>numpy.ndarray</code>) is returned.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node that the tensor is produced by.</li> <li>
<b><code>output_slot</code></b>: (<code>int</code>) output slot index of tensor.</li> <li>
<b><code>debug_op</code></b>: (<code>str</code>) name of the debug op.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if the specified debug_watch_key exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_11">Returns:</h4> <p>(<code>list</code> of <code>int</code>) list of relative timestamps.</p> <h4 id="raises_4">Raises:</h4> <ul> <li>
<b><code>WatchKeyDoesNotExistInDebugDumpDirError</code></b>: If the tensor watch key does not exist in the debug dump data.</li> </ul> <h3 id="get_tensor_file_paths"><code>get_tensor_file_paths</code></h3> <pre class="prettyprint lang-python" data-language="python">get_tensor_file_paths(
    node_name,
    output_slot,
    debug_op,
    device_name=None
)
</pre> <p>Get the file paths from a debug-dumped tensor.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node that the tensor is produced by.</li> <li>
<b><code>output_slot</code></b>: (<code>int</code>) output slot index of tensor.</li> <li>
<b><code>debug_op</code></b>: (<code>str</code>) name of the debug op.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if the specified debug_watch_key exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_12">Returns:</h4> <p>List of file path(s) loaded. This is a list because each debugged tensor may be dumped multiple times.</p> <h4 id="raises_5">Raises:</h4> <ul> <li>
<b><code>WatchKeyDoesNotExistInDebugDumpDirError</code></b>: If the tensor does not exist in the debug-dump data.</li> </ul> <h3 id="get_tensors"><code>get_tensors</code></h3> <pre class="prettyprint lang-python" data-language="python">get_tensors(
    node_name,
    output_slot,
    debug_op,
    device_name=None
)
</pre> <p>Get the tensor value from for a debug-dumped tensor.</p> <p>The tensor may be dumped multiple times in the dump root directory, so a list of tensors (<code>numpy.ndarray</code>) is returned.</p> <h4 id="args_7">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node that the tensor is produced by.</li> <li>
<b><code>output_slot</code></b>: (<code>int</code>) output slot index of tensor.</li> <li>
<b><code>debug_op</code></b>: (<code>str</code>) name of the debug op.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if the specified debug_watch_key exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_13">Returns:</h4> <p>List of tensors (<code>numpy.ndarray</code>) loaded from the debug-dump file(s).</p> <h4 id="raises_6">Raises:</h4> <ul> <li>
<b><code>WatchKeyDoesNotExistInDebugDumpDirError</code></b>: If the tensor does not exist in the debug-dump data.</li> </ul> <h3 id="loaded_partition_graphs"><code>loaded_partition_graphs</code></h3> <pre class="prettyprint lang-python" data-language="python">loaded_partition_graphs()
</pre> <p>Test whether partition graphs have been loaded.</p> <h3 id="node_attributes"><code>node_attributes</code></h3> <pre class="prettyprint lang-python" data-language="python">node_attributes(
    node_name,
    device_name=None
)
</pre> <p>Get the attributes of a node.</p> <h4 id="args_8">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: Name of the node in question.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if node_name exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_14">Returns:</h4> <p>Attributes of the node.</p> <h4 id="raises_7">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If no partition graphs have been loaded.</li> </ul> <h3 id="node_device"><code>node_device</code></h3> <pre class="prettyprint lang-python" data-language="python">node_device(node_name)
</pre> <p>Get the names of the devices that has nodes of the specified name.</p> <h4 id="args_9">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node.</li> </ul> <h4 id="returns_15">Returns:</h4> <p>(<code>str</code> or <code>list</code> of <code>str</code>) name of the device(s) on which the node of the given name is found. Returns a <code>str</code> if there is only one such device, otherwise return a <code>list</code> of <code>str</code>.</p> <h4 id="raises_8">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If node inputs and control inputs have not been loaded from partition graphs yet.</li> <li>
<b><code>ValueError</code></b>: If the node does not exist in partition graphs.</li> </ul> <h3 id="node_exists"><code>node_exists</code></h3> <pre class="prettyprint lang-python" data-language="python">node_exists(
    node_name,
    device_name=None
)
</pre> <p>Test if a node exists in the partition graphs.</p> <h4 id="args_10">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node to be checked.</li> <li>
<b><code>device_name</code></b>: optional device name. If None, will search for the node on all available devices. Otherwise, search for the node only on the given device.</li> </ul> <h4 id="returns_16">Returns:</h4> <p>A boolean indicating whether the node exists.</p> <h4 id="raises_9">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If no partition graphs have been loaded yet.</li> <li>
<b><code>ValueError</code></b>: If device_name is specified but cannot be found.</li> </ul> <h3 id="node_inputs"><code>node_inputs</code></h3> <pre class="prettyprint lang-python" data-language="python">node_inputs(
    node_name,
    is_control=False,
    device_name=None
)
</pre> <p>Get the inputs of given node according to partition graphs.</p> <h4 id="args_11">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: Name of the node.</li> <li>
<b><code>is_control</code></b>: (<code>bool</code>) Whether control inputs, rather than non-control inputs, are to be returned.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if node_name exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_17">Returns:</h4> <p>(<code>list</code> of <code>str</code>) inputs to the node, as a list of node names.</p> <h4 id="raises_10">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If node inputs and control inputs have not been loaded from partition graphs yet.</li> </ul> <h3 id="node_op_type"><code>node_op_type</code></h3> <pre class="prettyprint lang-python" data-language="python">node_op_type(
    node_name,
    device_name=None
)
</pre> <p>Get the op type of given node.</p> <h4 id="args_12">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if node_name exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_18">Returns:</h4> <p>(<code>str</code>) op type of the node.</p> <h4 id="raises_11">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If node op types have not been loaded from partition graphs yet.</li> </ul> <h3 id="node_recipients"><code>node_recipients</code></h3> <pre class="prettyprint lang-python" data-language="python">node_recipients(
    node_name,
    is_control=False,
    device_name=None
)
</pre> <p>Get recipient of the given node's output according to partition graphs.</p> <h4 id="args_13">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: (<code>str</code>) name of the node.</li> <li>
<b><code>is_control</code></b>: (<code>bool</code>) whether control outputs, rather than non-control outputs, are to be returned.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if node_name exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_19">Returns:</h4> <p>(<code>list</code> of <code>str</code>) all inputs to the node, as a list of node names.</p> <h4 id="raises_12">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If node inputs and control inputs have not been loaded from partition graphs yet.</li> </ul> <h3 id="node_traceback"><code>node_traceback</code></h3> <pre class="prettyprint lang-python" data-language="python">node_traceback(element_name)
</pre> <p>Try to retrieve the Python traceback of node's construction.</p> <h4 id="args_14">Args:</h4> <ul> <li>
<b><code>element_name</code></b>: (<code>str</code>) Name of a graph element (node or tensor).</li> </ul> <h4 id="returns_20">Returns:</h4> <p>(list) The traceback list object as returned by the <code>extract_trace</code> method of Python's traceback module.</p> <h4 id="raises_13">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If Python graph is not available for traceback lookup.</li> <li>
<b><code>KeyError</code></b>: If the node cannot be found in the Python graph loaded.</li> </ul> <h3 id="nodes"><code>nodes</code></h3> <pre class="prettyprint lang-python" data-language="python">nodes(device_name=None)
</pre> <p>Get a list of all nodes from the partition graphs.</p> <h4 id="args_15">Args:</h4> <ul> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of device. If None, all nodes from all available devices will be included.</li> </ul> <h4 id="returns_21">Returns:</h4> <p>All nodes' names, as a list of str.</p> <h4 id="raises_14">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If no partition graphs have been loaded.</li> <li>
<b><code>ValueError</code></b>: If specified node name does not exist.</li> </ul> <h3 id="partition_graphs"><code>partition_graphs</code></h3> <pre class="prettyprint lang-python" data-language="python">partition_graphs()
</pre> <p>Get the partition graphs.</p> <h4 id="returns_22">Returns:</h4> <p>Partition graphs as a list of GraphDef.</p> <h4 id="raises_15">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If no partition graphs have been loaded.</li> </ul> <h3 id="reconstructed_non_debug_partition_graphs"><code>reconstructed_non_debug_partition_graphs</code></h3> <pre class="prettyprint lang-python" data-language="python">reconstructed_non_debug_partition_graphs()
</pre> <p>Reconstruct partition graphs with the debugger-inserted ops stripped.</p> <p>The reconstructed partition graphs are identical to the original (i.e., non-debugger-decorated) partition graphs except in the following respects: 1) The exact names of the runtime-inserted internal nodes may differ. These include _Send, _Recv, _HostSend, _HostRecv, _Retval ops. 2) As a consequence of 1, the nodes that receive input directly from such send- and recv-type ops will have different input names. 3) The parallel_iteration attribute of while-loop Enter ops are set to 1.</p> <h4 id="returns_23">Returns:</h4> <p>A dict mapping device names (<code>str</code>s) to reconstructed <a href="../tf/graphdef"><code>tf.GraphDef</code></a>s.</p> <h3 id="set_python_graph"><code>set_python_graph</code></h3> <pre class="prettyprint lang-python" data-language="python">set_python_graph(python_graph)
</pre> <p>Provide Python <code>Graph</code> object to the wrapper.</p> <p>Unlike the partition graphs, which are protobuf <code>GraphDef</code> objects, <code>Graph</code> is a Python object and carries additional information such as the traceback of the construction of the nodes in the graph.</p> <h4 id="args_16">Args:</h4> <ul> <li>
<b><code>python_graph</code></b>: (ops.Graph) The Python Graph object.</li> </ul> <h3 id="transitive_inputs"><code>transitive_inputs</code></h3> <pre class="prettyprint lang-python" data-language="python">transitive_inputs(
    node_name,
    include_control=True,
    include_reversed_ref=False,
    device_name=None
)
</pre> <p>Get the transitive inputs of given node according to partition graphs.</p> <h4 id="args_17">Args:</h4> <ul> <li>
<b><code>node_name</code></b>: Name of the node.</li> <li>
<b><code>include_control</code></b>: Include control inputs (True by default).</li> <li>
<b><code>include_reversed_ref</code></b>: Whether a ref input, say from A to B, is to be also considered as an input from B to A. The rationale is that ref inputs generally let the recipient (e.g., B in this case) mutate the value of the source (e.g., A in this case). So the reverse direction of the ref edge reflects the direction of information flow.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if node_name exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_24">Returns:</h4> <p>(<code>list</code> of <code>str</code>) all transitive inputs to the node, as a list of node names.</p> <h4 id="raises_16">Raises:</h4> <ul> <li>
<b><code>LookupError</code></b>: If node inputs and control inputs have not been loaded from partition graphs yet.</li> </ul> <h3 id="watch_key_to_data"><code>watch_key_to_data</code></h3> <pre class="prettyprint lang-python" data-language="python">watch_key_to_data(
    debug_watch_key,
    device_name=None
)
</pre> <p>Get all <code>DebugTensorDatum</code> instances corresponding to a debug watch key.</p> <h4 id="args_18">Args:</h4> <ul> <li>
<b><code>debug_watch_key</code></b>: (<code>str</code>) debug watch key.</li> <li>
<b><code>device_name</code></b>: (<code>str</code>) name of the device. If there is only one device or if the specified debug_watch_key exists on only one device, this argument is optional.</li> </ul> <h4 id="returns_25">Returns:</h4> <p>A list of <code>DebugTensorDatum</code> instances that correspond to the debug watch key. If the watch key does not exist, returns an empty list.</p> <h4 id="raises_17">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If there are multiple devices that have the debug_watch_key, but device_name is not specified.</li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    © 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tfdbg/DebugDumpDir" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tfdbg/DebugDumpDir</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
