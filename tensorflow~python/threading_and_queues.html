
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Threading and Queues - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Multithreaded queues are a powerful and widely used mechanism supporting asynchronous computation. ">
  <meta name="keywords" content="threading, and, queues, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/threading_and_queues.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> Threading and Queues </h1>     <blockquote class="note">
<strong>Note:</strong><span> In versions of TensorFlow before 1.2, we recommended using multi-threaded, queue-based input pipelines for performance. Beginning with TensorFlow 1.4, however, we recommend using the <a href="https://www.tensorflow.org/api_docs/python/tf/data"><code>tf.data</code></a> module instead. (See <a href="https://www.tensorflow.org/programmers_guide/datasets">Datasets</a> for details. In TensorFlow 1.2 and 1.3, the module was called <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/data"><code>tf.contrib.data</code></a>.) The <a href="https://www.tensorflow.org/api_docs/python/tf/data"><code>tf.data</code></a> module offers an easier-to-use interface for constructing efficient input pipelines. Furthermore, we've stopped developing the old multi-threaded, queue-based input pipelines. We've retained the documentation in this file to help developers who are still maintaining older code.</span>
</blockquote> <p>Multithreaded queues are a powerful and widely used mechanism supporting asynchronous computation.</p> <p>Following the <a href="graphs">dataflow programming model</a>, TensorFlow's queues are implemented using nodes in the computation graph. A queue is a stateful node, like a variable: other nodes can modify its content. In particular, nodes can enqueue new items in to the queue, or dequeue existing items from the queue. TensorFlow's queues provide a way to coordinate multiple steps of a computation: a queue will <strong>block</strong> any step that attempts to dequeue from it when it is empty, or enqueue to it when it is full. When that condition no longer holds, the queue will unblock the step and allow execution to proceed.</p> <p>TensorFlow implements several classes of queue. The principal difference between these classes is the order that items are removed from the queue. To get a feel for queues, let's consider a simple example. We will create a "first in, first out" queue (<a href="https://www.tensorflow.org/api_docs/python/tf/FIFOQueue"><code>tf.FIFOQueue</code></a>) and fill it with zeros. Then we'll construct a graph that takes an item off the queue, adds one to that item, and puts it back on the end of the queue. Slowly, the numbers on the queue increase.</p> <div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;"> <img style="width:100%" src="https://www.tensorflow.org/images/IncremeterFifoQueue.gif"> </div> <p><code>Enqueue</code>, <code>EnqueueMany</code>, and <code>Dequeue</code> are special nodes. They take a pointer to the queue instead of a normal value, allowing them to mutate its state. We recommend that you think of these operations as being like methods of the queue in an object-oriented sense. In fact, in the Python API, these operations are created by calling methods on a queue object (e.g. <code>q.enqueue(...)</code>).</p> <blockquote class="note">
<strong>Note:</strong><span> Queue methods (such as <code>q.enqueue(...)</code>) <em>must</em> run on the same device as the queue. Incompatible device placement directives will be ignored when creating these operations.</span>
</blockquote> <p>Now that you have a bit of a feel for queues, let's dive into the details...</p> <h2 id="Queue_usage_overview">Queue usage overview</h2> <p>Queues, such as <a href="https://www.tensorflow.org/api_docs/python/tf/FIFOQueue"><code>tf.FIFOQueue</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue"><code>tf.RandomShuffleQueue</code></a>, are important TensorFlow objects that aid in computing tensors asynchronously in a graph.</p> <p>For example, a typical queue-based input pipeline uses a <code>RandomShuffleQueue</code> to prepare inputs for training a model as follows:</p> <ul> <li>Multiple threads prepare training examples and enqueue them.</li> <li>A training thread executes a training op that dequeues mini-batches from the queue</li> </ul> <p>We recommend using the <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"><code>shuffle</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch"><code>batch</code></a> methods of a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code>Dataset</code></a> to accomplish this. However, if you'd prefer to use a queue-based version instead, you can find a full implementation in the <a href="https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch"><code>tf.train.shuffle_batch</code></a> function.</p> <p>For demonstration purposes a simplified implementation is given below.</p> <p>This function takes a source tensor, a capacity, and a batch size as arguments and returns a tensor that dequeues a shuffled batch when executed.</p> <pre class="prettyprint lang-python" data-language="python">def simple_shuffle_batch(source, capacity, batch_size=10):
  # Create a random shuffle queue.
  queue = tf.RandomShuffleQueue(capacity=capacity,
                                min_after_dequeue=int(0.9*capacity),
                                shapes=source.shape, dtypes=source.dtype)

  # Create an op to enqueue one item.
  enqueue = queue.enqueue(source)

  # Create a queue runner that, when started, will launch 4 threads applying
  # that enqueue op.
  num_threads = 4
  qr = tf.train.QueueRunner(queue, [enqueue] * num_threads)

  # Register the queue runner so it can be found and started by
  # &lt;a href="../../api_docs/python/tf/train/start_queue_runners"&gt;&lt;code&gt;tf.train.start_queue_runners&lt;/code&gt;&lt;/a&gt; later (the threads are not launched yet).
  tf.train.add_queue_runner(qr)

  # Create an op to dequeue a batch
  return queue.dequeue_many(batch_size)
</pre> <p>Once started by <a href="https://www.tensorflow.org/api_docs/python/tf/train/start_queue_runners"><code>tf.train.start_queue_runners</code></a>, or indirectly through <a href="https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession"><code>tf.train.MonitoredSession</code></a>, the <code>QueueRunner</code> will launch the threads in the background to fill the queue. Meanwhile the main thread will execute the <code>dequeue_many</code> op to pull data from it. Note how these ops do not depend on each other, except indirectly through the internal state of the queue.</p> <p>The simplest possible use of this function might be something like this:</p> <pre class="prettyprint lang-python" data-language="python"># create a dataset that counts from 0 to 99
input = tf.constant(list(range(100)))
input = tf.data.Dataset.from_tensor_slices(input)
input = input.make_one_shot_iterator().get_next()

# Create a slightly shuffled batch from the sorted elements
get_batch = simple_shuffle_batch(input, capacity=20)

# `MonitoredSession` will start and manage the `QueueRunner` threads.
with tf.train.MonitoredSession() as sess:
  # Since the `QueueRunners` have been started, data is available in the
  # queue, so the `sess.run(get_batch)` call will not hang.
  while not sess.should_stop():
    print(sess.run(get_batch))
</pre> <pre class="prettyprint" data-language="python">[ 8 10  7  5  4 13 15 14 25  0]
[23 29 28 31 33 18 19 11 34 27]
[12 21 37 39 35 22 44 36 20 46]
...
</pre> <p>For most use cases, the automatic thread startup and management provided by <a href="https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession"><code>tf.train.MonitoredSession</code></a> is sufficient. In the rare case that it is not, TensorFlow provides tools for manually managing your threads and queues.</p> <h2 id="Manual_Thread_Management">Manual Thread Management</h2> <p>As we have seen, the TensorFlow <code>Session</code> object is multithreaded and thread-safe, so multiple threads can easily use the same session and run ops in parallel. However, it is not always easy to implement a Python program that drives threads as required. All threads must be able to stop together, exceptions must be caught and reported, and queues must be properly closed when stopping.</p> <p>TensorFlow provides two classes to help: <a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator"><code>tf.train.Coordinator</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/train/QueueRunner"><code>tf.train.QueueRunner</code></a>. These two classes are designed to be used together. The <code>Coordinator</code> class helps multiple threads stop together and report exceptions to a program that waits for them to stop. The <code>QueueRunner</code> class is used to create a number of threads cooperating to enqueue tensors in the same queue.</p> <h3 id="coordinator">Coordinator</h3> <p>The <a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator"><code>tf.train.Coordinator</code></a> class manages background threads in a TensorFlow program and helps multiple threads stop together.</p> <p>Its key methods are:</p> <ul> <li>
<a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator#should_stop"><code>tf.train.Coordinator.should_stop</code></a>: returns <code>True</code> if the threads should stop.</li> <li>
<a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator#request_stop"><code>tf.train.Coordinator.request_stop</code></a>: requests that threads should stop.</li> <li>
<a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator#join"><code>tf.train.Coordinator.join</code></a>: waits until the specified threads have stopped.</li> </ul> <p>You first create a <code>Coordinator</code> object, and then create a number of threads that use the coordinator. The threads typically run loops that stop when <code>should_stop()</code> returns <code>True</code>.</p> <p>Any thread can decide that the computation should stop. It only has to call <code>request_stop()</code> and the other threads will stop as <code>should_stop()</code> will then return <code>True</code>.</p> <pre class="prettyprint lang-python" data-language="python"># Using Python's threading library.
import threading

# Thread body: loop until the coordinator indicates a stop was requested.
# If some condition becomes true, ask the coordinator to stop.
def MyLoop(coord):
  while not coord.should_stop():
    ...do something...
    if ...some condition...:
      coord.request_stop()

# Main thread: create a coordinator.
coord = tf.train.Coordinator()

# Create 10 threads that run 'MyLoop()'
threads = [threading.Thread(target=MyLoop, args=(coord,)) for i in xrange(10)]

# Start the threads and wait for all of them to stop.
for t in threads:
  t.start()
coord.join(threads)
</pre> <p>Obviously, the coordinator can manage threads doing very different things. They don't have to be all the same as in the example above. The coordinator also has support to capture and report exceptions. See the <a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator"><code>tf.train.Coordinator</code></a> documentation for more details.</p> <h3 id="queuerunner">QueueRunner</h3> <p>The <a href="https://www.tensorflow.org/api_docs/python/tf/train/QueueRunner"><code>tf.train.QueueRunner</code></a> class creates a number of threads that repeatedly run an enqueue op. These threads can use a coordinator to stop together. In addition, a queue runner will run a <em>closer operation</em> that closes the queue if an exception is reported to the coordinator.</p> <p>You can use a queue runner to implement the architecture described above.</p> <p>First build a graph that uses a TensorFlow queue (e.g. a <a href="https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue"><code>tf.RandomShuffleQueue</code></a>) for input examples. Add ops that process examples and enqueue them in the queue. Add training ops that start by dequeueing from the queue.</p> <pre class="prettyprint lang-python" data-language="python">example = ...ops to create one example...
# Create a queue, and an op that enqueues examples one at a time in the queue.
queue = tf.RandomShuffleQueue(...)
enqueue_op = queue.enqueue(example)
# Create a training graph that starts by dequeueing a batch of examples.
inputs = queue.dequeue_many(batch_size)
train_op = ...use 'inputs' to build the training part of the graph...
</pre> <p>In the Python training program, create a <code>QueueRunner</code> that will run a few threads to process and enqueue examples. Create a <code>Coordinator</code> and ask the queue runner to start its threads with the coordinator. Write a training loop that also uses the coordinator.</p> <pre class="prettyprint lang-python" data-language="python"># Create a queue runner that will run 4 threads in parallel to enqueue
# examples.
qr = tf.train.QueueRunner(queue, [enqueue_op] * 4)

# Launch the graph.
sess = tf.Session()
# Create a coordinator, launch the queue runner threads.
coord = tf.train.Coordinator()
enqueue_threads = qr.create_threads(sess, coord=coord, start=True)
# Run the training loop, controlling termination with the coordinator.
for step in xrange(1000000):
  if coord.should_stop():
    break
  sess.run(train_op)
# When done, ask the threads to stop.
coord.request_stop()
# And wait for them to actually do it.
coord.join(enqueue_threads)
</pre> <h3 id="handling_exceptions">Handling exceptions</h3> <p>Threads started by queue runners do more than just run the enqueue ops. They also catch and handle exceptions generated by queues, including the <a href="https://www.tensorflow.org/api_docs/python/tf/errors/OutOfRangeError"><code>tf.errors.OutOfRangeError</code></a> exception, which is used to report that a queue was closed.</p> <p>A training program that uses a coordinator must similarly catch and report exceptions in its main loop.</p> <p>Here is an improved version of the training loop above.</p> <pre class="prettyprint lang-python" data-language="python">try:
  for step in xrange(1000000):
    if coord.should_stop():
      break
    sess.run(train_op)
except Exception, e:
  # Report exceptions to the coordinator.
  coord.request_stop(e)
finally:
  # Terminate as usual. It is safe to call `coord.request_stop()` twice.
  coord.request_stop()
  coord.join(threads)
</pre>
<div class="_attribution">
  <p class="_attribution-p">
    © 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_guides/python/threading_and_queues" class="_attribution-link">https://www.tensorflow.org/api_guides/python/threading_and_queues</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
