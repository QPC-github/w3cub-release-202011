
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>contrib.distributions.bijectors.MaskedAutoregressiveFlow - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Inherits From&#58; Bijector ">
  <meta name="keywords" content="tf, contrib, distributions, bijectors, maskedautoregressiveflow, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/tf/contrib/distributions/bijectors/maskedautoregressiveflow.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow"> <meta itemprop="path" content="r1.8"> <meta itemprop="property" content="dtype"> <meta itemprop="property" content="event_ndims"> <meta itemprop="property" content="graph_parents"> <meta itemprop="property" content="is_constant_jacobian"> <meta itemprop="property" content="name"> <meta itemprop="property" content="validate_args"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="forward"> <meta itemprop="property" content="forward_event_shape"> <meta itemprop="property" content="forward_event_shape_tensor"> <meta itemprop="property" content="forward_log_det_jacobian"> <meta itemprop="property" content="inverse"> <meta itemprop="property" content="inverse_event_shape"> <meta itemprop="property" content="inverse_event_shape_tensor"> <meta itemprop="property" content="inverse_log_det_jacobian"> </div> <h2 id="class_maskedautoregressiveflow">Class <code>MaskedAutoregressiveFlow</code>
</h2> <p>Inherits From: <a href="../../../distributions/bijectors/bijector"><code>Bijector</code></a></p> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py"><code>tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py</code></a>.</p> <p>Affine MaskedAutoregressiveFlow bijector for vector-valued events.</p> <p>The affine autoregressive flow [(Papamakarios et al., 2016)][3] provides a relatively simple framework for user-specified (deep) architectures to learn a distribution over vector-valued events. Regarding terminology,</p> <p>"Autoregressive models decompose the joint density as a product of conditionals, and model each conditional in turn. Normalizing flows transform a base density (e.g. a standard Gaussian) into the target density by an invertible transformation with tractable Jacobian." [(Papamakarios et al., 2016)][3]</p> <p>In other words, the "autoregressive property" is equivalent to the decomposition, <code>p(x) = prod{ p(x[i] | x[0:i]) : i=0, ..., d }</code>. The provided <code>shift_and_log_scale_fn</code>, <code>masked_autoregressive_default_template</code>, achieves this property by zeroing out weights in its <code>masked_dense</code> layers.</p> <p>In the <a href="../../../distributions"><code>tf.distributions</code></a> framework, a "normalizing flow" is implemented as a <a href="../../../distributions/bijectors/bijector"><code>tf.distributions.bijectors.Bijector</code></a>. The <code>forward</code> "autoregression" is implemented using a <a href="../../../while_loop"><code>tf.while_loop</code></a> and a deep neural network (DNN) with masked weights such that the autoregressive property is automatically met in the <code>inverse</code>.</p> <p>A <code>TransformedDistribution</code> using <code>MaskedAutoregressiveFlow(...)</code> uses the (expensive) forward-mode calculation to draw samples and the (cheap) reverse-mode calculation to compute log-probabilities. Conversely, a <code>TransformedDistribution</code> using <code>Invert(MaskedAutoregressiveFlow(...))</code> uses the (expensive) forward-mode calculation to compute log-probabilities and the (cheap) reverse-mode calculation to compute samples. See "Example Use" [below] for more details.</p> <p>Given a <code>shift_and_log_scale_fn</code>, the forward and inverse transformations are (a sequence of) affine transformations. A "valid" <code>shift_and_log_scale_fn</code> must compute each <code>shift</code> (aka <code>loc</code> or "mu" in [Germain et al. (2015)][1]) and <code>log(scale)</code> (aka "alpha" in [Germain et al. (2015)][1]) such that each are broadcastable with the arguments to <code>forward</code> and <code>inverse</code>, i.e., such that the calculations in <code>forward</code>, <code>inverse</code> [below] are possible.</p> <p>For convenience, <code>masked_autoregressive_default_template</code> is offered as a possible <code>shift_and_log_scale_fn</code> function. It implements the MADE architecture [(Germain et al., 2015)][1]. MADE is a feed-forward network that computes a <code>shift</code> and <code>log(scale)</code> using <code>masked_dense</code> layers in a deep neural network. Weights are masked to ensure the autoregressive property. It is possible that this architecture is suboptimal for your task. To build alternative networks, either change the arguments to <code>masked_autoregressive_default_template</code>, use the <code>masked_dense</code> function to roll-out your own, or use some other architecture, e.g., using <a href="../../../layers"><code>tf.layers</code></a>.</p> <aside class="warning"><strong>Warning:</strong><span> no attempt is made to validate that the <code>shift_and_log_scale_fn</code> enforces the "autoregressive property".</span></aside> <p>Assuming <code>shift_and_log_scale_fn</code> has valid shape and autoregressive semantics, the forward transformation is</p> <pre class="prettyprint lang-python" data-language="python">def forward(x):
  y = zeros_like(x)
  event_size = x.shape[-1]
  for _ in range(event_size):
    shift, log_scale = shift_and_log_scale_fn(y)
    y = x * math_ops.exp(log_scale) + shift
  return y
</pre> <p>and the inverse transformation is</p> <pre class="prettyprint lang-python" data-language="python">def inverse(y):
  shift, log_scale = shift_and_log_scale_fn(y)
  return (y - shift) / math_ops.exp(log_scale)
</pre> <p>Notice that the <code>inverse</code> does not need a for-loop. This is because in the forward pass each calculation of <code>shift</code> and <code>log_scale</code> is based on the <code>y</code> calculated so far (not <code>x</code>). In the <code>inverse</code>, the <code>y</code> is fully known, thus is equivalent to the scaling used in <code>forward</code> after <code>event_size</code> passes, i.e., the "last" <code>y</code> used to compute <code>shift</code>, <code>log_scale</code>. (Roughly speaking, this also proves the transform is bijective.)</p> <h4 id="examples">Examples</h4> <pre class="prettyprint lang-python" data-language="python">tfd = tf.contrib.distributions
tfb = tfd.bijectors

dims = 5

# A common choice for a normalizing flow is to use a Gaussian for the base
# distribution. (However, any continuous distribution would work.) E.g.,
maf = tfd.TransformedDistribution(
    distribution=tfd.Normal(loc=0., scale=1.),
    bijector=tfb.MaskedAutoregressiveFlow(
        shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(
            hidden_layers=[512, 512])),
    event_shape=[dims])

x = maf.sample()  # Expensive; uses &lt;a href="../../../../tf/while_loop"&gt;&lt;code&gt;tf.while_loop&lt;/code&gt;&lt;/a&gt;, no Bijector caching.
maf.log_prob(x)   # Almost free; uses Bijector caching.
maf.log_prob(0.)  # Cheap; no &lt;a href="../../../../tf/while_loop"&gt;&lt;code&gt;tf.while_loop&lt;/code&gt;&lt;/a&gt; despite no Bijector caching.

# [Papamakarios et al. (2016)][3] also describe an Inverse Autoregressive
# Flow [(Kingma et al., 2016)][2]:
iaf = tfd.TransformedDistribution(
    distribution=tfd.Normal(loc=0., scale=1.),
    bijector=tfb.Invert(tfb.MaskedAutoregressiveFlow(
        shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(
            hidden_layers=[512, 512]))),
    event_shape=[dims])

x = iaf.sample()  # Cheap; no &lt;a href="../../../../tf/while_loop"&gt;&lt;code&gt;tf.while_loop&lt;/code&gt;&lt;/a&gt; despite no Bijector caching.
iaf.log_prob(x)   # Almost free; uses Bijector caching.
iaf.log_prob(0.)  # Expensive; uses &lt;a href="../../../../tf/while_loop"&gt;&lt;code&gt;tf.while_loop&lt;/code&gt;&lt;/a&gt;, no Bijector caching.

# In many (if not most) cases the default `shift_and_log_scale_fn` will be a
# poor choice. Here's an example of using a "shift only" version and with a
# different number/depth of hidden layers.
shift_only = True
maf_no_scale_hidden2 = tfd.TransformedDistribution(
    distribution=tfd.Normal(loc=0., scale=1.),
    bijector=tfb.MaskedAutoregressiveFlow(
        tfb.masked_autoregressive_default_template(
            hidden_layers=[32],
            shift_only=shift_only),
        is_constant_jacobian=shift_only),
    event_shape=[dims])
</pre> <h4 id="references">References</h4> <p>[1]: Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked Autoencoder for Distribution Estimation. In <em>International Conference on Machine Learning</em>, 2015. https://arxiv.org/abs/1502.03509</p> <p>[2]: Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improving Variational Inference with Inverse Autoregressive Flow. In <em>Neural Information Processing Systems</em>, 2016. https://arxiv.org/abs/1606.04934</p> <p>[3]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017. https://arxiv.org/abs/1705.07057</p> <h2 id="properties">Properties</h2> <h3 id="dtype"><code>dtype</code></h3> <p>dtype of <code>Tensor</code>s transformable by this distribution.</p> <h3 id="event_ndims"><code>event_ndims</code></h3> <p>Returns then number of event dimensions this bijector operates on.</p> <h3 id="graph_parents"><code>graph_parents</code></h3> <p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p> <h3 id="is_constant_jacobian"><code>is_constant_jacobian</code></h3> <p>Returns true iff the Jacobian is not a function of x.</p> <blockquote class="note">
<strong>Note:</strong><span> Jacobian is either constant for both forward and inverse or neither.</span>
</blockquote> <h4 id="returns">Returns:</h4> <ul> <li>
<b><code>is_constant_jacobian</code></b>: Python <code>bool</code>.</li> </ul> <h3 id="name"><code>name</code></h3> <p>Returns the string name of this <code>Bijector</code>.</p> <h3 id="validate_args"><code>validate_args</code></h3> <p>Returns True if Tensor arguments will be validated.</p> <h2 id="methods">Methods</h2> <h3 id="__init__"><code>__init__</code></h3> <pre class="prettyprint lang-python" data-language="python">__init__(
    shift_and_log_scale_fn,
    is_constant_jacobian=False,
    validate_args=False,
    unroll_loop=False,
    name=None
)
</pre> <p>Creates the MaskedAutoregressiveFlow bijector.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>shift_and_log_scale_fn</code></b>: Python <code>callable</code> which computes <code>shift</code> and <code>log_scale</code> from both the forward domain (<code>x</code>) and the inverse domain (<code>y</code>). Calculation must respect the "autoregressive property" (see class docstring). Suggested default <code>masked_autoregressive_default_template(hidden_layers=...)</code>. Typically the function contains <code>tf.Variables</code> and is wrapped using <a href="../../../make_template"><code>tf.make_template</code></a>. Returning <code>None</code> for either (both) <code>shift</code>, <code>log_scale</code> is equivalent to (but more efficient than) returning zero.</li> <li>
<b><code>is_constant_jacobian</code></b>: Python <code>bool</code>. Default: <code>False</code>. When <code>True</code> the implementation assumes <code>log_scale</code> does not depend on the forward domain (<code>x</code>) or inverse domain (<code>y</code>) values. (No validation is made; <code>is_constant_jacobian=False</code> is always safe but possibly computationally inefficient.)</li> <li>
<b><code>validate_args</code></b>: Python <code>bool</code> indicating whether arguments should be checked for correctness.</li> <li>
<b><code>unroll_loop</code></b>: Python <code>bool</code> indicating whether the <a href="../../../while_loop"><code>tf.while_loop</code></a> in <code>_forward</code> should be replaced with a static for loop. Requires that the final dimension of <code>x</code> be known at graph construction time. Defaults to <code>False</code>.</li> <li>
<b><code>name</code></b>: Python <code>str</code>, name given to ops managed by this object.</li> </ul> <h3 id="forward"><code>forward</code></h3> <pre class="prettyprint lang-python" data-language="python">forward(
    x,
    name='forward'
)
</pre> <p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>x</code></b>: <code>Tensor</code>. The input to the "forward" evaluation.</li> <li>
<b><code>name</code></b>: The name to give this op.</li> </ul> <h4 id="returns_1">Returns:</h4> <p><code>Tensor</code>.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li> <li>
<b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li> </ul> <h3 id="forward_event_shape"><code>forward_event_shape</code></h3> <pre class="prettyprint lang-python" data-language="python">forward_event_shape(input_shape)
</pre> <p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p> <p>Same meaning as <code>forward_event_shape_tensor</code>. May be only partially defined.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li> </ul> <h4 id="returns_2">Returns:</h4> <ul> <li>
<b><code>forward_event_shape_tensor</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li> </ul> <h3 id="forward_event_shape_tensor"><code>forward_event_shape_tensor</code></h3> <pre class="prettyprint lang-python" data-language="python">forward_event_shape_tensor(
    input_shape,
    name='forward_event_shape_tensor'
)
</pre> <p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li> <li>
<b><code>name</code></b>: name to give to the op</li> </ul> <h4 id="returns_3">Returns:</h4> <ul> <li>
<b><code>forward_event_shape_tensor</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li> </ul> <h3 id="forward_log_det_jacobian"><code>forward_log_det_jacobian</code></h3> <pre class="prettyprint lang-python" data-language="python">forward_log_det_jacobian(
    x,
    name='forward_log_det_jacobian'
)
</pre> <p>Returns both the forward_log_det_jacobian.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code>x</code></b>: <code>Tensor</code>. The input to the "forward" Jacobian evaluation.</li> <li>
<b><code>name</code></b>: The name to give this op.</li> </ul> <h4 id="returns_4">Returns:</h4> <p><code>Tensor</code>, if this bijector is injective. If not injective this is not implemented.</p> <h4 id="raises_1">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li> <li>
<b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented, or this is a non-injective bijector.</li> </ul> <h3 id="inverse"><code>inverse</code></h3> <pre class="prettyprint lang-python" data-language="python">inverse(
    y,
    name='inverse'
)
</pre> <p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code>y</code></b>: <code>Tensor</code>. The input to the "inverse" evaluation.</li> <li>
<b><code>name</code></b>: The name to give this op.</li> </ul> <h4 id="returns_5">Returns:</h4> <p><code>Tensor</code>, if this bijector is injective. If not injective, returns the k-tuple containing the unique <code>k</code> points <code>(x1, ..., xk)</code> such that <code>g(xi) = y</code>.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li> <li>
<b><code>NotImplementedError</code></b>: if <code>_inverse</code> is not implemented.</li> </ul> <h3 id="inverse_event_shape"><code>inverse_event_shape</code></h3> <pre class="prettyprint lang-python" data-language="python">inverse_event_shape(output_shape)
</pre> <p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p> <p>Same meaning as <code>inverse_event_shape_tensor</code>. May be only partially defined.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li> </ul> <h4 id="returns_6">Returns:</h4> <ul> <li>
<b><code>inverse_event_shape_tensor</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li> </ul> <h3 id="inverse_event_shape_tensor"><code>inverse_event_shape_tensor</code></h3> <pre class="prettyprint lang-python" data-language="python">inverse_event_shape_tensor(
    output_shape,
    name='inverse_event_shape_tensor'
)
</pre> <p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p> <h4 id="args_7">Args:</h4> <ul> <li>
<b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li> <li>
<b><code>name</code></b>: name to give to the op</li> </ul> <h4 id="returns_7">Returns:</h4> <ul> <li>
<b><code>inverse_event_shape_tensor</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li> </ul> <h3 id="inverse_log_det_jacobian"><code>inverse_log_det_jacobian</code></h3> <pre class="prettyprint lang-python" data-language="python">inverse_log_det_jacobian(
    y,
    name='inverse_log_det_jacobian'
)
</pre> <p>Returns the (log o det o Jacobian o inverse)(y).</p> <p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p> <p>Note that <code>forward_log_det_jacobian</code> is the negative of this function, evaluated at <code>g^{-1}(y)</code>.</p> <h4 id="args_8">Args:</h4> <ul> <li>
<b><code>y</code></b>: <code>Tensor</code>. The input to the "inverse" Jacobian evaluation.</li> <li>
<b><code>name</code></b>: The name to give this op.</li> </ul> <h4 id="returns_8">Returns:</h4> <p><code>Tensor</code>, if this bijector is injective. If not injective, returns the tuple of local log det Jacobians, <code>log(det(Dg_i^{-1}(y)))</code>, where <code>g_i</code> is the restriction of <code>g</code> to the <code>ith</code> partition <code>Di</code>.</p> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li> <li>
<b><code>NotImplementedError</code></b>: if <code>_inverse_log_det_jacobian</code> is not implemented.</li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    © 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/bijectors/MaskedAutoregressiveFlow" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/bijectors/MaskedAutoregressiveFlow</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
