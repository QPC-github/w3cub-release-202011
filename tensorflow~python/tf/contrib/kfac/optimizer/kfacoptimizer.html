
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>contrib.kfac.optimizer.KfacOptimizer - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Inherits From&#58; GradientDescentOptimizer ">
  <meta name="keywords" content="tf, contrib, kfac, optimizer, kfacoptimizer, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/tf/contrib/kfac/optimizer/kfacoptimizer.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.kfac.optimizer.KfacOptimizer </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.contrib.kfac.optimizer.KfacOptimizer"> <meta itemprop="path" content="r1.8"> <meta itemprop="property" content="cov_update_op"> <meta itemprop="property" content="cov_update_ops"> <meta itemprop="property" content="cov_update_thunks"> <meta itemprop="property" content="damping"> <meta itemprop="property" content="damping_adaptation_interval"> <meta itemprop="property" content="inv_update_op"> <meta itemprop="property" content="inv_update_ops"> <meta itemprop="property" content="inv_update_thunks"> <meta itemprop="property" content="variables"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="compute_gradients"> <meta itemprop="property" content="create_ops_and_vars_thunks"> <meta itemprop="property" content="get_name"> <meta itemprop="property" content="get_slot"> <meta itemprop="property" content="get_slot_names"> <meta itemprop="property" content="make_ops_and_vars"> <meta itemprop="property" content="make_vars_and_create_op_thunks"> <meta itemprop="property" content="minimize"> <meta itemprop="property" content="set_damping_adaptation_params"> <meta itemprop="property" content="GATE_GRAPH"> <meta itemprop="property" content="GATE_NONE"> <meta itemprop="property" content="GATE_OP"> </div> <h2 id="class_kfacoptimizer">Class <code>KfacOptimizer</code>
</h2> <p>Inherits From: <a href="../../../train/gradientdescentoptimizer"><code>GradientDescentOptimizer</code></a></p> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/kfac/python/ops/optimizer.py"><code>tensorflow/contrib/kfac/python/ops/optimizer.py</code></a>.</p> <p>The KFAC Optimizer (https://arxiv.org/abs/1503.05671).</p> <h2 id="properties">Properties</h2> <h3 id="cov_update_op"><code>cov_update_op</code></h3> <h3 id="cov_update_ops"><code>cov_update_ops</code></h3> <h3 id="cov_update_thunks"><code>cov_update_thunks</code></h3> <h3 id="damping"><code>damping</code></h3> <h3 id="damping_adaptation_interval"><code>damping_adaptation_interval</code></h3> <h3 id="inv_update_op"><code>inv_update_op</code></h3> <h3 id="inv_update_ops"><code>inv_update_ops</code></h3> <h3 id="inv_update_thunks"><code>inv_update_thunks</code></h3> <h3 id="variables"><code>variables</code></h3> <p>A list of variables which encode the current state of <code>Optimizer</code>.</p> <p>Includes slot variables and additional global variables created by the optimizer in the current default graph.</p> <h4 id="returns">Returns:</h4> <p>A list of variables.</p> <h2 id="methods">Methods</h2> <h3 id="__init__"><code>__init__</code></h3> <pre class="prettyprint lang-python" data-language="python">__init__(
    learning_rate,
    cov_ema_decay,
    damping,
    layer_collection,
    var_list=None,
    momentum=0.9,
    momentum_type='regular',
    norm_constraint=None,
    name='KFAC',
    estimation_mode='gradients',
    colocate_gradients_with_ops=True,
    batch_size=None,
    placement_strategy=None,
    **kwargs
)
</pre> <p>Initializes the KFAC optimizer with the given settings.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>learning_rate</code></b>: The base learning rate for the optimizer. Should probably be set to 1.0 when using momentum_type = 'qmodel', but can still be set lowered if desired (effectively lowering the trust in the quadratic model.)</li> <li>
<b><code>cov_ema_decay</code></b>: The decay factor used when calculating the covariance estimate moving averages.</li> <li>
<b><code>damping</code></b>: The damping factor used to stabilize training due to errors in the local approximation with the Fisher information matrix, and to regularize the update direction by making it closer to the gradient. If damping is adapted during training then this value is used for initializing damping varaible. (Higher damping means the update looks more like a standard gradient update - see Tikhonov regularization.)</li> <li>
<b><code>layer_collection</code></b>: The layer collection object, which holds the fisher blocks, kronecker factors, and losses associated with the graph. The layer_collection cannot be modified after KfacOptimizer's initialization.</li> <li>
<b><code>var_list</code></b>: Optional list or tuple of variables to train. Defaults to the list of variables collected in the graph under the key <code>GraphKeys.TRAINABLE_VARIABLES</code>.</li> <li>
<b><code>momentum</code></b>: The momentum decay constant to use. Only applies when momentum_type is 'regular' or 'adam'. (Default: 0.9)</li> <li>
<b><code>momentum_type</code></b>: The type of momentum to use in this optimizer, one of 'regular', 'adam', or 'qmodel'. (Default: 'regular')</li> <li>
<b><code>norm_constraint</code></b>: float or Tensor. If specified, the update is scaled down so that its approximate squared Fisher norm v^T F v is at most the specified value. May only be used with momentum type 'regular'. (Default: None)</li> <li>
<b><code>name</code></b>: The name for this optimizer. (Default: 'KFAC')</li> <li>
<b><code>estimation_mode</code></b>: The type of estimator to use for the Fishers. Can be 'gradients', 'empirical', 'curvature_propagation', or 'exact'. (Default: 'gradients'). See the doc-string for FisherEstimator for more a more detailed description of these options.</li> <li>
<b><code>colocate_gradients_with_ops</code></b>: Whether we should request gradients we compute in the estimator be colocated with their respective ops. (Default: True)</li> <li>
<b><code>batch_size</code></b>: The size of the mini-batch. Only needed when momentum_type == 'qmodel' or when automatic adjustment is used. (Default: None)</li> <li>
<b><code>placement_strategy</code></b>: string, Device placement strategy used when creating covariance variables, covariance ops, and inverse ops. (Default: <code>None</code>)</li> <li>
<b><code>**kwargs</code></b>: Arguments to be passesd to specific placement strategy mixin. Check <code>placement.RoundRobinPlacementMixin</code> for example.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If the momentum type is unsupported.</li> <li>
<b><code>ValueError</code></b>: If clipping is used with momentum type other than 'regular'.</li> <li>
<b><code>ValueError</code></b>: If no losses have been registered with layer_collection.</li> <li>
<b><code>ValueError</code></b>: If momentum is non-zero and momentum_type is not 'regular' or 'adam'.</li> </ul> <h3 id="apply_gradients"><code>apply_gradients</code></h3> <pre class="prettyprint lang-python" data-language="python">apply_gradients(
    grads_and_vars,
    *args,
    **kwargs
)
</pre> <p>Applies gradients to variables.</p> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>grads_and_vars</code></b>: List of (gradient, variable) pairs.</li> <li>
<b><code>*args</code></b>: Additional arguments for super.apply_gradients.</li> <li>
<b><code>**kwargs</code></b>: Additional keyword arguments for super.apply_gradients.</li> </ul> <h4 id="returns_1">Returns:</h4> <p>An <code>Operation</code> that applies the specified gradients.</p> <h3 id="compute_gradients"><code>compute_gradients</code></h3> <pre class="prettyprint lang-python" data-language="python">compute_gradients(
    *args,
    **kwargs
)
</pre> <p>Compute gradients of <code>loss</code> for the variables in <code>var_list</code>.</p> <p>This is the first part of <code>minimize()</code>. It returns a list of (gradient, variable) pairs where "gradient" is the gradient for "variable". Note that "gradient" can be a <code>Tensor</code>, an <code>IndexedSlices</code>, or <code>None</code> if there is no gradient for the given variable.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code>loss</code></b>: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.</li> <li>
<b><code>var_list</code></b>: Optional list or tuple of <a href="../../../variable"><code>tf.Variable</code></a> to update to minimize <code>loss</code>. Defaults to the list of variables collected in the graph under the key <code>GraphKeys.TRAINABLE_VARIABLES</code>.</li> <li>
<b><code>gate_gradients</code></b>: How to gate the computation of gradients. Can be <code>GATE_NONE</code>, <code>GATE_OP</code>, or <code>GATE_GRAPH</code>.</li> <li>
<b><code>aggregation_method</code></b>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code>AggregationMethod</code>.</li> <li>
<b><code>colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li> <li>
<b><code>grad_loss</code></b>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A list of (gradient, variable) pairs. Variable is always present, but gradient can be <code>None</code>.</p> <h4 id="raises_1">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: If <code>var_list</code> contains anything else than <code>Variable</code> objects.</li> <li>
<b><code>ValueError</code></b>: If some arguments are invalid.</li> <li>
<b><code>RuntimeError</code></b>: If called with eager execution enabled and <code>loss</code> is not callable.</li> </ul> <h4 id="eager_compatibility">Eager Compatibility</h4> <p>When eager execution is enabled, <code>gate_gradients</code>, <code>aggregation_method</code>, and <code>colocate_gradients_with_ops</code> are ignored.</p> <h3 id="create_ops_and_vars_thunks"><code>create_ops_and_vars_thunks</code></h3> <pre class="prettyprint lang-python" data-language="python">create_ops_and_vars_thunks()
</pre> <p>Create thunks that make the ops and vars on demand.</p> <p>This function returns 4 lists of thunks: cov_variable_thunks, cov_update_thunks, inv_variable_thunks, and inv_update_thunks.</p> <p>The length of each list is the number of factors and the i-th element of each list corresponds to the i-th factor (given by the "factors" property).</p> <p>Note that the execution of these thunks must happen in a certain partial order. The i-th element of cov_variable_thunks must execute before the i-th element of cov_update_thunks (and also the i-th element of inv_update_thunks). Similarly, the i-th element of inv_variable_thunks must execute before the i-th element of inv_update_thunks.</p> <p>TL;DR (oversimplified): Execute the thunks according to the order that they are returned.</p> <h4 id="returns_3">Returns:</h4> <ul> <li>
<b><code>cov_variable_thunks</code></b>: A list of thunks that make the cov variables.</li> <li>
<b><code>cov_update_thunks</code></b>: A list of thunks that make the cov update ops.</li> <li>
<b><code>inv_variable_thunks</code></b>: A list of thunks that make the inv variables.</li> <li>
<b><code>inv_update_thunks</code></b>: A list of thunks that make the inv update ops.</li> </ul> <h3 id="get_name"><code>get_name</code></h3> <pre class="prettyprint lang-python" data-language="python">get_name()
</pre> <h3 id="get_slot"><code>get_slot</code></h3> <pre class="prettyprint lang-python" data-language="python">get_slot(
    var,
    name
)
</pre> <p>Return a slot named <code>name</code> created for <code>var</code> by the Optimizer.</p> <p>Some <code>Optimizer</code> subclasses use additional variables. For example <code>Momentum</code> and <code>Adagrad</code> use variables to accumulate updates. This method gives access to these <code>Variable</code> objects if for some reason you need them.</p> <p>Use <code>get_slot_names()</code> to get the list of slot names created by the <code>Optimizer</code>.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>var</code></b>: A variable passed to <code>minimize()</code> or <code>apply_gradients()</code>.</li> <li>
<b><code>name</code></b>: A string.</li> </ul> <h4 id="returns_4">Returns:</h4> <p>The <code>Variable</code> for the slot if it was created, <code>None</code> otherwise.</p> <h3 id="get_slot_names"><code>get_slot_names</code></h3> <pre class="prettyprint lang-python" data-language="python">get_slot_names()
</pre> <p>Return a list of the names of slots created by the <code>Optimizer</code>.</p> <p>See <code>get_slot()</code>.</p> <h4 id="returns_5">Returns:</h4> <p>A list of strings.</p> <h3 id="make_ops_and_vars"><code>make_ops_and_vars</code></h3> <pre class="prettyprint lang-python" data-language="python">make_ops_and_vars()
</pre> <p>Make ops and vars with device placement <code>self._placement_strategy</code>.</p> <p>See <code>FisherEstimator.make_ops_and_vars</code> for details.</p> <h4 id="returns_6">Returns:</h4> <ul> <li>
<b><code>cov_update_ops</code></b>: List of ops that compute the cov updates. Corresponds one-to-one with the list of factors given by the "factors" property.</li> <li>
<b><code>cov_update_op</code></b>: cov_update_ops grouped into a single op.</li> <li>
<b><code>inv_update_ops</code></b>: List of ops that compute the inv updates. Corresponds one-to-one with the list of factors given by the "factors" property.</li> <li>
<b><code>cov_update_op</code></b>: cov_update_ops grouped into a single op.</li> <li>
<b><code>inv_update_op</code></b>: inv_update_ops grouped into a single op.</li> </ul> <h3 id="make_vars_and_create_op_thunks"><code>make_vars_and_create_op_thunks</code></h3> <pre class="prettyprint lang-python" data-language="python">make_vars_and_create_op_thunks()
</pre> <p>Make vars and create op thunks.</p> <h4 id="returns_7">Returns:</h4> <ul> <li>
<b><code>cov_update_thunks</code></b>: List of cov update thunks. Corresponds one-to-one with the list of factors given by the "factors" property.</li> <li>
<b><code>inv_update_thunks</code></b>: List of inv update thunks. Corresponds one-to-one with the list of factors given by the "factors" property.</li> </ul> <h3 id="minimize"><code>minimize</code></h3> <pre class="prettyprint lang-python" data-language="python">minimize(
    *args,
    **kwargs
)
</pre> <p>Add operations to minimize <code>loss</code> by updating <code>var_list</code>.</p> <p>This method simply combines calls <code>compute_gradients()</code> and <code>apply_gradients()</code>. If you want to process the gradient before applying them call <code>compute_gradients()</code> and <code>apply_gradients()</code> explicitly instead of using this function.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code>loss</code></b>: A <code>Tensor</code> containing the value to minimize.</li> <li>
<b><code>global_step</code></b>: Optional <code>Variable</code> to increment by one after the variables have been updated.</li> <li>
<b><code>var_list</code></b>: Optional list or tuple of <code>Variable</code> objects to update to minimize <code>loss</code>. Defaults to the list of variables collected in the graph under the key <code>GraphKeys.TRAINABLE_VARIABLES</code>.</li> <li>
<b><code>gate_gradients</code></b>: How to gate the computation of gradients. Can be <code>GATE_NONE</code>, <code>GATE_OP</code>, or <code>GATE_GRAPH</code>.</li> <li>
<b><code>aggregation_method</code></b>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code>AggregationMethod</code>.</li> <li>
<b><code>colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li> <li>
<b><code>name</code></b>: Optional name for the returned operation.</li> <li>
<b><code>grad_loss</code></b>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li> </ul> <h4 id="returns_8">Returns:</h4> <p>An Operation that updates the variables in <code>var_list</code>. If <code>global_step</code> was not <code>None</code>, that operation also increments <code>global_step</code>.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If some of the variables are not <code>Variable</code> objects.</li> </ul> <h4 id="eager_compatibility_1">Eager Compatibility</h4> <p>When eager execution is enabled, <code>loss</code> should be a Python function that takes elements of <code>var_list</code> as arguments and computes the value to be minimized. If <code>var_list</code> is None, <code>loss</code> should take no arguments. Minimization (and gradient computation) is done with respect to the elements of <code>var_list</code> if not None, else with respect to any trainable variables created during the execution of the <code>loss</code> function. <code>gate_gradients</code>, <code>aggregation_method</code>, <code>colocate_gradients_with_ops</code> and <code>grad_loss</code> are ignored when eager execution is enabled.</p> <h3 id="set_damping_adaptation_params"><code>set_damping_adaptation_params</code></h3> <pre class="prettyprint lang-python" data-language="python">set_damping_adaptation_params(
    is_chief,
    prev_train_batch,
    loss_fn,
    min_damping=1e-05,
    damping_adaptation_decay=0.99,
    damping_adaptation_interval=5
)
</pre> <p>Sets parameters required to adapt damping during training.</p> <p>When called, enables damping adaptation according to the Levenberg-Marquardt style rule described in Section 6.5 of "Optimizing Neural Networks with Kronecker-factored Approximate Curvature".</p> <p>Note that this function creates Tensorflow variables which store a few scalars and are accessed by the ops which update the damping (as part of the training op returned by the minimize() method).</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code>is_chief</code></b>: <code>Boolean</code>, <code>True</code> if the worker is chief.</li> <li>
<b><code>prev_train_batch</code></b>: Training data used to minimize loss in the previous step. This will be used to evaluate loss by calling <code>loss_fn(prev_train_batch)</code>.</li> <li>
<b><code>loss_fn</code></b>: <code>function</code> that takes as input training data tensor and returns a scalar loss.</li> <li>
<b><code>min_damping</code></b>: <code>float</code>(Optional), Minimum value the damping parameter can take. Default value 1e-5.</li> <li>
<b><code>damping_adaptation_decay</code></b>: <code>float</code>(Optional), The <code>damping</code> parameter is multipled by the <code>damping_adaptation_decay</code> every <code>damping_adaptation_interval</code> number of iterations. Default value 0.99.</li> <li>
<b><code>damping_adaptation_interval</code></b>: <code>int</code>(Optional), Number of steps in between updating the <code>damping</code> parameter. Default value 5.</li> </ul> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If <code>set_damping_adaptation_params</code> is already called and the the <code>adapt_damping</code> is <code>True</code>.</li> </ul> <h2 id="class_members">Class Members</h2> <h3 id="GATE_GRAPH"><code>GATE_GRAPH</code></h3> <h3 id="GATE_NONE"><code>GATE_NONE</code></h3> <h3 id="GATE_OP"><code>GATE_OP</code></h3>
<div class="_attribution">
  <p class="_attribution-p">
    © 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/kfac/optimizer/KfacOptimizer" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/contrib/kfac/optimizer/KfacOptimizer</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
