
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>contrib.kfac.layer_collection.LayerCollection - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Defined in tensorflow&#47;contrib&#47;kfac&#47;python&#47;ops&#47;layer_collection.py. ">
  <meta name="keywords" content="tf, contrib, kfac, layer, collection, layercollection, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/tf/contrib/kfac/layer_collection/layercollection.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.kfac.layer_collection.LayerCollection </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.contrib.kfac.layer_collection.LayerCollection"> <meta itemprop="path" content="r1.8"> <meta itemprop="property" content="default_conv2d_approximation"> <meta itemprop="property" content="default_conv2d_multi_approximation"> <meta itemprop="property" content="default_embedding_approximation"> <meta itemprop="property" content="default_embedding_multi_approximation"> <meta itemprop="property" content="default_fully_connected_approximation"> <meta itemprop="property" content="default_fully_connected_multi_approximation"> <meta itemprop="property" content="default_generic_approximation"> <meta itemprop="property" content="graph"> <meta itemprop="property" content="linked_parameters"> <meta itemprop="property" content="losses"> <meta itemprop="property" content="registered_variables"> <meta itemprop="property" content="subgraph"> <meta itemprop="property" content="towers_by_loss"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="as_default"> <meta itemprop="property" content="check_registration"> <meta itemprop="property" content="create_subgraph"> <meta itemprop="property" content="define_linked_parameters"> <meta itemprop="property" content="eval_losses"> <meta itemprop="property" content="eval_losses_on_samples"> <meta itemprop="property" content="get_blocks"> <meta itemprop="property" content="get_factors"> <meta itemprop="property" content="make_or_get_factor"> <meta itemprop="property" content="register_block"> <meta itemprop="property" content="register_categorical_predictive_distribution"> <meta itemprop="property" content="register_conv2d"> <meta itemprop="property" content="register_conv2d_multi"> <meta itemprop="property" content="register_convolution"> <meta itemprop="property" content="register_depthwise_conv2d"> <meta itemprop="property" content="register_embedding"> <meta itemprop="property" content="register_embedding_multi"> <meta itemprop="property" content="register_fully_connected"> <meta itemprop="property" content="register_fully_connected_multi"> <meta itemprop="property" content="register_generic"> <meta itemprop="property" content="register_loss_function"> <meta itemprop="property" content="register_multi_bernoulli_predictive_distribution"> <meta itemprop="property" content="register_normal_predictive_distribution"> <meta itemprop="property" content="register_separable_conv2d"> <meta itemprop="property" content="set_default_conv2d_approximation"> <meta itemprop="property" content="set_default_embedding_approximation"> <meta itemprop="property" content="set_default_fully_connected_approximation"> <meta itemprop="property" content="set_default_fully_connected_multi_approximation"> <meta itemprop="property" content="set_default_generic_approximation"> <meta itemprop="property" content="total_loss"> <meta itemprop="property" content="total_sampled_loss"> </div> <h2 id="class_layercollection">Class <code>LayerCollection</code>
</h2> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/kfac/python/ops/layer_collection.py"><code>tensorflow/contrib/kfac/python/ops/layer_collection.py</code></a>.</p> <p>Registry of information about layers and losses.</p> <p>Note that you need to create a new one of these for each MatrixEstimator or KfacOptimizer.</p> <h4 id="attributes">Attributes:</h4> <ul> <li>
<b><code>fisher_blocks</code></b>: a LayersParamsDict (subclass of OrderedDict) mapping layer parameters (Tensors or tuples of Tensors) to FisherBlock instances.</li> <li>
<b><code>fisher_factors</code></b>: an OrderedDict mapping tuples to FisherFactor instances.</li> <li>
<b><code>losses</code></b>: a list of LossFunction objects. The loss to be optimized is their sum.</li> <li>
<b><code>loss_colocation_ops</code></b>: ops to colocate loss function evaluations with. These will typically be the inputs to the losses.</li> </ul> <h2 id="properties">Properties</h2> <h3 id="default_conv2d_approximation"><code>default_conv2d_approximation</code></h3> <h3 id="default_conv2d_multi_approximation"><code>default_conv2d_multi_approximation</code></h3> <h3 id="default_embedding_approximation"><code>default_embedding_approximation</code></h3> <h3 id="default_embedding_multi_approximation"><code>default_embedding_multi_approximation</code></h3> <h3 id="default_fully_connected_approximation"><code>default_fully_connected_approximation</code></h3> <h3 id="default_fully_connected_multi_approximation"><code>default_fully_connected_multi_approximation</code></h3> <h3 id="default_generic_approximation"><code>default_generic_approximation</code></h3> <h3 id="graph"><code>graph</code></h3> <h3 id="linked_parameters"><code>linked_parameters</code></h3> <p>Groups of parameters with an optionally specified approximation.</p> <p>Linked parameters can be added using <code>define_linked_parameters</code>. If an approximation is specified, then this approximation will be used when registering a layer with exactly these parameters, unless an approximation is specified when calling the registration function.</p> <h4 id="returns">Returns:</h4> <p>A <code>dict</code> mapping tuples of parameters to an optional string.</p> <h3 id="losses"><code>losses</code></h3> <p>Tuple of LossFunction objects registered with this LayerCollection.</p> <h3 id="registered_variables"><code>registered_variables</code></h3> <p>A tuple of all of the variables currently registered.</p> <h3 id="subgraph"><code>subgraph</code></h3> <h3 id="towers_by_loss"><code>towers_by_loss</code></h3> <p>Tuple across losses of LossFunction objects registered to each tower.</p> <h2 id="methods">Methods</h2> <h3 id="__init__"><code>__init__</code></h3> <pre class="prettyprint lang-python" data-language="python">__init__(
    graph=None,
    name='LayerCollection'
)
</pre> <p>Initialize self. See help(type(self)) for accurate signature.</p> <h3 id="as_default"><code>as_default</code></h3> <pre class="prettyprint lang-python" data-language="python">as_default(
    *args,
    **kwds
)
</pre> <p>Sets this LayerCollection as the default.</p> <h3 id="check_registration"><code>check_registration</code></h3> <pre class="prettyprint lang-python" data-language="python">check_registration(variables)
</pre> <p>Checks that all variable uses have been registered properly.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>variables</code></b>: List of variables.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If any registered variables are not included in the list.</li> <li>
<b><code>ValueError</code></b>: If any variable in the list is not registered.</li> <li>
<b><code>ValueError</code></b>: If any variable in the list is registered with the wrong number of "uses" in the subgraph recorded (vs the number of times that variable is actually used in the subgraph).</li> </ul> <h3 id="create_subgraph"><code>create_subgraph</code></h3> <pre class="prettyprint lang-python" data-language="python">create_subgraph()
</pre> <h3 id="define_linked_parameters"><code>define_linked_parameters</code></h3> <pre class="prettyprint lang-python" data-language="python">define_linked_parameters(
    params,
    approximation=None
)
</pre> <p>Identify a set of parameters that should be grouped together.</p> <p>During automatic graph scanning, any matches containing variables that have been identified as part of a linked group will be filtered out unless the match parameters are exactly equal to the ones specified in the linked group.</p> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>params</code></b>: A variable, or a tuple or list of variables. The variables to be linked.</li> <li>
<b><code>approximation</code></b>: Optional string specifying the type of approximation to use for these variables. If unspecified, this layer collection's default approximation for the layer type will be used.</li> </ul> <h4 id="raises_1">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If the parameters were already registered in a layer or identified as part of an incompatible group.</li> </ul> <h3 id="eval_losses"><code>eval_losses</code></h3> <pre class="prettyprint lang-python" data-language="python">eval_losses()
</pre> <p>Return evaluated losses (colocated with inputs to losses).</p> <h3 id="eval_losses_on_samples"><code>eval_losses_on_samples</code></h3> <pre class="prettyprint lang-python" data-language="python">eval_losses_on_samples()
</pre> <p>Return losses evaluated on samples (colocated with inputs to losses).</p> <h3 id="get_blocks"><code>get_blocks</code></h3> <pre class="prettyprint lang-python" data-language="python">get_blocks()
</pre> <h3 id="get_factors"><code>get_factors</code></h3> <pre class="prettyprint lang-python" data-language="python">get_factors()
</pre> <h3 id="make_or_get_factor"><code>make_or_get_factor</code></h3> <pre class="prettyprint lang-python" data-language="python">make_or_get_factor(
    cls,
    args
)
</pre> <p>Insert <code>cls(args)</code> into 'self.fisher_factors` if not already present.</p> <p>Wraps constructor in <code>tf.variable_scope()</code> to ensure variables constructed in <code>cls.__init__</code> are placed under this LayerCollection's scope.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code>cls</code></b>: Class that implements FisherFactor.</li> <li>
<b><code>args</code></b>: Tuple of arguments to pass into `cls's constructor. Must be hashable.</li> </ul> <h4 id="returns_1">Returns:</h4> <p>Instance of <code>cls</code> found in self.fisher_factors.</p> <h3 id="register_block"><code>register_block</code></h3> <pre class="prettyprint lang-python" data-language="python">register_block(
    layer_key,
    fisher_block,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Validates and registers the layer_key associated with the fisher_block.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>layer_key</code></b>: A variable or tuple of variables. The key to check for in existing registrations and to register if valid.</li> <li>
<b><code>fisher_block</code></b>: The associated <code>FisherBlock</code>.</li> <li>
<b><code>reuse</code></b>: Method to use for inserting new <code>FisherBlock's. One of True, False, or</code>VARIABLE_SCOPE`.</li> </ul> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If <code>layer_key</code> was already registered and reuse is <code>False</code>, if <code>layer_key</code> was registered with a different block type, or if <code>layer_key</code> shares any variables with but is not equal to a previously registered key.</li> <li>
<b><code>KeyError</code></b>: If <code>reuse</code> is <code>True</code> but <code>layer_key</code> was not previously registered.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>The <code>FisherBlock</code> registered under <code>layer_key</code>. If <code>layer_key</code> was already registered, this will be the previously registered <code>FisherBlock</code>.</p> <h3 id="register_categorical_predictive_distribution"><code>register_categorical_predictive_distribution</code></h3> <pre class="prettyprint lang-python" data-language="python">register_categorical_predictive_distribution(
    logits,
    seed=None,
    targets=None,
    name=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers a categorical predictive distribution.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code>logits</code></b>: The logits of the distribution (i.e. its parameters).</li> <li>
<b><code>seed</code></b>: The seed for the RNG (for debugging) (Default: None)</li> <li>
<b><code>targets</code></b>: (OPTIONAL) The targets for the loss function. Only required if one wants to call total_loss() instead of total_sampled_loss(). total_loss() is required, for example, to estimate the "empirical Fisher" (instead of the true Fisher). (Default: None)</li> <li>
<b><code>name</code></b>: (OPTIONAL) str or None. Unique name for this loss function. If None, a new name is generated. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>logits</code> as an additional mini-batch/tower of inputs to the loss-function/predictive distribution (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h3 id="register_conv2d"><code>register_conv2d</code></h3> <pre class="prettyprint lang-python" data-language="python">register_conv2d(
    params,
    strides,
    padding,
    inputs,
    outputs,
    data_format=None,
    dilations=None,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers a call to tf.nn.conv2d().</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code>params</code></b>: Tensor or 2-tuple of Tensors corresponding to weight and bias of this layer. Weight matrix should have shape [kernel_height, kernel_width, in_channels, out_channels]. Bias should have shape [out_channels].</li> <li>
<b><code>strides</code></b>: List of 4 ints. Strides for convolution kernel.</li> <li>
<b><code>padding</code></b>: string. see tf.nn.conv2d for valid values.</li> <li>
<b><code>inputs</code></b>: Tensor of shape [batch_size, height, width, in_channels]. Inputs to layer.</li> <li>
<b><code>outputs</code></b>: Tensor of shape [batch_size, height, width, out_channels]. Output produced by layer.</li> <li>
<b><code>data_format</code></b>: str or None. Format of data.</li> <li>
<b><code>dilations</code></b>: List of 4 ints. Dilations along each dimension.</li> <li>
<b><code>approx</code></b>: str or None. If not None must be one of "kron" or "diagonal". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="register_conv2d_multi"><code>register_conv2d_multi</code></h3> <pre class="prettyprint lang-python" data-language="python">register_conv2d_multi(
    params,
    strides,
    padding,
    inputs,
    outputs,
    num_uses=None,
    data_format=None,
    dilations=None,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers convolutional layers with shared parameters.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code>params</code></b>: Tensor or 2-tuple of Tensors corresponding to weight and bias of this layer. Weight matrix should have shape [kernel_height, kernel_width, in_channels, out_channels]. Bias should have shape [out_channels].</li> <li>
<b><code>strides</code></b>: 1-D Tensor of length 4. Strides for convolution kernel.</li> <li>
<b><code>padding</code></b>: string. see tf.nn.conv2d for valid values.</li> <li>
<b><code>inputs</code></b>: A list of Tensors, each of shape [batch_size, height, width, in_channels]. Inputs to layer. The list indexes each use in the graph (which might correspond to a "time-step" in an RNN). OR, can be single Tensor, of shape [num_uses * batch_size, height, width, in_channels], which is a reshaped version of a Tensor of shape [num_uses, batch_size, height, width, in_channels].</li> <li>
<b><code>outputs</code></b>: A list of Tensors, each of shape [batch_size, height, width, out_channels]. Output produced by layer. The list indexes each use in the graph (which might correspond to a "time-step" in an RNN). Needs to correspond with the order used in <code>inputs</code>. OR, can be a single Tensor, of shape [num_uses * batch_size, height, width, out_channels], which is a reshaped version of a Tensor of shape [num_uses, batch_size, height, width, out_channels].</li> <li>
<b><code>num_uses</code></b>: int or None. The number uses/time-steps in the graph where the layer appears. Only needed if both inputs and outputs are given in the single Tensor format. (Default: None)</li> <li>
<b><code>data_format</code></b>: str or None. Format of data.</li> <li>
<b><code>dilations</code></b>: List of 4 ints. Dilations along each dimension.</li> <li>
<b><code>approx</code></b>: str or None. If not None must by "kron_indep". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Note that the word <code>use</code> here has a completely different meaning to "use in the graph" as it perturns to the <code>inputs</code>, <code>outputs</code>, and <code>num_uses</code> arguments.) (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_4">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="register_convolution"><code>register_convolution</code></h3> <pre class="prettyprint lang-python" data-language="python">register_convolution(
    params,
    inputs,
    outputs,
    padding,
    strides=None,
    dilation_rate=None,
    data_format=None,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Register a call to tf.nn.convolution().</p> <h4 id="args_7">Args:</h4> <ul> <li>
<b><code>params</code></b>: Tensor or 2-tuple of Tensors corresponding to weight and bias of this layer. Weight matrix should have shape [..filter_spatial_size.., in_channels, out_channels]. Bias should have shape [out_channels].</li> <li>
<b><code>inputs</code></b>: Tensor of shape [batch_size, ..input_spatial_size.., in_channels]. Inputs to layer.</li> <li>
<b><code>outputs</code></b>: Tensor of shape [batch_size, ..output_spatial_size.., out_channels]. Output produced by layer.</li> <li>
<b><code>padding</code></b>: string. see tf.nn.conv2d for valid values.</li> <li>
<b><code>strides</code></b>: List of ints of length len(..input_spatial_size..). Strides for convolution kernel in spatial dimensions.</li> <li>
<b><code>dilation_rate</code></b>: List of ints of length len(..input_spatial_size..). Dilations along spatial dimension.</li> <li>
<b><code>data_format</code></b>: str or None. Format of data.</li> <li>
<b><code>approx</code></b>: str or None. If not None must be one of "kron" or "diagonal". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_5">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="register_depthwise_conv2d"><code>register_depthwise_conv2d</code></h3> <pre class="prettyprint lang-python" data-language="python">register_depthwise_conv2d(
    params,
    inputs,
    outputs,
    strides,
    padding,
    rate=None,
    data_format=None,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Register a call to tf.nn.depthwise_conv2d().</p> <h4 id="args_8">Args:</h4> <ul> <li>
<b><code>params</code></b>: 4-D Tensor of shape [filter_height, filter_width, in_channels, channel_multiplier]. Convolutional filter.</li> <li>
<b><code>inputs</code></b>: Tensor of shape [batch_size, input_height, input_width, in_channels]. Inputs to layer.</li> <li>
<b><code>outputs</code></b>: Tensor of shape [batch_size, output_height, output_width, in_channels * channel_multiplier]. Output produced by depthwise conv2d.</li> <li>
<b><code>strides</code></b>: List of ints of length 4. Strides along all dimensions.</li> <li>
<b><code>padding</code></b>: string. see tf.nn.conv2d for valid values.</li> <li>
<b><code>rate</code></b>: None or List of ints of length 2. Dilation rates in spatial dimensions.</li> <li>
<b><code>data_format</code></b>: str or None. Format of data.</li> <li>
<b><code>approx</code></b>: str or None. If not None must "diagonal". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_6">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="register_embedding"><code>register_embedding</code></h3> <pre class="prettyprint lang-python" data-language="python">register_embedding(
    params,
    inputs,
    outputs,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers an embedding layer.</p> <h4 id="args_9">Args:</h4> <ul> <li>
<b><code>params</code></b>: Embedding matrix of shape [vocab_size, embedding_size].</li> <li>
<b><code>inputs</code></b>: Tensor of shape [batch_size, input_size] and dtype int32. Indices into embedding matrix.</li> <li>
<b><code>outputs</code></b>: Tensor of shape [batch_size, embedding_size]. Outputs produced by layer.</li> <li>
<b><code>approx</code></b>: str or None. If not None must be "kron". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_7">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="register_embedding_multi"><code>register_embedding_multi</code></h3> <pre class="prettyprint lang-python" data-language="python">register_embedding_multi(
    params,
    inputs,
    outputs,
    num_uses=None,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers embedding layers with shared parameters.</p> <h4 id="args_10">Args:</h4> <ul> <li>
<b><code>params</code></b>: Embedding matrix of shape [vocab_size, embedding_size].</li> <li>
<b><code>inputs</code></b>: A list of Tensors, each of shape [batch_size, input_size] and dtype int32. Indices into embedding matrix. The list indexes each use in the graph (which might correspond to a "time-step" in an RNN). OR, can be single Tensor, of shape [num_uses*batch_size, input_size], which is a reshaped version of a Tensor of shape [num_uses, batch_size, input_size].</li> <li>
<b><code>outputs</code></b>: A list of Tensors, each of shape [batch_size, embedding_size]. Outputs produced by layer. The list indexes each use in the graph (which might correspond to a "time-step" in an RNN). Needs to correspond with the order used in <code>inputs</code>. OR, can be a single Tensor, of shape [num_uses * batch_size, embedding_size], which is a reshaped version of a Tensor of shape [num_uses, batch_size, embedding_size].</li> <li>
<b><code>num_uses</code></b>: int or None. The number uses/time-steps in the graph where the layer appears. Only needed if both inputs and outputs are given in the single Tensor format. (Default: None)</li> <li>
<b><code>approx</code></b>: str or None. If not None must by "kron_indep". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Note that the word <code>use</code> here has a completely different meaning to "use in the graph" as it perturns to the <code>inputs</code>, <code>outputs</code>, and <code>num_uses</code> arguments.) (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_8">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="register_fully_connected"><code>register_fully_connected</code></h3> <pre class="prettyprint lang-python" data-language="python">register_fully_connected(
    params,
    inputs,
    outputs,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers a fully connnected layer.</p> <h4 id="args_11">Args:</h4> <ul> <li>
<b><code>params</code></b>: Tensor or 2-tuple of Tensors corresponding to weight and bias of this layer. Weight matrix should have shape [input_size, output_size]. Bias should have shape [output_size].</li> <li>
<b><code>inputs</code></b>: Tensor of shape [batch_size, input_size]. Inputs to layer.</li> <li>
<b><code>outputs</code></b>: Tensor of shape [batch_size, output_size]. Outputs produced by layer.</li> <li>
<b><code>approx</code></b>: str or None. If not None must be one of "kron" or "diagonal". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_9">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="register_fully_connected_multi"><code>register_fully_connected_multi</code></h3> <pre class="prettyprint lang-python" data-language="python">register_fully_connected_multi(
    params,
    inputs,
    outputs,
    num_uses=None,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Register fully connected layers with shared parameters.</p> <p>This can handle general fully-connected layers with shared parameters, but has specialized approximations to deal with the case where there is a meaningful linear order to the share instances (such as in an RNN).</p> <h4 id="args_12">Args:</h4> <ul> <li>
<b><code>params</code></b>: Tensor or 2-tuple of Tensors corresponding to weight and bias of this layer. Weight matrix should have shape [input_size, output_size]. Bias should have shape [output_size].</li> <li>
<b><code>inputs</code></b>: A list of Tensors, each of shape [batch_size, input_size]. Inputs to layer. The list indexes each use in the graph (which might correspond to a "time-step" in an RNN). OR, can be single Tensor, of shape [num_uses * batch_size , input_size], which is a reshaped version of a Tensor of shape [num_uses, batch_size, input_size].</li> <li>
<b><code>outputs</code></b>: A list of Tensors, the same length as <code>inputs</code>, each of shape [batch_size, output_size]. Outputs produced by layer. The list indexes each use in the graph (which might correspond to a "time-step" in an RNN). Needs to correspond with the order used in <code>inputs</code>. OR, can be a single Tensor of shape [num_uses * batch_size, output_size], which is a reshaped version of a Tensor of shape [num_uses, batch_size, output_size].</li> <li>
<b><code>num_uses</code></b>: int or None. The number uses/time-steps in the graph where the layer appears. Only needed if both inputs and outputs are given in the single Tensor format. (Default: None)</li> <li>
<b><code>approx</code></b>: str or None. If not None, must be of "kron_indep", "kron_series_1" or "kron_series_2". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Note that the word <code>use</code> here has a completely different meaning to "use in the graph" as it perturns to the <code>inputs</code>, <code>outputs</code>, and <code>num_uses</code> arguments.) (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_10">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> </ul> <h3 id="register_generic"><code>register_generic</code></h3> <pre class="prettyprint lang-python" data-language="python">register_generic(
    params,
    batch_size,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers a generic layer.</p> <h4 id="args_13">Args:</h4> <ul> <li>
<b><code>params</code></b>: Tensor or tuple of Tensors corresponding to the parameters.</li> <li>
<b><code>batch_size</code></b>: 0-D Tensor. Size of the minibatch (for this tower).</li> <li>
<b><code>approx</code></b>: str or None. It not None, must be one of "full" or "diagonal". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>batch_size</code> to the total mini-batch size use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_11">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="register_loss_function"><code>register_loss_function</code></h3> <pre class="prettyprint lang-python" data-language="python">register_loss_function(
    loss,
    colocation_op,
    base_name,
    name=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers a LossFunction object.</p> <h4 id="args_14">Args:</h4> <ul> <li>
<b><code>loss</code></b>: The LossFunction object.</li> <li>
<b><code>colocation_op</code></b>: The op to colocate the loss function's computations with.</li> <li>
<b><code>base_name</code></b>: The name to derive a new unique name from is the name argument is None.</li> <li>
<b><code>name</code></b>: (OPTIONAL) str or None. Unique name for this loss function. If None, a new name is generated. (Default: None)</li> <li>
<b><code>reuse</code></b>: (OPTIONAL) bool or str. If True, adds <code>loss</code> as an additional tower for the existing loss function.</li> </ul> <h4 id="raises_12">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If reuse == True and name == None.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and seed != None.</li> <li>
<b><code>KeyError</code></b>: If reuse == True and no existing LossFunction with <code>name</code> found.</li> <li>
<b><code>KeyError</code></b>: If reuse == False and existing LossFunction with <code>name</code> found.</li> </ul> <h3 id="register_multi_bernoulli_predictive_distribution"><code>register_multi_bernoulli_predictive_distribution</code></h3> <pre class="prettyprint lang-python" data-language="python">register_multi_bernoulli_predictive_distribution(
    logits,
    seed=None,
    targets=None,
    name=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers a multi-Bernoulli predictive distribution.</p> <h4 id="args_15">Args:</h4> <ul> <li>
<b><code>logits</code></b>: The logits of the distribution (i.e. its parameters).</li> <li>
<b><code>seed</code></b>: The seed for the RNG (for debugging) (Default: None)</li> <li>
<b><code>targets</code></b>: (OPTIONAL) The targets for the loss function. Only required if one wants to call total_loss() instead of total_sampled_loss(). total_loss() is required, for example, to estimate the "empirical Fisher" (instead of the true Fisher). (Default: None)</li> <li>
<b><code>name</code></b>: (OPTIONAL) str or None. Unique name for this loss function. If None, a new name is generated. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>logits</code> as an additional mini-batch/tower of inputs to the loss-function/predictive distribution (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h3 id="register_normal_predictive_distribution"><code>register_normal_predictive_distribution</code></h3> <pre class="prettyprint lang-python" data-language="python">register_normal_predictive_distribution(
    mean,
    var=0.5,
    seed=None,
    targets=None,
    name=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Registers a normal predictive distribution.</p> <h4 id="args_16">Args:</h4> <ul> <li>
<b><code>mean</code></b>: The mean vector defining the distribution.</li> <li>
<b><code>var</code></b>: The variance (must be a scalar). Note that the default value of 0.5 corresponds to a standard squared error loss (target - prediction)<strong>2. If your squared error loss is of the form 0.5*(target - prediction)</strong>2 you should use var=1.0. (Default: 0.5)</li> <li>
<b><code>seed</code></b>: The seed for the RNG (for debugging) (Default: None)</li> <li>
<b><code>targets</code></b>: (OPTIONAL) The targets for the loss function. Only required if one wants to call total_loss() instead of total_sampled_loss(). total_loss() is required, for example, to estimate the "empirical Fisher" (instead of the true Fisher). (Default: None)</li> <li>
<b><code>name</code></b>: (OPTIONAL) str or None. Unique name for this loss function. If None, a new name is generated. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>mean</code> and <code>var</code> as an additional mini-batch/tower of inputs to the loss-function/predictive distribution (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h3 id="register_separable_conv2d"><code>register_separable_conv2d</code></h3> <pre class="prettyprint lang-python" data-language="python">register_separable_conv2d(
    depthwise_params,
    pointwise_params,
    inputs,
    depthwise_outputs,
    pointwise_outputs,
    strides,
    padding,
    rate=None,
    data_format=None,
    approx=None,
    reuse=VARIABLE_SCOPE
)
</pre> <p>Register a call to tf.nn.separable_conv2d().</p> <blockquote class="note">
<strong>Note:</strong><span> This requires access to intermediate outputs between depthwise and pointwise convolutions.</span>
</blockquote> <h4 id="args_17">Args:</h4> <ul> <li>
<b><code>depthwise_params</code></b>: 4-D Tensor of shape [filter_height, filter_width, in_channels, channel_multiplier]. Filter for depthwise conv2d.</li> <li>
<b><code>pointwise_params</code></b>: 4-D Tensor of shape [1, 1, in_channels * channel_multiplier, out_channels]. Filter for pointwise conv2d.</li> <li>
<b><code>inputs</code></b>: Tensor of shape [batch_size, input_height, input_width, in_channels]. Inputs to layer.</li> <li>
<b><code>depthwise_outputs</code></b>: Tensor of shape [batch_size, output_height, output_width, in_channels * channel_multiplier]. Output produced by depthwise conv2d.</li> <li>
<b><code>pointwise_outputs</code></b>: Tensor of shape [batch_size, output_height, output_width, out_channels]. Output produced by pointwise conv2d.</li> <li>
<b><code>strides</code></b>: List of ints of length 4. Strides for depthwise conv2d kernel in all dimensions.</li> <li>
<b><code>padding</code></b>: string. see tf.nn.conv2d for valid values.</li> <li>
<b><code>rate</code></b>: None or List of ints of length 2. Dilation rate of depthwise conv2d kernel in spatial dimensions.</li> <li>
<b><code>data_format</code></b>: str or None. Format of data.</li> <li>
<b><code>approx</code></b>: str or None. If not None must be one of "kron" or "diagonal". The Fisher approximation to use. If None the default value is used. (Default: None)</li> <li>
<b><code>reuse</code></b>: bool or str. If True, this adds <code>inputs</code> and <code>outputs</code> as an additional mini-batch/tower of data to use when estimating the Fisher block for this layer (which must have already been registered). If "VARIABLE_SCOPE", use tf.get_variable_scope().reuse. (Default: "VARIABLE_SCOPE")</li> </ul> <h4 id="raises_13">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: For improper value to <code>approx</code>.</li> <li>
<b><code>KeyError</code></b>: If reuse == True but no FisherBlock found for <code>params</code>.</li> <li>
<b><code>ValueError</code></b>: If reuse == True and FisherBlock found but of the wrong type.</li> </ul> <h3 id="set_default_conv2d_approximation"><code>set_default_conv2d_approximation</code></h3> <pre class="prettyprint lang-python" data-language="python">set_default_conv2d_approximation(value)
</pre> <h3 id="set_default_embedding_approximation"><code>set_default_embedding_approximation</code></h3> <pre class="prettyprint lang-python" data-language="python">set_default_embedding_approximation(value)
</pre> <h3 id="set_default_fully_connected_approximation"><code>set_default_fully_connected_approximation</code></h3> <pre class="prettyprint lang-python" data-language="python">set_default_fully_connected_approximation(value)
</pre> <h3 id="set_default_fully_connected_multi_approximation"><code>set_default_fully_connected_multi_approximation</code></h3> <pre class="prettyprint lang-python" data-language="python">set_default_fully_connected_multi_approximation(value)
</pre> <h3 id="set_default_generic_approximation"><code>set_default_generic_approximation</code></h3> <pre class="prettyprint lang-python" data-language="python">set_default_generic_approximation(value)
</pre> <h3 id="total_loss"><code>total_loss</code></h3> <pre class="prettyprint lang-python" data-language="python">total_loss()
</pre> <h3 id="total_sampled_loss"><code>total_sampled_loss</code></h3> <pre class="prettyprint lang-python" data-language="python">total_sampled_loss()
</pre>
<div class="_attribution">
  <p class="_attribution-p">
    © 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/kfac/layer_collection/LayerCollection" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/contrib/kfac/layer_collection/LayerCollection</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
