
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>contrib.layers.optimize_loss - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Defined in tensorflow&#47;contrib&#47;layers&#47;python&#47;layers&#47;optimizers.py. ">
  <meta name="keywords" content="tf, contrib, layers, optimize, loss, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/tf/contrib/layers/optimize_loss.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.layers.optimize_loss </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.contrib.layers.optimize_loss"> <meta itemprop="path" content="r1.8"> </div> <pre class="prettyprint lang-python" data-language="python">tf.contrib.layers.optimize_loss(
    loss,
    global_step,
    learning_rate,
    optimizer,
    gradient_noise_scale=None,
    gradient_multipliers=None,
    clip_gradients=None,
    learning_rate_decay_fn=None,
    update_ops=None,
    variables=None,
    name=None,
    summaries=None,
    colocate_gradients_with_ops=False,
    increment_global_step=True
)
</pre> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/layers/python/layers/optimizers.py"><code>tensorflow/contrib/layers/python/layers/optimizers.py</code></a>.</p> <p>See the guide: <a href="https://www.tensorflow.org/api_guides/python/contrib.layers#Optimization">Layers (contrib) &gt; Optimization</a></p> <p>Given loss and parameters for optimizer, returns a training op.</p> <p>Various ways of passing optimizers include:</p> <ul> <li>by string specifying the name of the optimizer. See OPTIMIZER_CLS_NAMES for full list. E.g. <code>optimize_loss(..., optimizer='Adam')</code>.</li> <li>by function taking learning rate <code>Tensor</code> as argument and returning an <code>Optimizer</code> instance. E.g. <code>optimize_loss(..., optimizer=lambda lr: tf.train.MomentumOptimizer(lr, momentum=0.5))</code>. Alternatively, if <code>learning_rate</code> is <code>None</code>, the function takes no arguments. E.g. <code>optimize_loss(..., learning_rate=None, optimizer=lambda: tf.train.MomentumOptimizer(0.5, momentum=0.5))</code>.</li> <li>by a subclass of <code>Optimizer</code> having a single-argument constructor (the argument is the learning rate), such as AdamOptimizer or AdagradOptimizer. E.g. <code>optimize_loss(..., optimizer=tf.train.AdagradOptimizer)</code>.</li> <li>by an instance of a subclass of <code>Optimizer</code>. E.g., <code>optimize_loss(..., optimizer=tf.train.AdagradOptimizer(0.5))</code>.</li> </ul> <h4 id="args">Args:</h4> <ul> <li>
<b><code>loss</code></b>: Scalar <code>Tensor</code>.</li> <li>
<b><code>global_step</code></b>: Scalar int <code>Tensor</code>, step counter to update on each step unless <code>increment_global_step</code> is <code>False</code>. If not supplied, it will be fetched from the default graph (see <a href="../../train/get_global_step"><code>tf.train.get_global_step</code></a> for details). If it has not been created, no step will be incremented with each weight update. <code>learning_rate_decay_fn</code> requires <code>global_step</code>.</li> <li>
<b><code>learning_rate</code></b>: float or <code>Tensor</code>, magnitude of update per each training step. Can be <code>None</code>.</li> <li>
<b><code>optimizer</code></b>: string, class or optimizer instance, used as trainer. string should be name of optimizer, like 'SGD', 'Adam', 'Adagrad'. Full list in OPTIMIZER_CLS_NAMES constant. class should be sub-class of <code>tf.Optimizer</code> that implements <code>compute_gradients</code> and <code>apply_gradients</code> functions. optimizer instance should be instantiation of <code>tf.Optimizer</code> sub-class and have <code>compute_gradients</code> and <code>apply_gradients</code> functions.</li> <li>
<b><code>gradient_noise_scale</code></b>: float or None, adds 0-mean normal noise scaled by this value.</li> <li>
<b><code>gradient_multipliers</code></b>: dict of variables or variable names to floats. If present, gradients for specified variables will be multiplied by given constant.</li> <li>
<b><code>clip_gradients</code></b>: float, callable or <code>None</code>. If float, is provided, a global clipping is applied to prevent the norm of the gradient to exceed this value. Alternatively, a callable can be provided e.g.: adaptive_clipping. This callable takes a <code>list</code> of <code>(gradients, variables)</code> <code>tuple</code>s and returns the same thing with the gradients modified.</li> <li>
<b><code>learning_rate_decay_fn</code></b>: function, takes <code>learning_rate</code> and <code>global_step</code> <code>Tensor</code>s, returns <code>Tensor</code>. Can be used to implement any learning rate decay functions. For example: <a href="../../train/exponential_decay"><code>tf.train.exponential_decay</code></a>. Ignored if <code>learning_rate</code> is not supplied.</li> <li>
<b><code>update_ops</code></b>: list of update <code>Operation</code>s to execute at each step. If <code>None</code>, uses elements of UPDATE_OPS collection. The order of execution between <code>update_ops</code> and <code>loss</code> is non-deterministic.</li> <li>
<b><code>variables</code></b>: list of variables to optimize or <code>None</code> to use all trainable variables.</li> <li>
<b><code>name</code></b>: The name for this operation is used to scope operations and summaries.</li> <li>
<b><code>summaries</code></b>: List of internal quantities to visualize on tensorboard. If not set, the loss, the learning rate, and the global norm of the gradients will be reported. The complete list of possible values is in OPTIMIZER_SUMMARIES.</li> <li>
<b><code>colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li> <li>
<b><code>increment_global_step</code></b>: Whether to increment <code>global_step</code>. If your model calls <code>optimize_loss</code> multiple times per training step (e.g. to optimize different parts of the model), use this arg to avoid incrementing <code>global_step</code> more times than necessary.</li> </ul> <h4 id="returns">Returns:</h4> <p>Training op.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: if:<ul> <li>
<code>loss</code> is an invalid type or shape.</li> <li>
<code>global_step</code> is an invalid type or shape.</li> <li>
<code>learning_rate</code> is an invalid type or value.</li> <li>
<code>optimizer</code> has the wrong type.</li> <li>
<code>clip_gradients</code> is neither float nor callable.</li> <li>
<code>learning_rate</code> and <code>learning_rate_decay_fn</code> are supplied, but no <code>global_step</code> is available.</li> <li>
<code>gradients</code> is empty.</li> </ul> </li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/optimize_loss" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/contrib/layers/optimize_loss</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
