
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Seq2seq Library - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Module for constructing seq2seq models and dynamic decoding. Builds on top of libraries in tf.contrib.rnn. ">
  <meta name="keywords" content="seq, library, contrib, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/contrib.seq2seq.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> Seq2seq Library (contrib) </h1>     <p>Module for constructing seq2seq models and dynamic decoding. Builds on top of libraries in <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn"><code>tf.contrib.rnn</code></a>.</p> <p>This library is composed of two primary components:</p> <ul> <li>New attention wrappers for <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell"><code>tf.contrib.rnn.RNNCell</code></a> objects.</li> <li>A new object-oriented dynamic decoding framework.</li> </ul> <h2 id="Attention">Attention</h2> <p>Attention wrappers are <code>RNNCell</code> objects that wrap other <code>RNNCell</code> objects and implement attention. The form of attention is determined by a subclass of <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionMechanism"><code>tf.contrib.seq2seq.AttentionMechanism</code></a>. These subclasses describe the form of attention (e.g. additive vs. multiplicative) to use when creating the wrapper. An instance of an <code>AttentionMechanism</code> is constructed with a <code>memory</code> tensor, from which lookup keys and values tensors are created.</p> <h3 id="attention_mechanisms">Attention Mechanisms</h3> <p>The two basic attention mechanisms are: <em> <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention"><code>tf.contrib.seq2seq.BahdanauAttention</code></a> (additive attention, <a href="https://arxiv.org/abs/1409.0473">ref.</a>) </em> <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/LuongAttention"><code>tf.contrib.seq2seq.LuongAttention</code></a> (multiplicative attention, <a href="https://arxiv.org/abs/1508.04025">ref.</a>)</p> <p>The <code>memory</code> tensor passed the attention mechanism's constructor is expected to be shaped <code>[batch_size, memory_max_time, memory_depth]</code>; and often an additional <code>memory_sequence_length</code> vector is accepted. If provided, the <code>memory</code> tensors' rows are masked with zeros past their true sequence lengths.</p> <p>Attention mechanisms also have a concept of depth, usually determined as a construction parameter <code>num_units</code>. For some kinds of attention (like <code>BahdanauAttention</code>), both queries and memory are projected to tensors of depth <code>num_units</code>. For other kinds (like <code>LuongAttention</code>), <code>num_units</code> should match the depth of the queries; and the <code>memory</code> tensor will be projected to this depth.</p> <h3 id="attention_wrappers">Attention Wrappers</h3> <p>The basic attention wrapper is <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper"><code>tf.contrib.seq2seq.AttentionWrapper</code></a>. This wrapper accepts an <code>RNNCell</code> instance, an instance of <code>AttentionMechanism</code>, and an attention depth parameter (<code>attention_size</code>); as well as several optional arguments that allow one to customize intermediate calculations.</p> <p>At each time step, the basic calculation performed by this wrapper is:</p> <pre class="prettyprint lang-python" data-language="python">cell_inputs = concat([inputs, prev_state.attention], -1)
cell_output, next_cell_state = cell(cell_inputs, prev_state.cell_state)
score = attention_mechanism(cell_output)
alignments = softmax(score)
context = matmul(alignments, attention_mechanism.values)
attention = tf.layers.Dense(attention_size)(concat([cell_output, context], 1))
next_state = AttentionWrapperState(
  cell_state=next_cell_state,
  attention=attention)
output = attention
return output, next_state
</pre> <p>In practice, a number of the intermediate calculations are configurable. For example, the initial concatenation of <code>inputs</code> and <code>prev_state.attention</code> can be replaced with another mixing function. The function <code>softmax</code> can be replaced with alternative options when calculating <code>alignments</code> from the <code>score</code>. Finally, the outputs returned by the wrapper can be configured to be the value <code>cell_output</code> instead of <code>attention</code>.</p> <p>The benefit of using a <code>AttentionWrapper</code> is that it plays nicely with other wrappers and the dynamic decoder described below. For example, one can write:</p> <pre class="prettyprint lang-python" data-language="python">cell = tf.contrib.rnn.DeviceWrapper(LSTMCell(512), "/device:GPU:0")
attention_mechanism = tf.contrib.seq2seq.LuongAttention(512, encoder_outputs)
attn_cell = tf.contrib.seq2seq.AttentionWrapper(
  cell, attention_mechanism, attention_size=256)
attn_cell = tf.contrib.rnn.DeviceWrapper(attn_cell, "/device:GPU:1")
top_cell = tf.contrib.rnn.DeviceWrapper(LSTMCell(512), "/device:GPU:1")
multi_cell = MultiRNNCell([attn_cell, top_cell])
</pre> <p>The <code>multi_rnn</code> cell will perform the bottom layer calculations on GPU 0; attention calculations will be performed on GPU 1 and immediately passed up to the top layer which is also calculated on GPU 1. The attention is also passed forward in time to the next time step and copied to GPU 0 for the next time step of <code>cell</code>. (<em>Note</em>: This is just an example of use, not a suggested device partitioning strategy.)</p> <h2 id="Dynamic_Decoding">Dynamic Decoding</h2> <p>Example usage:</p> <pre class="prettyprint lang-python" data-language="python">cell = # instance of RNNCell

if mode == "train":
  helper = tf.contrib.seq2seq.TrainingHelper(
    input=input_vectors,
    sequence_length=input_lengths)
elif mode == "infer":
  helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
      embedding=embedding,
      start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
      end_token=END_SYMBOL)

decoder = tf.contrib.seq2seq.BasicDecoder(
    cell=cell,
    helper=helper,
    initial_state=cell.zero_state(batch_size, tf.float32))
outputs, _ = tf.contrib.seq2seq.dynamic_decode(
   decoder=decoder,
   output_time_major=False,
   impute_finished=True,
   maximum_iterations=20)
</pre> <h3 id="decoder_base_class_and_functions">Decoder base class and functions</h3> <ul> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/Decoder"><code>tf.contrib.seq2seq.Decoder</code></a></li> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode"><code>tf.contrib.seq2seq.dynamic_decode</code></a></li> </ul> <h3 id="basic_decoder">Basic Decoder</h3> <ul> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoderOutput"><code>tf.contrib.seq2seq.BasicDecoderOutput</code></a></li> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder"><code>tf.contrib.seq2seq.BasicDecoder</code></a></li> </ul> <h3 id="decoder_helpers">Decoder Helpers</h3> <ul> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/Helper"><code>tf.contrib.seq2seq.Helper</code></a></li> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/CustomHelper"><code>tf.contrib.seq2seq.CustomHelper</code></a></li> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper"><code>tf.contrib.seq2seq.GreedyEmbeddingHelper</code></a></li> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/ScheduledEmbeddingTrainingHelper"><code>tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper</code></a></li> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/ScheduledOutputTrainingHelper"><code>tf.contrib.seq2seq.ScheduledOutputTrainingHelper</code></a></li> <li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper"><code>tf.contrib.seq2seq.TrainingHelper</code></a></li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    © 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_guides/python/contrib.seq2seq" class="_attribution-link">https://www.tensorflow.org/api_guides/python/contrib.seq2seq</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
