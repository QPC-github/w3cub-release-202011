
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>torchvision.models - PyTorch - W3cubDocs</title>
  
  <meta name="description" content="The models subpackage contains definitions of models for addressing different tasks, including&#58; image classification, pixelwise semantic &hellip;">
  <meta name="keywords" content="torchvision, models, pytorch">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/pytorch/torchvision/models.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/pytorch.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/pytorch/" class="_nav-link" title="" style="margin-left:0;">PyTorch</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="torchvision-models">torchvision.models</h1> <p>The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.</p>  <h2 id="classification">Classification</h2> <p>The models subpackage contains definitions for the following model architectures for image classification:</p> <ul class="simple"> <li><a class="reference external" href="https://arxiv.org/abs/1404.5997">AlexNet</a></li> <li><a class="reference external" href="https://arxiv.org/abs/1409.1556">VGG</a></li> <li><a class="reference external" href="https://arxiv.org/abs/1512.03385">ResNet</a></li> <li><a class="reference external" href="https://arxiv.org/abs/1602.07360">SqueezeNet</a></li> <li><a class="reference external" href="https://arxiv.org/abs/1608.06993">DenseNet</a></li> <li>
<a class="reference external" href="https://arxiv.org/abs/1512.00567">Inception</a> v3</li> <li><a class="reference external" href="https://arxiv.org/abs/1409.4842">GoogLeNet</a></li> <li>
<a class="reference external" href="https://arxiv.org/abs/1807.11164">ShuffleNet</a> v2</li> <li>
<a class="reference external" href="https://arxiv.org/abs/1801.04381">MobileNet</a> v2</li> <li><a class="reference external" href="https://arxiv.org/abs/1611.05431">ResNeXt</a></li> <li><a class="reference internal" href="#wide-resnet">Wide ResNet</a></li> <li><a class="reference external" href="https://arxiv.org/abs/1807.11626">MNASNet</a></li> </ul> <p>You can construct a model with random weights by calling its constructor:</p> <pre data-language="python">import torchvision.models as models
resnet18 = models.resnet18()
alexnet = models.alexnet()
vgg16 = models.vgg16()
squeezenet = models.squeezenet1_0()
densenet = models.densenet161()
inception = models.inception_v3()
googlenet = models.googlenet()
shufflenet = models.shufflenet_v2_x1_0()
mobilenet = models.mobilenet_v2()
resnext50_32x4d = models.resnext50_32x4d()
wide_resnet50_2 = models.wide_resnet50_2()
mnasnet = models.mnasnet1_0()
</pre> <p>We provide pre-trained models, using the PyTorch <a class="reference internal" href="../model_zoo#module-torch.utils.model_zoo" title="torch.utils.model_zoo"><code>torch.utils.model_zoo</code></a>. These can be constructed by passing <code>pretrained=True</code>:</p> <pre data-language="python">import torchvision.models as models
resnet18 = models.resnet18(pretrained=True)
alexnet = models.alexnet(pretrained=True)
squeezenet = models.squeezenet1_0(pretrained=True)
vgg16 = models.vgg16(pretrained=True)
densenet = models.densenet161(pretrained=True)
inception = models.inception_v3(pretrained=True)
googlenet = models.googlenet(pretrained=True)
shufflenet = models.shufflenet_v2_x1_0(pretrained=True)
mobilenet = models.mobilenet_v2(pretrained=True)
resnext50_32x4d = models.resnext50_32x4d(pretrained=True)
wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)
mnasnet = models.mnasnet1_0(pretrained=True)
</pre> <p>Instancing a pre-trained model will download its weights to a cache directory. This directory can be set using the <code>TORCH_MODEL_ZOO</code> environment variable. See <a class="reference internal" href="../model_zoo#torch.utils.model_zoo.load_url" title="torch.utils.model_zoo.load_url"><code>torch.utils.model_zoo.load_url()</code></a> for details.</p> <p>Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use <code>model.train()</code> or <code>model.eval()</code> as appropriate. See <a class="reference internal" href="../generated/torch.nn.module#torch.nn.Module.train" title="torch.nn.Module.train"><code>train()</code></a> or <a class="reference internal" href="../generated/torch.nn.module#torch.nn.Module.eval" title="torch.nn.Module.eval"><code>eval()</code></a> for details.</p> <p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>. You can use the following transform to normalize:</p> <pre data-language="python">normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
</pre> <p>An example of such normalization can be found in the imagenet example <a class="reference external" href="https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101">here</a></p> <p>The process for obtaining the values of <code>mean</code> and <code>std</code> is roughly equivalent to:</p> <pre data-language="python">import torch
from torchvision import datasets, transforms as T

transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])
dataset = datasets.ImageNet(".", split="train", transform=transform)

means = []
stds = []
for img in subset(dataset):
    means.append(torch.mean(img))
    stds.append(torch.std(img))

mean = torch.mean(torch.tensor(means))
std = torch.mean(torch.tensor(stds))
</pre> <p>Unfortunately, the concrete <code>subset</code> that was used is lost. For more information see <a class="reference external" href="https://github.com/pytorch/vision/issues/1439">this discussion</a> or <a class="reference external" href="https://github.com/pytorch/vision/pull/1965">these experiments</a>.</p> <p>ImageNet 1-crop error rates (224x224)</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>Network</p></th> <th class="head"><p>Top-1 error</p></th> <th class="head"><p>Top-5 error</p></th> </tr> </thead>  <tr>
<td><p>AlexNet</p></td> <td><p>43.45</p></td> <td><p>20.91</p></td> </tr> <tr>
<td><p>VGG-11</p></td> <td><p>30.98</p></td> <td><p>11.37</p></td> </tr> <tr>
<td><p>VGG-13</p></td> <td><p>30.07</p></td> <td><p>10.75</p></td> </tr> <tr>
<td><p>VGG-16</p></td> <td><p>28.41</p></td> <td><p>9.62</p></td> </tr> <tr>
<td><p>VGG-19</p></td> <td><p>27.62</p></td> <td><p>9.12</p></td> </tr> <tr>
<td><p>VGG-11 with batch normalization</p></td> <td><p>29.62</p></td> <td><p>10.19</p></td> </tr> <tr>
<td><p>VGG-13 with batch normalization</p></td> <td><p>28.45</p></td> <td><p>9.63</p></td> </tr> <tr>
<td><p>VGG-16 with batch normalization</p></td> <td><p>26.63</p></td> <td><p>8.50</p></td> </tr> <tr>
<td><p>VGG-19 with batch normalization</p></td> <td><p>25.76</p></td> <td><p>8.15</p></td> </tr> <tr>
<td><p>ResNet-18</p></td> <td><p>30.24</p></td> <td><p>10.92</p></td> </tr> <tr>
<td><p>ResNet-34</p></td> <td><p>26.70</p></td> <td><p>8.58</p></td> </tr> <tr>
<td><p>ResNet-50</p></td> <td><p>23.85</p></td> <td><p>7.13</p></td> </tr> <tr>
<td><p>ResNet-101</p></td> <td><p>22.63</p></td> <td><p>6.44</p></td> </tr> <tr>
<td><p>ResNet-152</p></td> <td><p>21.69</p></td> <td><p>5.94</p></td> </tr> <tr>
<td><p>SqueezeNet 1.0</p></td> <td><p>41.90</p></td> <td><p>19.58</p></td> </tr> <tr>
<td><p>SqueezeNet 1.1</p></td> <td><p>41.81</p></td> <td><p>19.38</p></td> </tr> <tr>
<td><p>Densenet-121</p></td> <td><p>25.35</p></td> <td><p>7.83</p></td> </tr> <tr>
<td><p>Densenet-169</p></td> <td><p>24.00</p></td> <td><p>7.00</p></td> </tr> <tr>
<td><p>Densenet-201</p></td> <td><p>22.80</p></td> <td><p>6.43</p></td> </tr> <tr>
<td><p>Densenet-161</p></td> <td><p>22.35</p></td> <td><p>6.20</p></td> </tr> <tr>
<td><p>Inception v3</p></td> <td><p>22.55</p></td> <td><p>6.44</p></td> </tr> <tr>
<td><p>GoogleNet</p></td> <td><p>30.22</p></td> <td><p>10.47</p></td> </tr> <tr>
<td><p>ShuffleNet V2</p></td> <td><p>30.64</p></td> <td><p>11.68</p></td> </tr> <tr>
<td><p>MobileNet V2</p></td> <td><p>28.12</p></td> <td><p>9.71</p></td> </tr> <tr>
<td><p>ResNeXt-50-32x4d</p></td> <td><p>22.38</p></td> <td><p>6.30</p></td> </tr> <tr>
<td><p>ResNeXt-101-32x8d</p></td> <td><p>20.69</p></td> <td><p>5.47</p></td> </tr> <tr>
<td><p>Wide ResNet-50-2</p></td> <td><p>21.49</p></td> <td><p>5.91</p></td> </tr> <tr>
<td><p>Wide ResNet-101-2</p></td> <td><p>21.16</p></td> <td><p>5.72</p></td> </tr> <tr>
<td><p>MNASNet 1.0</p></td> <td><p>26.49</p></td> <td><p>8.456</p></td> </tr>  </table>  <h3 id="id1">Alexnet</h3> <dl class="function"> <dt id="torchvision.models.alexnet">
<code>torchvision.models.alexnet(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/alexnet.html#alexnet"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>AlexNet model architecture from the <a class="reference external" href="https://arxiv.org/abs/1404.5997">“One weird trick…”</a> paper.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="id2">VGG</h3> <dl class="function"> <dt id="torchvision.models.vgg11">
<code>torchvision.models.vgg11(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>VGG 11-layer model (configuration “A”) from <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.vgg11_bn">
<code>torchvision.models.vgg11_bn(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11_bn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>VGG 11-layer model (configuration “A”) with batch normalization <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.vgg13">
<code>torchvision.models.vgg13(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>VGG 13-layer model (configuration “B”) <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.vgg13_bn">
<code>torchvision.models.vgg13_bn(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13_bn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>VGG 13-layer model (configuration “B”) with batch normalization <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.vgg16">
<code>torchvision.models.vgg16(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>VGG 16-layer model (configuration “D”) <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.vgg16_bn">
<code>torchvision.models.vgg16_bn(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16_bn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>VGG 16-layer model (configuration “D”) with batch normalization <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.vgg19">
<code>torchvision.models.vgg19(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>VGG 19-layer model (configuration “E”) <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.vgg19_bn">
<code>torchvision.models.vgg19_bn(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19_bn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>VGG 19-layer model (configuration ‘E’) with batch normalization <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="id10">ResNet</h3> <dl class="function"> <dt id="torchvision.models.resnet18">
<code>torchvision.models.resnet18(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet18"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>ResNet-18 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.resnet34">
<code>torchvision.models.resnet34(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet34"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>ResNet-34 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.resnet50">
<code>torchvision.models.resnet50(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet50"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>ResNet-50 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.resnet101">
<code>torchvision.models.resnet101(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet101"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>ResNet-101 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.resnet152">
<code>torchvision.models.resnet152(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet152"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>ResNet-152 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="id15">SqueezeNet</h3> <dl class="function"> <dt id="torchvision.models.squeezenet1_0">
<code>torchvision.models.squeezenet1_0(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_0"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>SqueezeNet model architecture from the <a class="reference external" href="https://arxiv.org/abs/1602.07360">“SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size”</a> paper.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.squeezenet1_1">
<code>torchvision.models.squeezenet1_1(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_1"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>SqueezeNet 1.1 model from the <a class="reference external" href="https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1">official SqueezeNet repo</a>. SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters than SqueezeNet 1.0, without sacrificing accuracy.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="id16">DenseNet</h3> <dl class="function"> <dt id="torchvision.models.densenet121">
<code>torchvision.models.densenet121(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet121"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Densenet-121 model from <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">“Densely Connected Convolutional Networks”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<strong>memory_efficient</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – but slower. Default: <em>False</em>. See <a class="reference external" href="https://arxiv.org/pdf/1707.06990.pdf">“paper”</a>
</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.densenet169">
<code>torchvision.models.densenet169(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet169"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Densenet-169 model from <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">“Densely Connected Convolutional Networks”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<p><strong>memory_efficient</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – </p>
<p>but slower. Default: <em>False</em>. See <a class="reference external" href="https://arxiv.org/pdf/1707.06990.pdf">“paper”</a></p> </li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.densenet161">
<code>torchvision.models.densenet161(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet161"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Densenet-161 model from <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">“Densely Connected Convolutional Networks”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<p><strong>memory_efficient</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – </p>
<p>but slower. Default: <em>False</em>. See <a class="reference external" href="https://arxiv.org/pdf/1707.06990.pdf">“paper”</a></p> </li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.densenet201">
<code>torchvision.models.densenet201(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet201"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Densenet-201 model from <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">“Densely Connected Convolutional Networks”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<p><strong>memory_efficient</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – </p>
<p>but slower. Default: <em>False</em>. See <a class="reference external" href="https://arxiv.org/pdf/1707.06990.pdf">“paper”</a></p> </li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="inception-v3">Inception v3</h3> <dl class="function"> <dt id="torchvision.models.inception_v3">
<code>torchvision.models.inception_v3(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/inception.html#inception_v3"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Inception v3 model architecture from <a class="reference external" href="http://arxiv.org/abs/1512.00567">“Rethinking the Inception Architecture for Computer Vision”</a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p><strong>Important</strong>: In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<strong>aux_logits</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, add an auxiliary branch that can improve training. Default: <em>True</em>
</li> <li>
<strong>transform_input</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, preprocesses the input according to the method with which it was trained on ImageNet. Default: <em>False</em>
</li> </ul> </dd> </dl> </dd>
</dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This requires <code>scipy</code> to be installed</p> </div>   <h3 id="id23">GoogLeNet</h3> <dl class="function"> <dt id="torchvision.models.googlenet">
<code>torchvision.models.googlenet(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/googlenet.html#googlenet"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>GoogLeNet (Inception v1) model architecture from <a class="reference external" href="http://arxiv.org/abs/1409.4842">“Going Deeper with Convolutions”</a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<strong>aux_logits</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, adds two auxiliary branches that can improve training. Default: <em>False</em> when pretrained is True otherwise <em>True</em>
</li> <li>
<strong>transform_input</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, preprocesses the input according to the method with which it was trained on ImageNet. Default: <em>False</em>
</li> </ul> </dd> </dl> </dd>
</dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This requires <code>scipy</code> to be installed</p> </div>   <h3 id="shufflenet-v2">ShuffleNet v2</h3> <dl class="function"> <dt id="torchvision.models.shufflenet_v2_x0_5">
<code>torchvision.models.shufflenet_v2_x0_5(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x0_5"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a ShuffleNetV2 with 0.5x output channels, as described in <a class="reference external" href="https://arxiv.org/abs/1807.11164">“ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design”</a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.shufflenet_v2_x1_0">
<code>torchvision.models.shufflenet_v2_x1_0(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_0"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a ShuffleNetV2 with 1.0x output channels, as described in <a class="reference external" href="https://arxiv.org/abs/1807.11164">“ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design”</a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.shufflenet_v2_x1_5">
<code>torchvision.models.shufflenet_v2_x1_5(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_5"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a ShuffleNetV2 with 1.5x output channels, as described in <a class="reference external" href="https://arxiv.org/abs/1807.11164">“ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design”</a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.shufflenet_v2_x2_0">
<code>torchvision.models.shufflenet_v2_x2_0(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x2_0"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a ShuffleNetV2 with 2.0x output channels, as described in <a class="reference external" href="https://arxiv.org/abs/1807.11164">“ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design”</a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="mobilenet-v2">MobileNet v2</h3> <dl class="function"> <dt id="torchvision.models.mobilenet_v2">
<code>torchvision.models.mobilenet_v2(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mobilenet.html#mobilenet_v2"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a MobileNetV2 architecture from <a class="reference external" href="https://arxiv.org/abs/1801.04381">“MobileNetV2: Inverted Residuals and Linear Bottlenecks”</a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="id27">ResNext</h3> <dl class="function"> <dt id="torchvision.models.resnext50_32x4d">
<code>torchvision.models.resnext50_32x4d(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext50_32x4d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>ResNeXt-50 32x4d model from <a class="reference external" href="https://arxiv.org/pdf/1611.05431.pdf">“Aggregated Residual Transformation for Deep Neural Networks”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.resnext101_32x8d">
<code>torchvision.models.resnext101_32x8d(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext101_32x8d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>ResNeXt-101 32x8d model from <a class="reference external" href="https://arxiv.org/pdf/1611.05431.pdf">“Aggregated Residual Transformation for Deep Neural Networks”</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="wide-resnet">Wide ResNet</h3> <dl class="function"> <dt id="torchvision.models.wide_resnet50_2">
<code>torchvision.models.wide_resnet50_2(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet50_2"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wide ResNet-50-2 model from <a class="reference external" href="https://arxiv.org/pdf/1605.07146.pdf">“Wide Residual Networks”</a></p> <p>The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.wide_resnet101_2">
<code>torchvision.models.wide_resnet101_2(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet101_2"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wide ResNet-101-2 model from <a class="reference external" href="https://arxiv.org/pdf/1605.07146.pdf">“Wide Residual Networks”</a></p> <p>The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on ImageNet</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="id30">MNASNet</h3> <dl class="function"> <dt id="torchvision.models.mnasnet0_5">
<code>torchvision.models.mnasnet0_5(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_5"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>MNASNet with depth multiplier of 0.5 from <a class="reference external" href="https://arxiv.org/pdf/1807.11626.pdf">“MnasNet: Platform-Aware Neural Architecture Search for Mobile”</a>. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</p> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.mnasnet0_75">
<code>torchvision.models.mnasnet0_75(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_75"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>MNASNet with depth multiplier of 0.75 from <a class="reference external" href="https://arxiv.org/pdf/1807.11626.pdf">“MnasNet: Platform-Aware Neural Architecture Search for Mobile”</a>. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</p> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.mnasnet1_0">
<code>torchvision.models.mnasnet1_0(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_0"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>MNASNet with depth multiplier of 1.0 from <a class="reference external" href="https://arxiv.org/pdf/1807.11626.pdf">“MnasNet: Platform-Aware Neural Architecture Search for Mobile”</a>. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</p> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.mnasnet1_3">
<code>torchvision.models.mnasnet1_3(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_3"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>MNASNet with depth multiplier of 1.3 from <a class="reference external" href="https://arxiv.org/pdf/1807.11626.pdf">“MnasNet: Platform-Aware Neural Architecture Search for Mobile”</a>. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</p> </dd>
</dl>    <h2 id="semantic-segmentation">Semantic Segmentation</h2> <p>The models subpackage contains definitions for the following model architectures for semantic segmentation:</p> <ul class="simple"> <li><a class="reference external" href="https://arxiv.org/abs/1411.4038">FCN ResNet50, ResNet101</a></li> <li><a class="reference external" href="https://arxiv.org/abs/1706.05587">DeepLabV3 ResNet50, ResNet101</a></li> </ul> <p>As with image classification models, all pre-trained models expect input images normalized in the same way. The images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>. They have been trained on images resized such that their minimum size is 520.</p> <p>The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in <code>references/segmentation/coco_utils.py</code>. The classes that the pre-trained model outputs are the following, in order:</p>  <pre data-language="python">['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',
 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',
 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']
</pre>  <p>The accuracies of the pre-trained models evaluated on COCO val2017 are as follows</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>Network</p></th> <th class="head"><p>mean IoU</p></th> <th class="head"><p>global pixelwise acc</p></th> </tr> </thead>  <tr>
<td><p>FCN ResNet50</p></td> <td><p>60.5</p></td> <td><p>91.4</p></td> </tr> <tr>
<td><p>FCN ResNet101</p></td> <td><p>63.7</p></td> <td><p>91.9</p></td> </tr> <tr>
<td><p>DeepLabV3 ResNet50</p></td> <td><p>66.4</p></td> <td><p>92.4</p></td> </tr> <tr>
<td><p>DeepLabV3 ResNet101</p></td> <td><p>67.4</p></td> <td><p>92.4</p></td> </tr>  </table>  <h3 id="fully-convolutional-networks">Fully Convolutional Networks</h3> <dl class="function"> <dt id="torchvision.models.segmentation.fcn_resnet50">
<code>torchvision.models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet50"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.segmentation.fcn_resnet101">
<code>torchvision.models.segmentation.fcn_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet101"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="deeplabv3">DeepLabV3</h3> <dl class="function"> <dt id="torchvision.models.segmentation.deeplabv3_resnet50">
<code>torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet50"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a DeepLabV3 model with a ResNet-50 backbone.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torchvision.models.segmentation.deeplabv3_resnet101">
<code>torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet101"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a DeepLabV3 model with a ResNet-101 backbone.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>    <h2 id="object-detection-instance-segmentation-and-person-keypoint-detection">Object Detection, Instance Segmentation and Person Keypoint Detection</h2> <p>The models subpackage contains definitions for the following model architectures for detection:</p> <ul class="simple"> <li><a class="reference external" href="https://arxiv.org/abs/1506.01497">Faster R-CNN ResNet-50 FPN</a></li> <li><a class="reference external" href="https://arxiv.org/abs/1703.06870">Mask R-CNN ResNet-50 FPN</a></li> </ul> <p>The pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision.</p> <p>The models expect a list of <code>Tensor[C, H, W]</code>, in the range <code>0-1</code>. The models internally resize the images so that they have a minimum size of <code>800</code>. This option can be changed by passing the option <code>min_size</code> to the constructor of the models.</p> <p>For object detection and instance segmentation, the pre-trained models return the predictions of the following classes:</p>  <pre data-language="python">COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',
    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]
</pre>  <p>Here are the summary of the accuracies for the models trained on the instances set of COCO train2017 and evaluated on COCO val2017.</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>Network</p></th> <th class="head"><p>box AP</p></th> <th class="head"><p>mask AP</p></th> <th class="head"><p>keypoint AP</p></th> </tr> </thead>  <tr>
<td><p>Faster R-CNN ResNet-50 FPN</p></td> <td><p>37.0</p></td> <td>
<ul class="simple"> <li> </li>
</ul> </td> <td>
<ul class="simple"> <li> </li>
</ul> </td> </tr> <tr>
<td><p>RetinaNet ResNet-50 FPN</p></td> <td><p>36.4</p></td> <td>
<ul class="simple"> <li> </li>
</ul> </td> <td>
<ul class="simple"> <li> </li>
</ul> </td> </tr> <tr>
<td><p>Mask R-CNN ResNet-50 FPN</p></td> <td><p>37.9</p></td> <td><p>34.6</p></td> <td>
<ul class="simple"> <li> </li>
</ul> </td> </tr>  </table> <p>For person keypoint detection, the accuracies for the pre-trained models are as follows</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>Network</p></th> <th class="head"><p>box AP</p></th> <th class="head"><p>mask AP</p></th> <th class="head"><p>keypoint AP</p></th> </tr> </thead>  <tr>
<td><p>Keypoint R-CNN ResNet-50 FPN</p></td> <td><p>54.6</p></td> <td>
<ul class="simple"> <li> </li>
</ul> </td> <td><p>65.0</p></td> </tr>  </table> <p>For person keypoint detection, the pre-trained model return the keypoints in the following order:</p>  <pre data-language="python">COCO_PERSON_KEYPOINT_NAMES = [
    'nose',
    'left_eye',
    'right_eye',
    'left_ear',
    'right_ear',
    'left_shoulder',
    'right_shoulder',
    'left_elbow',
    'right_elbow',
    'left_wrist',
    'right_wrist',
    'left_hip',
    'right_hip',
    'left_knee',
    'right_knee',
    'left_ankle',
    'right_ankle'
]
</pre>   <h3 id="runtime-characteristics">Runtime characteristics</h3> <p>The implementations of the models for object detection, instance segmentation and keypoint detection are efficient.</p> <p>In the following table, we use 8 V100 GPUs, with CUDA 10.0 and CUDNN 7.4 to report the results. During training, we use a batch size of 2 per GPU, and during testing a batch size of 1 is used.</p> <p>For test time, we report the time for the model evaluation and postprocessing (including mask pasting in image), but not the time for computing the precision-recall.</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>Network</p></th> <th class="head"><p>train time (s / it)</p></th> <th class="head"><p>test time (s / it)</p></th> <th class="head"><p>memory (GB)</p></th> </tr> </thead>  <tr>
<td><p>Faster R-CNN ResNet-50 FPN</p></td> <td><p>0.2288</p></td> <td><p>0.0590</p></td> <td><p>5.2</p></td> </tr> <tr>
<td><p>RetinaNet ResNet-50 FPN</p></td> <td><p>0.2514</p></td> <td><p>0.0939</p></td> <td><p>4.1</p></td> </tr> <tr>
<td><p>Mask R-CNN ResNet-50 FPN</p></td> <td><p>0.2728</p></td> <td><p>0.0903</p></td> <td><p>5.4</p></td> </tr> <tr>
<td><p>Keypoint R-CNN ResNet-50 FPN</p></td> <td><p>0.3789</p></td> <td><p>0.1242</p></td> <td><p>6.8</p></td> </tr>  </table>   <h3 id="faster-r-cnn">Faster R-CNN</h3> <dl class="function"> <dt id="torchvision.models.detection.fasterrcnn_resnet50_fpn">
<code>torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/faster_rcnn.html#fasterrcnn_resnet50_fpn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</p> <p>The input to the model is expected to be a list of tensors, each of shape <code>[C, H, W]</code>, one for each image, and should be in <code>0-1</code> range. Different images can have different sizes.</p> <p>The behavior of the model changes depending if it is in training or evaluation mode.</p> <p>During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:</p>  <ul class="simple"> <li>boxes (<code>FloatTensor[N, 4]</code>): the ground-truth boxes in <code>[x1, y1, x2, y2]</code> format, with values of <code>x</code> between <code>0</code> and <code>W</code> and values of <code>y</code> between <code>0</code> and <code>H</code>
</li> <li>labels (<code>Int64Tensor[N]</code>): the class label for each ground-truth box</li> </ul>  <p>The model returns a <code>Dict[Tensor]</code> during training, containing the classification and regression losses for both the RPN and the R-CNN.</p> <p>During inference, the model requires only the input tensors, and returns the post-processed predictions as a <code>List[Dict[Tensor]]</code>, one for each input image. The fields of the <code>Dict</code> are as follows:</p>  <ul class="simple"> <li>boxes (<code>FloatTensor[N, 4]</code>): the predicted boxes in <code>[x1, y1, x2, y2]</code> format, with values of <code>x</code> between <code>0</code> and <code>W</code> and values of <code>y</code> between <code>0</code> and <code>H</code>
</li> <li>labels (<code>Int64Tensor[N]</code>): the predicted labels for each image</li> <li>scores (<code>Tensor[N]</code>): the scores or each prediction</li> </ul>  <p>Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
&gt;&gt;&gt; # For training
&gt;&gt;&gt; images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)
&gt;&gt;&gt; labels = torch.randint(1, 91, (4, 11))
&gt;&gt;&gt; images = list(image for image in images)
&gt;&gt;&gt; targets = []
&gt;&gt;&gt; for i in range(len(images)):
&gt;&gt;&gt;     d = {}
&gt;&gt;&gt;     d['boxes'] = boxes[i]
&gt;&gt;&gt;     d['labels'] = labels[i]
&gt;&gt;&gt;     targets.append(d)
&gt;&gt;&gt; output = model(images, targets)
&gt;&gt;&gt; # For inference
&gt;&gt;&gt; model.eval()
&gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
&gt;&gt;&gt; predictions = model(x)
&gt;&gt;&gt;
&gt;&gt;&gt; # optionally, if you want to export the model to ONNX:
&gt;&gt;&gt; torch.onnx.export(model, x, "faster_rcnn.onnx", opset_version = 11)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on COCO train2017</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<strong>pretrained_backbone</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model with backbone pre-trained on Imagenet</li> <li>
<strong>num_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – number of output classes of the model (including the background)</li> <li>
<strong>trainable_backbone_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – number of trainable (not frozen) resnet layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="retinanet">RetinaNet</h3> <dl class="function"> <dt id="torchvision.models.detection.retinanet_resnet50_fpn">
<code>torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/retinanet.html#retinanet_resnet50_fpn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a RetinaNet model with a ResNet-50-FPN backbone.</p> <p>The input to the model is expected to be a list of tensors, each of shape <code>[C, H, W]</code>, one for each image, and should be in <code>0-1</code> range. Different images can have different sizes.</p> <p>The behavior of the model changes depending if it is in training or evaluation mode.</p> <p>During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:</p>  <ul class="simple"> <li>boxes (<code>FloatTensor[N, 4]</code>): the ground-truth boxes in <code>[x1, y1, x2, y2]</code> format, with values between <code>0</code> and <code>H</code> and <code>0</code> and <code>W</code>
</li> <li>labels (<code>Int64Tensor[N]</code>): the class label for each ground-truth box</li> </ul>  <p>The model returns a <code>Dict[Tensor]</code> during training, containing the classification and regression losses.</p> <p>During inference, the model requires only the input tensors, and returns the post-processed predictions as a <code>List[Dict[Tensor]]</code>, one for each input image. The fields of the <code>Dict</code> are as follows:</p>  <ul class="simple"> <li>boxes (<code>FloatTensor[N, 4]</code>): the predicted boxes in <code>[x1, y1, x2, y2]</code> format, with values between <code>0</code> and <code>H</code> and <code>0</code> and <code>W</code>
</li> <li>labels (<code>Int64Tensor[N]</code>): the predicted labels for each image</li> <li>scores (<code>Tensor[N]</code>): the scores or each prediction</li> </ul>  <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)
&gt;&gt;&gt; model.eval()
&gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
&gt;&gt;&gt; predictions = model(x)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on COCO train2017</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="mask-r-cnn">Mask R-CNN</h3> <dl class="function"> <dt id="torchvision.models.detection.maskrcnn_resnet50_fpn">
<code>torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/mask_rcnn.html#maskrcnn_resnet50_fpn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</p> <p>The input to the model is expected to be a list of tensors, each of shape <code>[C, H, W]</code>, one for each image, and should be in <code>0-1</code> range. Different images can have different sizes.</p> <p>The behavior of the model changes depending if it is in training or evaluation mode.</p> <p>During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:</p>  <ul class="simple"> <li>boxes (<code>FloatTensor[N, 4]</code>): the ground-truth boxes in <code>[x1, y1, x2, y2]</code> format, with values of <code>x</code> between <code>0</code> and <code>W</code> and values of <code>y</code> between <code>0</code> and <code>H</code>
</li> <li>labels (<code>Int64Tensor[N]</code>): the class label for each ground-truth box</li> <li>masks (<code>UInt8Tensor[N, H, W]</code>): the segmentation binary masks for each instance</li> </ul>  <p>The model returns a <code>Dict[Tensor]</code> during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss.</p> <p>During inference, the model requires only the input tensors, and returns the post-processed predictions as a <code>List[Dict[Tensor]]</code>, one for each input image. The fields of the <code>Dict</code> are as follows:</p>  <ul class="simple"> <li>boxes (<code>FloatTensor[N, 4]</code>): the predicted boxes in <code>[x1, y1, x2, y2]</code> format, with values of <code>x</code> between <code>0</code> and <code>W</code> and values of <code>y</code> between <code>0</code> and <code>H</code>
</li> <li>labels (<code>Int64Tensor[N]</code>): the predicted labels for each image</li> <li>scores (<code>Tensor[N]</code>): the scores or each prediction</li> <li>masks (<code>UInt8Tensor[N, 1, H, W]</code>): the predicted masks for each instance, in <code>0-1</code> range. In order to obtain the final segmentation masks, the soft masks can be thresholded, generally with a value of 0.5 (<code>mask &gt;= 0.5</code>)</li> </ul>  <p>Mask R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
&gt;&gt;&gt; model.eval()
&gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
&gt;&gt;&gt; predictions = model(x)
&gt;&gt;&gt;
&gt;&gt;&gt; # optionally, if you want to export the model to ONNX:
&gt;&gt;&gt; torch.onnx.export(model, x, "mask_rcnn.onnx", opset_version = 11)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on COCO train2017</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<strong>pretrained_backbone</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model with backbone pre-trained on Imagenet</li> <li>
<strong>num_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – number of output classes of the model (including the background)</li> <li>
<strong>trainable_backbone_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – number of trainable (not frozen) resnet layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.</li> </ul> </dd> </dl> </dd>
</dl>   <h3 id="keypoint-r-cnn">Keypoint R-CNN</h3> <dl class="function"> <dt id="torchvision.models.detection.keypointrcnn_resnet50_fpn">
<code>torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=2, num_keypoints=17, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/keypoint_rcnn.html#keypointrcnn_resnet50_fpn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</p> <p>The input to the model is expected to be a list of tensors, each of shape <code>[C, H, W]</code>, one for each image, and should be in <code>0-1</code> range. Different images can have different sizes.</p> <p>The behavior of the model changes depending if it is in training or evaluation mode.</p> <p>During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:</p>  <ul class="simple"> <li>boxes (<code>FloatTensor[N, 4]</code>): the ground-truth boxes in <code>[x1, y1, x2, y2]</code> format, with values of <code>x</code> between <code>0</code> and <code>W</code> and values of <code>y</code> between <code>0</code> and <code>H</code>
</li> <li>labels (<code>Int64Tensor[N]</code>): the class label for each ground-truth box</li> <li>keypoints (<code>FloatTensor[N, K, 3]</code>): the <code>K</code> keypoints location for each of the <code>N</code> instances, in the format <code>[x, y, visibility]</code>, where <code>visibility=0</code> means that the keypoint is not visible.</li> </ul>  <p>The model returns a <code>Dict[Tensor]</code> during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss.</p> <p>During inference, the model requires only the input tensors, and returns the post-processed predictions as a <code>List[Dict[Tensor]]</code>, one for each input image. The fields of the <code>Dict</code> are as follows:</p>  <ul class="simple"> <li>boxes (<code>FloatTensor[N, 4]</code>): the predicted boxes in <code>[x1, y1, x2, y2]</code> format, with values of <code>x</code> between <code>0</code> and <code>W</code> and values of <code>y</code> between <code>0</code> and <code>H</code>
</li> <li>labels (<code>Int64Tensor[N]</code>): the predicted labels for each image</li> <li>scores (<code>Tensor[N]</code>): the scores or each prediction</li> <li>keypoints (<code>FloatTensor[N, K, 3]</code>): the locations of the predicted keypoints, in <code>[x, y, v]</code> format.</li> </ul>  <p>Keypoint R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)
&gt;&gt;&gt; model.eval()
&gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
&gt;&gt;&gt; predictions = model(x)
&gt;&gt;&gt;
&gt;&gt;&gt; # optionally, if you want to export the model to ONNX:
&gt;&gt;&gt; torch.onnx.export(model, x, "keypoint_rcnn.onnx", opset_version = 11)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on COCO train2017</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> <li>
<strong>pretrained_backbone</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model with backbone pre-trained on Imagenet</li> <li>
<strong>num_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – number of output classes of the model (including the background)</li> <li>
<strong>trainable_backbone_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – number of trainable (not frozen) resnet layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.</li> </ul> </dd> </dl> </dd>
</dl>    <h2 id="video-classification">Video classification</h2> <p>We provide models for action recognition pre-trained on Kinetics-400. They have all been trained with the scripts provided in <code>references/video_classification</code>.</p> <p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB videos of shape (3 x T x H x W), where H and W are expected to be 112, and T is a number of video frames in a clip. The images have to be loaded in to a range of [0, 1] and then normalized using <code>mean = [0.43216, 0.394666, 0.37645]</code> and <code>std = [0.22803, 0.22145, 0.216989]</code>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The normalization parameters are different from the image classification ones, and correspond to the mean and std from Kinetics-400.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>For now, normalization code can be found in <code>references/video_classification/transforms.py</code>, see the <code>Normalize</code> function there. Note that it differs from standard normalization for images because it assumes the video is 4d.</p> </div> <p>Kinetics 1-crop accuracies for clip length 16 (16x112x112)</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>Network</p></th> <th class="head"><p>Clip acc@1</p></th> <th class="head"><p>Clip acc@5</p></th> </tr> </thead>  <tr>
<td><p>ResNet 3D 18</p></td> <td><p>52.75</p></td> <td><p>75.45</p></td> </tr> <tr>
<td><p>ResNet MC 18</p></td> <td><p>53.90</p></td> <td><p>76.29</p></td> </tr> <tr>
<td><p>ResNet (2+1)D</p></td> <td><p>57.50</p></td> <td><p>78.81</p></td> </tr>  </table>  <h3 id="resnet-3d">ResNet 3D</h3> <dl class="function"> <dt id="torchvision.models.video.r3d_18">
<code>torchvision.models.video.r3d_18(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r3d_18"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Construct 18 layer Resnet3D model as in <a class="reference external" href="https://arxiv.org/abs/1711.11248">https://arxiv.org/abs/1711.11248</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on Kinetics-400</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>R3D-18 network</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="../generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">nn.Module</a></p> </dd> </dl> </dd>
</dl>   <h3 id="resnet-mixed-convolution">ResNet Mixed Convolution</h3> <dl class="function"> <dt id="torchvision.models.video.mc3_18">
<code>torchvision.models.video.mc3_18(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#mc3_18"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructor for 18 layer Mixed Convolution network as in <a class="reference external" href="https://arxiv.org/abs/1711.11248">https://arxiv.org/abs/1711.11248</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on Kinetics-400</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>MC3 Network definition</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="../generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">nn.Module</a></p> </dd> </dl> </dd>
</dl>   <h3 id="resnet-2-1-d">ResNet (2+1)D</h3> <dl class="function"> <dt id="torchvision.models.video.r2plus1d_18">
<code>torchvision.models.video.r2plus1d_18(pretrained=False, progress=True, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r2plus1d_18"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructor for the 18 layer deep R(2+1)D network as in <a class="reference external" href="https://arxiv.org/abs/1711.11248">https://arxiv.org/abs/1711.11248</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pretrained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, returns a model pre-trained on Kinetics-400</li> <li>
<strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If True, displays a progress bar of the download to stderr</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>R(2+1)D-18 network</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="../generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">nn.Module</a></p> </dd> </dl> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.7.0/torchvision/models.html" class="_attribution-link">https://pytorch.org/docs/1.7.0/torchvision/models.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
