
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>torch.utils.cpp_extension - PyTorch - W3cubDocs</title>
  
  <meta name="description" content=" Creates a setuptools.Extension for C++. ">
  <meta name="keywords" content="torch, utils, cpp, extension, pytorch">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/pytorch/cpp_extension.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/pytorch.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/pytorch/" class="_nav-link" title="" style="margin-left:0;">PyTorch</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="torch-utils-cpp-extension">torch.utils.cpp_extension</h1> <dl class="function"> <dt id="torch.utils.cpp_extension.CppExtension">
<code>torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CppExtension"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Creates a <code>setuptools.Extension</code> for C++.</p> <p>Convenience method that creates a <code>setuptools.Extension</code> with the bare minimum (but often sufficient) arguments to build a C++ extension.</p> <p>All arguments are forwarded to the <code>setuptools.Extension</code> constructor.</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from setuptools import setup
&gt;&gt;&gt; from torch.utils.cpp_extension import BuildExtension, CppExtension
&gt;&gt;&gt; setup(
        name='extension',
        ext_modules=[
            CppExtension(
                name='extension',
                sources=['extension.cpp'],
                extra_compile_args=['-g']),
        ],
        cmdclass={
            'build_ext': BuildExtension
        })
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.CUDAExtension">
<code>torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CUDAExtension"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Creates a <code>setuptools.Extension</code> for CUDA/C++.</p> <p>Convenience method that creates a <code>setuptools.Extension</code> with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</p> <p>All arguments are forwarded to the <code>setuptools.Extension</code> constructor.</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from setuptools import setup
&gt;&gt;&gt; from torch.utils.cpp_extension import BuildExtension, CUDAExtension
&gt;&gt;&gt; setup(
        name='cuda_extension',
        ext_modules=[
            CUDAExtension(
                    name='cuda_extension',
                    sources=['extension.cpp', 'extension_kernel.cu'],
                    extra_compile_args={'cxx': ['-g'],
                                        'nvcc': ['-O2']})
        ],
        cmdclass={
            'build_ext': BuildExtension
        })
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.BuildExtension">
<code>torch.utils.cpp_extension.BuildExtension(*args, **kwargs) â†’ None</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#BuildExtension"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A custom <code>setuptools</code> build extension .</p> <p>This <code>setuptools.build_ext</code> subclass takes care of passing the minimum required compiler flags (e.g. <code>-std=c++14</code>) as well as mixed C++/CUDA compilation (and support for CUDA files in general).</p> <p>When using <a class="reference internal" href="#torch.utils.cpp_extension.BuildExtension" title="torch.utils.cpp_extension.BuildExtension"><code>BuildExtension</code></a>, it is allowed to supply a dictionary for <code>extra_compile_args</code> (rather than the usual list) that maps from languages (<code>cxx</code> or <code>nvcc</code>) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.</p> <p><code>use_ninja</code> (bool): If <code>use_ninja</code> is <code>True</code> (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard <code>setuptools.build_ext</code>. Fallbacks to the standard distutils backend if Ninja is not available.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the <code>MAX_JOBS</code> environment variable to a non-negative number.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.load">
<code>torch.utils.cpp_extension.load(name, sources: List[str], extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda: Optional[bool] = None, is_python_module=True, keep_intermediates=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Loads a PyTorch C++ extension just-in-time (JIT).</p> <p>To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.</p> <p>By default, the directory to which the build file is emitted and the resulting library compiled to is <code>&lt;tmp&gt;/torch_extensions/&lt;name&gt;</code>, where <code>&lt;tmp&gt;</code> is the temporary folder on the current platform and <code>&lt;name&gt;</code> the name of the extension. This location can be overridden in two ways. First, if the <code>TORCH_EXTENSIONS_DIR</code> environment variable is set, it replaces <code>&lt;tmp&gt;/torch_extensions</code> and all extensions will be compiled into subfolders of this directory. Second, if the <code>build_directory</code> argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.</p> <p>To compile the sources, the default system compiler (<code>c++</code>) is used, which can be overridden by setting the <code>CXX</code> environment variable. To pass additional arguments to the compilation process, <code>extra_cflags</code> or <code>extra_ldflags</code> can be provided. For example, to compile your extension with optimizations, pass <code>extra_cflags=['-O3']</code>. You can also use <code>extra_cflags</code> to pass further include directories.</p> <p>CUDA support with mixed compilation is provided. Simply pass CUDA source files (<code>.cu</code> or <code>.cuh</code>) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking <code>cudart</code>. You can pass additional flags to nvcc via <code>extra_cuda_cflags</code>, just like with <code>extra_cflags</code> for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the <code>CUDA_HOME</code> environment variable is the safest option.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> â€“ The name of the extension to build. This MUST be the same as the name of the pybind11 module!</li> <li>
<strong>sources</strong> â€“ A list of relative or absolute paths to C++ source files.</li> <li>
<strong>extra_cflags</strong> â€“ optional list of compiler flags to forward to the build.</li> <li>
<strong>extra_cuda_cflags</strong> â€“ optional list of compiler flags to forward to nvcc when building CUDA sources.</li> <li>
<strong>extra_ldflags</strong> â€“ optional list of linker flags to forward to the build.</li> <li>
<strong>extra_include_paths</strong> â€“ optional list of include directories to forward to the build.</li> <li>
<strong>build_directory</strong> â€“ optional path to use as build workspace.</li> <li>
<strong>verbose</strong> â€“ If <code>True</code>, turns on verbose logging of load steps.</li> <li>
<strong>with_cuda</strong> â€“ Determines whether CUDA headers and libraries are added to the build. If set to <code>None</code> (default), this value is automatically determined based on the existence of <code>.cu</code> or <code>.cuh</code> in <code>sources</code>. Set it to <code>True`</code> to force CUDA headers and libraries to be included.</li> <li>
<strong>is_python_module</strong> â€“ If <code>True</code> (default), imports the produced shared library as a Python module. If <code>False</code>, loads it into the process as a plain dynamic library.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>If <code>is_python_module</code> is <code>True</code>, returns the loaded PyTorch extension as a Python module. If <code>is_python_module</code> is <code>False</code> returns nothing (the shared library is loaded into the process as a side effect).</p> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from torch.utils.cpp_extension import load
&gt;&gt;&gt; module = load(
        name='extension',
        sources=['extension.cpp', 'extension_kernel.cu'],
        extra_cflags=['-O2'],
        verbose=True)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.load_inline">
<code>torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load_inline"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Loads a PyTorch C++ extension just-in-time (JIT) from string sources.</p> <p>This function behaves exactly like <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code>load()</code></a>, but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of <a class="reference internal" href="#torch.utils.cpp_extension.load_inline" title="torch.utils.cpp_extension.load_inline"><code>load_inline()</code></a> is identical to <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code>load()</code></a>.</p> <p>See <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/test/test_cpp_extensions.py">the tests</a> for good examples of using this function.</p> <p>Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to <code>cpp_sources</code> are first concatenated into a single <code>.cpp</code> file. This file is then prepended with <code>#include
&lt;torch/extension.h&gt;</code>.</p> <p>Furthermore, if the <code>functions</code> argument is supplied, bindings will be automatically generated for each function specified. <code>functions</code> can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.</p> <p>The sources in <code>cuda_sources</code> are concatenated into a separate <code>.cu</code> file and prepended with <code>torch/types.h</code>, <code>cuda.h</code> and <code>cuda_runtime.h</code> includes. The <code>.cpp</code> and <code>.cu</code> files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in <code>cuda_sources</code> per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the <code>cpp_sources</code> (and include its name in <code>functions</code>).</p> <p>See <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code>load()</code></a> for a description of arguments omitted below.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>cpp_sources</strong> â€“ A string, or list of strings, containing C++ source code.</li> <li>
<strong>cuda_sources</strong> â€“ A string, or list of strings, containing CUDA source code.</li> <li>
<strong>functions</strong> â€“ A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).</li> <li>
<strong>with_cuda</strong> â€“ Determines whether CUDA headers and libraries are added to the build. If set to <code>None</code> (default), this value is automatically determined based on whether <code>cuda_sources</code> is provided. Set it to <code>True</code> to force CUDA headers and libraries to be included.</li> <li>
<strong>with_pytorch_error_handling</strong> â€“ Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function <code>foo</code> is called via an intermediary <code>_safe_foo</code> function. This redirection might cause issues in obscure cases of cpp. This flag should be set to <code>False</code> when this redirect causes issues.</li> </ul> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from torch.utils.cpp_extension import load_inline
&gt;&gt;&gt; source = \'\'\'
at::Tensor sin_add(at::Tensor x, at::Tensor y) {
  return x.sin() + y.sin();
}
\'\'\'
&gt;&gt;&gt; module = load_inline(name='inline_extension',
                         cpp_sources=[source],
                         functions=['sin_add'])
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the <code>MAX_JOBS</code> environment variable to a non-negative number.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.include_paths">
<code>torch.utils.cpp_extension.include_paths(cuda: bool = False) â†’ List[str]</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#include_paths"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get the include paths required to build a C++ or CUDA extension.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>cuda</strong> â€“ If <code>True</code>, includes CUDA-specific include paths.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A list of include path strings.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.check_compiler_abi_compatibility">
<code>torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler) â†’ bool</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#check_compiler_abi_compatibility"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Verifies that the given compiler is ABI-compatible with PyTorch.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>compiler</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) â€“ The compiler executable name to check (e.g. <code>g++</code>). Must be executable in a shell process.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>False if the compiler is (likely) ABI-incompatible with PyTorch, else True.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.verify_ninja_availability">
<code>torch.utils.cpp_extension.verify_ninja_availability()</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#verify_ninja_availability"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Raises <code>RuntimeError</code> if <a class="reference external" href="https://ninja-build.org/">ninja</a> build system is not available on the system, does nothing otherwise.</p> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.is_ninja_available">
<code>torch.utils.cpp_extension.is_ninja_available()</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#is_ninja_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns <code>True</code> if the <a class="reference external" href="https://ninja-build.org/">ninja</a> build system is available on the system, <code>False</code> otherwise.</p> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.7.0/cpp_extension.html" class="_attribution-link">https://pytorch.org/docs/1.7.0/cpp_extension.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
