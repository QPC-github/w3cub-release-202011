
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>CTCLoss - PyTorch - W3cubDocs</title>
  
  <meta name="description" content=" The Connectionist Temporal Classification loss. ">
  <meta name="keywords" content="ctcloss, pytorch">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/pytorch/generated/torch.nn.ctcloss.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/pytorch.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/pytorch/" class="_nav-link" title="" style="margin-left:0;">PyTorch</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="ctcloss">CTCLoss</h1> <dl class="class"> <dt id="torch.nn.CTCLoss">
<code>class torch.nn.CTCLoss(blank: int = 0, reduction: str = 'mean', zero_infinity: bool = False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/loss.html#CTCLoss"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The Connectionist Temporal Classification loss.</p> <p>Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be “many-to-one”, which limits the length of the target sequence such that it must be <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo></mrow><annotation encoding="application/x-tex">\leq</annotation></semantics></math></span></span> </span> the input length.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>blank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – blank label. Default <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> </span>.</li> <li>
<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: <code>'mean'</code>
</li> <li>
<strong>zero_infinity</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – Whether to zero infinite losses and the associated gradients. Default: <code>False</code> Infinite losses mainly occur when the inputs are too short to be aligned to the targets.</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Log_probs: Tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, N, C)</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mtext>input length</mtext></mrow><annotation encoding="application/x-tex">T = \text{input length}</annotation></semantics></math></span></span> </span>, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mtext>batch size</mtext></mrow><annotation encoding="application/x-tex">N = \text{batch size}</annotation></semantics></math></span></span> </span>, and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mtext>number of classes (including blank)</mtext></mrow><annotation encoding="application/x-tex">C = \text{number of classes (including blank)}</annotation></semantics></math></span></span> </span>. The logarithmized probabilities of the outputs (e.g. obtained with <a class="reference internal" href="../nn.functional#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code>torch.nn.functional.log_softmax()</code></a>).</li> <li>Targets: Tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S)</annotation></semantics></math></span></span> </span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">sum</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>target_lengths</mtext><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\operatorname{sum}(\text{target\_lengths}))</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mtext>batch size</mtext></mrow><annotation encoding="application/x-tex">N = \text{batch size}</annotation></semantics></math></span></span> </span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mtext>max target length, if shape is </mtext><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S = \text{max target length, if shape is } (N, S)</annotation></semantics></math></span></span> </span>. It represent the target sequences. Each element in the target sequence is a class index. And the target index cannot be blank (default=0). In the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S)</annotation></semantics></math></span></span> </span> form, targets are padded to the length of the longest sequence, and stacked. In the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">sum</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>target_lengths</mtext><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\operatorname{sum}(\text{target\_lengths}))</annotation></semantics></math></span></span> </span> form, the targets are assumed to be un-padded and concatenated within 1 dimension.</li> <li>Input_lengths: Tuple or tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N)</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mtext>batch size</mtext></mrow><annotation encoding="application/x-tex">N = \text{batch size}</annotation></semantics></math></span></span> </span>. It represent the lengths of the inputs (must each be <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">\leq T</annotation></semantics></math></span></span> </span>). And the lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths.</li> <li>Target_lengths: Tuple or tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N)</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mtext>batch size</mtext></mrow><annotation encoding="application/x-tex">N = \text{batch size}</annotation></semantics></math></span></span> </span>. It represent lengths of the targets. Lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. If target shape is <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,S)</annotation></semantics></math></span></span> </span>, target_lengths are effectively the stop index <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">s_n</annotation></semantics></math></span></span> </span> for each target sequence, such that <code>target_n = targets[n,0:s_n]</code> for each target in a batch. Lengths must each be <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">\leq S</annotation></semantics></math></span></span> </span> If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor.</li> <li>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N)</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mtext>batch size</mtext></mrow><annotation encoding="application/x-tex">N = \text{batch size}</annotation></semantics></math></span></span> </span>.</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # Target are to be padded
&gt;&gt;&gt; T = 50      # Input sequence length
&gt;&gt;&gt; C = 20      # Number of classes (including blank)
&gt;&gt;&gt; N = 16      # Batch size
&gt;&gt;&gt; S = 30      # Target sequence length of longest target in batch (padding length)
&gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes
&gt;&gt;&gt;
&gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)
&gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
&gt;&gt;&gt;
&gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)
&gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)
&gt;&gt;&gt;
&gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
&gt;&gt;&gt; target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)
&gt;&gt;&gt; ctc_loss = nn.CTCLoss()
&gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)
&gt;&gt;&gt; loss.backward()
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; # Target are to be un-padded
&gt;&gt;&gt; T = 50      # Input sequence length
&gt;&gt;&gt; C = 20      # Number of classes (including blank)
&gt;&gt;&gt; N = 16      # Batch size
&gt;&gt;&gt;
&gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)
&gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
&gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
&gt;&gt;&gt;
&gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)
&gt;&gt;&gt; target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)
&gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)
&gt;&gt;&gt; ctc_loss = nn.CTCLoss()
&gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)
&gt;&gt;&gt; loss.backward()
</pre> <dl class="simple"> <dt>Reference:</dt>
<dd>
<p>A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: <a class="reference external" href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In order to use CuDNN, the following must be satisfied: <code>targets</code> must be in concatenated format, all <code>input_lengths</code> must be <code>T</code>. <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>l</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">blank=0</annotation></semantics></math></span></span> </span>, <code>target_lengths</code> <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">\leq 256</annotation></semantics></math></span></span> </span>, the integer arguments must be of dtype <code>torch.int32</code>.</p> <p>The regular implementation uses the (more common in PyTorch) <code>torch.long</code> dtype.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic =
True</code>. Please see the notes on <a class="reference internal" href="https://pytorch.org/docs/1.7.0/notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p> </div> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.7.0/generated/torch.nn.CTCLoss.html" class="_attribution-link">https://pytorch.org/docs/1.7.0/generated/torch.nn.CTCLoss.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
