
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Fold - PyTorch - W3cubDocs</title>
  
  <meta name="description" content=" Combines an array of sliding local blocks into a large containing tensor. ">
  <meta name="keywords" content="fold, pytorch">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/pytorch/generated/torch.nn.fold.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/pytorch.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/pytorch/" class="_nav-link" title="" style="margin-left:0;">PyTorch</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="fold">Fold</h1> <dl class="class"> <dt id="torch.nn.Fold">
<code>class torch.nn.Fold(output_size: Union[T, Tuple[T, ...]], kernel_size: Union[T, Tuple[T, ...]], dilation: Union[T, Tuple[T, ...]] = 1, padding: Union[T, Tuple[T, ...]] = 0, stride: Union[T, Tuple[T, ...]] = 1)</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/fold.html#Fold"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Combines an array of sliding local blocks into a large containing tensor.</p> <p>Consider a batched <code>input</code> tensor containing sliding local blocks, e.g., patches of images, of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo>×</mo><mo>∏</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C \times \prod(\text{kernel\_size}), L)</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> </span> is batch dimension, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>×</mo><mo>∏</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C \times \prod(\text{kernel\_size})</annotation></semantics></math></span></span> </span> is the number of values within a block (a block has <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∏</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\prod(\text{kernel\_size})</annotation></semantics></math></span></span> </span> spatial locations each containing a <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span> </span>-channeled vector), and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> </span> is the total number of blocks. (This is exactly the same specification as the output shape of <a class="reference internal" href="torch.nn.unfold#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a>.) This operation combines these local blocks into the large <code>output</code> tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mtext>output_size</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mtext>output_size</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mo>…</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)</annotation></semantics></math></span></span> </span> by summing the overlapping values. Similar to <a class="reference internal" href="torch.nn.unfold#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a>, the arguments must satisfy</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><munder><mo>∏</mo><mi>d</mi></munder><mrow><mo fence="true">⌊</mo><mfrac><mrow><mtext>output_size</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo><mo>+</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo><mo>−</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><mrow><mtext>stride</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo></mrow></mfrac><mo>+</mo><mn>1</mn><mo fence="true">⌋</mo></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">L = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor, </annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span></span> </span> is over all spatial dimensions.</p> <ul class="simple"> <li>
<code>output_size</code> describes the spatial shape of the large containing tensor of the sliding local blocks. It is useful to resolve the ambiguity when multiple input shapes map to same number of sliding blocks, e.g., with <code>stride &gt; 0</code>.</li> </ul> <p>The <code>padding</code>, <code>stride</code> and <code>dilation</code> arguments specify how the sliding blocks are retrieved.</p> <ul class="simple"> <li>
<code>stride</code> controls the stride for the sliding blocks.</li> <li>
<code>padding</code> controls the amount of implicit zero-paddings on both sides for <code>padding</code> number of points for each dimension before reshaping.</li> <li>
<code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</li> </ul> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>output_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a>) – the shape of the spatial dimensions of the output (i.e., <code>output.sizes()[2:]</code>)</li> <li>
<strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a>) – the size of the sliding blocks</li> <li>
<strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a>) – the stride of the sliding blocks in the input spatial dimensions. Default: 1</li> <li>
<strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – implicit zero padding to be added on both sides of input. Default: 0</li> <li>
<strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – a parameter that controls the stride of elements within the neighborhood. Default: 1</li> </ul> </dd> </dl> <ul class="simple"> <li>If <code>output_size</code>, <code>kernel_size</code>, <code>dilation</code>, <code>padding</code> or <code>stride</code> is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions.</li> <li>For the case of two output spatial dimensions this operation is sometimes called <code>col2im</code>.</li> </ul> <div class="admonition note"> <p class="admonition-title">Note</p> <p><a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a> calculates each combined value in the resulting large tensor by summing all values from all containing blocks. <a class="reference internal" href="torch.nn.unfold#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.</p> <p>In general, folding and unfolding operations are related as follows. Consider <a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a> and <a class="reference internal" href="torch.nn.unfold#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> instances created with the same parameters:</p> <pre data-language="python">&gt;&gt;&gt; fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)
&gt;&gt;&gt; fold = nn.Fold(output_size=..., **fold_params)
&gt;&gt;&gt; unfold = nn.Unfold(**fold_params)
</pre> <p>Then for any (supported) <code>input</code> tensor the following equality holds:</p> <pre data-language="python">fold(unfold(input)) == divisor * input
</pre> <p>where <code>divisor</code> is a tensor that depends only on the shape and dtype of the <code>input</code>:</p> <pre data-language="python">&gt;&gt;&gt; input_ones = torch.ones(input.shape, dtype=input.dtype)
&gt;&gt;&gt; divisor = fold(unfold(input_ones))
</pre> <p>When the <code>divisor</code> tensor contains no zero elements, then <code>fold</code> and <code>unfold</code> operations are inverses of each other (up to constant divisor).</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Currently, only 4-D output tensors (batched image-like tensors) are supported.</p> </div> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo>×</mo><mo>∏</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C \times \prod(\text{kernel\_size}), L)</annotation></semantics></math></span></span> </span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mtext>output_size</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mtext>output_size</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mo>…</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)</annotation></semantics></math></span></span> </span> as described above</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))
&gt;&gt;&gt; input = torch.randn(1, 3 * 2 * 2, 12)
&gt;&gt;&gt; output = fold(input)
&gt;&gt;&gt; output.size()
torch.Size([1, 3, 4, 5])
</pre> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.7.0/generated/torch.nn.Fold.html" class="_attribution-link">https://pytorch.org/docs/1.7.0/generated/torch.nn.Fold.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
