
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>ConvTranspose3d - PyTorch - W3cubDocs</title>
  
  <meta name="description" content="Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies &hellip;">
  <meta name="keywords" content="convtranspose, d, pytorch">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/pytorch/generated/torch.nn.convtranspose3d.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/pytorch.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/pytorch/" class="_nav-link" title="" style="margin-left:0;">PyTorch</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="convtranspose3d">ConvTranspose3d</h1> <dl class="class"> <dt id="torch.nn.ConvTranspose3d">
<code>class torch.nn.ConvTranspose3d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T, T]], stride: Union[T, Tuple[T, T, T]] = 1, padding: Union[T, Tuple[T, T, T]] = 0, output_padding: Union[T, Tuple[T, T, T]] = 0, groups: int = 1, bias: bool = True, dilation: Union[T, Tuple[T, T, T]] = 1, padding_mode: str = 'zeros')</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/conv.html#ConvTranspose3d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes.</p> <p>This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</p> <p>This module supports <a class="reference internal" href="https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere"><span class="std std-ref">TensorFloat32</span></a>.</p> <ul> <li>
<code>stride</code> controls the stride for the cross-correlation.</li> <li>
<code>padding</code> controls the amount of implicit zero-paddings on both sides for <code>dilation * (kernel_size - 1) - padding</code> number of points. See note below for details.</li> <li>
<code>output_padding</code> controls the additional size added to one side of the output shape. See note below for details.</li> <li>
<code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</li> <li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>  <ul class="simple"> <li>At groups=1, all inputs are convolved to all outputs.</li> <li>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.</li> <li>At groups= <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">⌊</mo><mfrac><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>c</mi><mi>h</mi><mi>a</mi><mi>n</mi><mi>n</mi><mi>e</mi><mi>l</mi><mi>s</mi></mrow><mrow><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>c</mi><mi>h</mi><mi>a</mi><mi>n</mi><mi>n</mi><mi>e</mi><mi>l</mi><mi>s</mi></mrow></mfrac><mo fence="true">⌋</mo></mrow><annotation encoding="application/x-tex">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</annotation></semantics></math></span></span> </span>).</li> </ul>  </li> </ul> <p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> can either be:</p>  <ul class="simple"> <li>a single <code>int</code> – in which case the same value is used for the depth, height and width dimensions</li> <li>a <code>tuple</code> of three ints – in which case, the first <code>int</code> is used for the depth dimension, the second <code>int</code> for the height dimension and the third <code>int</code> for the width dimension</li> </ul>  <div class="admonition note"> <p class="admonition-title">Note</p> <p>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>. It is up to the user to add proper padding.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>padding</code> argument effectively adds <code>dilation * (kernel_size - 1) - padding</code> amount of zero padding to both sizes of the input. This is set so that when a <a class="reference internal" href="torch.nn.conv3d#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code>ConvTranspose3d</code></a> are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when <code>stride &gt; 1</code>, <a class="reference internal" href="torch.nn.conv3d#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a> maps multiple input shapes to the same output shape. <code>output_padding</code> is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that <code>output_padding</code> is only used to find output shape, but does not actually add zero-padding to output.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic =
True</code>. Please see the notes on <a class="reference internal" href="https://pytorch.org/docs/1.7.0/notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – Number of channels in the input image</li> <li>
<strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – Number of channels produced by the convolution</li> <li>
<strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a>) – Size of the convolving kernel</li> <li>
<strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li> <li>
<strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – <code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Default: 0</li> <li>
<strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – Additional size added to one side of each dimension in the output shape. Default: 0</li> <li>
<strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li> <li>
<strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>
</li> <li>
<strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li> </ul> </dd> </dl> <dl> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>D</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{in}, D_{in}, H_{in}, W_{in})</annotation></semantics></math></span></span> </span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>D</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{out}, D_{out}, H_{out}, W_{out})</annotation></semantics></math></span></span> </span> where</li> </ul> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>D</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>D</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mtext>stride</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>−</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>+</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mtext>output_padding</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1 </annotation></semantics></math></span></span></span> </div>
<div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mtext>stride</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>−</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>+</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mtext>output_padding</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1 </annotation></semantics></math></span></span></span> </div>
<div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mtext>stride</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo>−</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo>+</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mtext>output_padding</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1 </annotation></semantics></math></span></span></span> </div>
</dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>~ConvTranspose3d.weight</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the learnable weights of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>in_channels</mtext><mo separator="true">,</mo><mfrac><mtext>out_channels</mtext><mtext>groups</mtext></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},</annotation></semantics></math></span></span> </span> <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>kernel_size[0]</mtext><mo separator="true">,</mo><mtext>kernel_size[1]</mtext><mo separator="true">,</mo><mtext>kernel_size[2]</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})</annotation></semantics></math></span></span> </span>. The values of these weights are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span> </span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi></mrow><mrow><msub><mi>C</mi><mtext>out</mtext></msub><mo>∗</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></msubsup><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>~ConvTranspose3d.bias</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the learnable bias of the module of shape (out_channels) If <code>bias</code> is <code>True</code>, then the values of these weights are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span> </span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi></mrow><mrow><msub><mi>C</mi><mtext>out</mtext></msub><mo>∗</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></msubsup><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}</annotation></semantics></math></span></span> </span>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)
&gt;&gt;&gt; output = m(input)
</pre> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.7.0/generated/torch.nn.ConvTranspose3d.html" class="_attribution-link">https://pytorch.org/docs/1.7.0/generated/torch.nn.ConvTranspose3d.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
