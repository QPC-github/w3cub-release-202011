
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Unflatten - PyTorch - W3cubDocs</title>
  
  <meta name="description" content=" Unflattens a tensor dim expanding it to a desired shape. For use with Sequential. ">
  <meta name="keywords" content="unflatten, pytorch">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/pytorch/generated/torch.nn.unflatten.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/pytorch.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/pytorch/" class="_nav-link" title="" style="margin-left:0;">PyTorch</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="unflatten">Unflatten</h1> <dl class="class"> <dt id="torch.nn.Unflatten">
<code>class torch.nn.Unflatten(dim: Union[int, str], unflattened_size: Union[torch.Size, Tuple[Tuple[str, int]]])</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/flatten.html#Unflatten"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Unflattens a tensor dim expanding it to a desired shape. For use with <code>Sequential</code>.</p> <ul class="simple"> <li>
<code>dim</code> specifies the dimension of the input tensor to be unflattened, and it can be either <code>int</code> or <code>str</code> when <code>Tensor</code> or <code>NamedTensor</code> is used, respectively.</li> <li>
<code>unflattened_size</code> is the new shape of the unflattened dimension of the tensor and it can be a <code>tuple</code> of ints or <code>torch.Size</code> for <code>Tensor</code> input or a <code>NamedShape</code> (tuple of <code>(name, size)</code> tuples) for <code>NamedTensor</code> input.</li> </ul> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mi>d</mi><mi>i</mi><mi>m</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *dims)</annotation></semantics></math></span></span> </span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mtext>out</mtext></msub><mo separator="true">,</mo><msub><mi>H</mi><mtext>out</mtext></msub><mo separator="true">,</mo><msub><mi>W</mi><mtext>out</mtext></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})</annotation></semantics></math></span></span> </span>
</li> </ul> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dim</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em>]</em>) – Dimension to be unflattened</li> <li>
<strong>unflattened_size</strong> (<em>Union</em><em>[</em><em>torch.Size</em><em>, </em><em>NamedShape</em><em>]</em>) – New shape of the unflattened dimension</li> </ul> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; input = torch.randn(2, 50)
&gt;&gt;&gt; # With tuple of ints
&gt;&gt;&gt; m = nn.Sequential(
&gt;&gt;&gt;     nn.Linear(50, 50),
&gt;&gt;&gt;     nn.Unflatten(1, (2, 5, 5))
&gt;&gt;&gt; )
&gt;&gt;&gt; output = m(output)
&gt;&gt;&gt; output.size()
torch.Size([2, 2, 5, 5])
&gt;&gt;&gt; # With torch.Size
&gt;&gt;&gt; m = nn.Sequential(
&gt;&gt;&gt;     nn.Linear(50, 50),
&gt;&gt;&gt;     nn.Unflatten(1, torch.Size([2, 5, 5]))
&gt;&gt;&gt; )
&gt;&gt;&gt; output = m(output)
&gt;&gt;&gt; output.size()
torch.Size([2, 2, 5, 5])
&gt;&gt;&gt; # With namedshape (tuple of tuples)
&gt;&gt;&gt; m = nn.Sequential(
&gt;&gt;&gt;     nn.Linear(50, 50),
&gt;&gt;&gt;     nn.Unflatten('features', (('C', 2), ('H', 50), ('W',50)))
&gt;&gt;&gt; )
&gt;&gt;&gt; output = m(output)
&gt;&gt;&gt; output.size()
torch.Size([2, 2, 5, 5])
</pre> <dl class="attribute"> <dt id="torch.nn.Unflatten.NamedShape">
<code>NamedShape</code> </dt> <dd>
<p>alias of <code>typing.Tuple</code></p> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.add_module">
<code>add_module(name: str, module: Optional[Module]) → None</code> </dt> <dd>
<p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the child module. The child module can be accessed from this module using the given name</li> <li>
<strong>module</strong> (<a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – child module to be added to the module.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.apply">
<code>apply(fn: Callable[Module, None]) → T</code> </dt> <dd>
<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also <a class="reference internal" href="../nn.init#nn-init-doc"><span class="std std-ref">torch.nn.init</span></a>).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>fn</strong> (<a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> -&gt; None) – function to be applied to each submodule</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.bfloat16">
<code>bfloat16() → T</code> </dt> <dd>
<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.buffers">
<code>buffers(recurse: bool = True) → Iterator[torch.Tensor]</code> </dt> <dd>
<p>Returns an iterator over module buffers.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</p> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>torch.Tensor</em> – module buffer</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.children">
<code>children() → Iterator[torch.nn.modules.module.Module]</code> </dt> <dd>
<p>Returns an iterator over immediate children modules.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>Module</em> – a child module</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.cpu">
<code>cpu() → T</code> </dt> <dd>
<p>Moves all model parameters and buffers to the CPU.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.cuda">
<code>cuda(device: Union[int, torch.device, None] = None) → T</code> </dt> <dd>
<p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – if specified, all parameters will be copied to that device</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.double">
<code>double() → T</code> </dt> <dd>
<p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.eval">
<code>eval() → T</code> </dt> <dd>
<p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <a class="reference internal" href="torch.nn.dropout#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>, <code>BatchNorm</code>, etc.</p> <p>This is equivalent with <a class="reference internal" href="torch.nn.module#torch.nn.Module.train" title="torch.nn.Module.train"><code>self.train(False)</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.float">
<code>float() → T</code> </dt> <dd>
<p>Casts all floating point parameters and buffers to float datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.half">
<code>half() → T</code> </dt> <dd>
<p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.load_state_dict">
<code>load_state_dict(state_dict: Dict[str, torch.Tensor], strict: bool = True)</code> </dt> <dd>
<p>Copies parameters and buffers from <a class="reference internal" href="#torch.nn.Unflatten.state_dict" title="torch.nn.Unflatten.state_dict"><code>state_dict</code></a> into this module and its descendants. If <code>strict</code> is <code>True</code>, then the keys of <a class="reference internal" href="#torch.nn.Unflatten.state_dict" title="torch.nn.Unflatten.state_dict"><code>state_dict</code></a> must exactly match the keys returned by this module’s <a class="reference internal" href="torch.nn.module#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> function.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)">dict</a>) – a dict containing parameters and persistent buffers.</li> <li>
<strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – whether to strictly enforce that the keys in <a class="reference internal" href="#torch.nn.Unflatten.state_dict" title="torch.nn.Unflatten.state_dict"><code>state_dict</code></a> match the keys returned by this module’s <a class="reference internal" href="torch.nn.module#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> function. Default: <code>True</code>
</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<ul class="simple"> <li>
<strong>missing_keys</strong> is a list of str containing the missing keys</li> <li>
<strong>unexpected_keys</strong> is a list of str containing the unexpected keys</li> </ul> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.modules">
<code>modules() → Iterator[torch.nn.modules.module.Module]</code> </dt> <dd>
<p>Returns an iterator over all modules in the network.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>Module</em> – a module in the network</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.named_buffers">
<code>named_buffers(prefix: str = '', recurse: bool = True) → Iterator[Tuple[str, torch.Tensor]]</code> </dt> <dd>
<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – prefix to prepend to all buffer names.</li> <li>
<strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li> </ul> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.named_children">
<code>named_children() → Iterator[Tuple[str, torch.nn.modules.module.Module]]</code> </dt> <dd>
<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>(string, Module)</em> – Tuple containing a name and child module</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.named_modules">
<code>named_modules(memo: Optional[Set[Module]] = None, prefix: str = '')</code> </dt> <dd>
<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>(string, Module)</em> – Tuple of name and module</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.named_parameters">
<code>named_parameters(prefix: str = '', recurse: bool = True) → Iterator[Tuple[str, torch.Tensor]]</code> </dt> <dd>
<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – prefix to prepend to all parameter names.</li> <li>
<strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li> </ul> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.parameters">
<code>parameters(recurse: bool = True) → Iterator[torch.nn.parameter.Parameter]</code> </dt> <dd>
<p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</p> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>Parameter</em> – module parameter</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.register_backward_hook">
<code>register_backward_hook(hook: Callable[[Module, Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[None, torch.Tensor]]) → torch.utils.hooks.RemovableHandle</code> </dt> <dd>
<p>Registers a backward hook on the module.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The current implementation will not have the presented behavior for complex <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> that perform many operations. In some failure cases, <code>grad_input</code> and <code>grad_output</code> will only contain the gradients for a subset of the inputs and outputs. For such <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a>, you should use <a class="reference internal" href="../autograd#torch.Tensor.register_hook" title="torch.Tensor.register_hook"><code>torch.Tensor.register_hook()</code></a> directly on a specific input or output to get the required gradients.</p> </div> <p>The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:</p> <pre data-language="python">hook(module, grad_input, grad_output) -&gt; Tensor or None
</pre> <p>The <code>grad_input</code> and <code>grad_output</code> may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.register_buffer">
<code>register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) → None</code> </dt> <dd>
<p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm’s <code>running_mean</code> is not a parameter, but is part of the module’s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module’s <a class="reference internal" href="#torch.nn.Unflatten.state_dict" title="torch.nn.Unflatten.state_dict"><code>state_dict</code></a>.</p> <p>Buffers can be accessed as attributes using given names.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed from this module using the given name</li> <li>
<strong>tensor</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – buffer to be registered.</li> <li>
<strong>persistent</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether the buffer is part of this module’s <a class="reference internal" href="#torch.nn.Unflatten.state_dict" title="torch.nn.Unflatten.state_dict"><code>state_dict</code></a>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.register_forward_hook">
<code>register_forward_hook(hook: Callable[..., None]) → torch.utils.hooks.RemovableHandle</code> </dt> <dd>
<p>Registers a forward hook on the module.</p> <p>The hook will be called every time after <code>forward()</code> has computed an output. It should have the following signature:</p> <pre data-language="python">hook(module, input, output) -&gt; None or modified output
</pre> <p>The input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward()</code> is called.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.register_forward_pre_hook">
<code>register_forward_pre_hook(hook: Callable[..., None]) → torch.utils.hooks.RemovableHandle</code> </dt> <dd>
<p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before <code>forward()</code> is invoked. It should have the following signature:</p> <pre data-language="python">hook(module, input) -&gt; None or modified input
</pre> <p>The input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.register_parameter">
<code>register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) → None</code> </dt> <dd>
<p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed from this module using the given name</li> <li>
<strong>param</strong> (<a class="reference internal" href="torch.nn.parameter.parameter#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter">Parameter</a>) – parameter to be added to the module.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.requires_grad_">
<code>requires_grad_(requires_grad: bool = True) → T</code> </dt> <dd>
<p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters’ <code>requires_grad</code> attributes in-place.</p> <p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether autograd should record operations on parameters in this module. Default: <code>True</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.state_dict">
<code>state_dict(destination=None, prefix='', keep_vars=False)</code> </dt> <dd>
<p>Returns a dictionary containing a whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a dictionary containing a whole state of the module</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)">dict</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.to">
<code>to(*args, **kwargs)</code> </dt> <dd>
<p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as</p> <dl class="function"> <dt>
<code>to(device=None, dtype=None, non_blocking=False)</code> </dt> 
</dl> <dl class="function"> <dt>
<code>to(dtype, non_blocking=False)</code> </dt> 
</dl> <dl class="function"> <dt>
<code>to(tensor, non_blocking=False)</code> </dt> 
</dl> <dl class="function"> <dt>
<code>to(memory_format=torch.channels_last)</code> </dt> 
</dl> <p>Its signature is similar to <a class="reference internal" href="../tensors#torch.Tensor.to" title="torch.Tensor.to"><code>torch.Tensor.to()</code></a>, but only accepts floating point desired <code>dtype</code> s. In addition, this method will only cast the floating point parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p> <p>See below for examples.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This method modifies the module in-place.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<code>torch.device</code>) – the desired device of the parameters and buffers in this module</li> <li>
<strong>dtype</strong> (<code>torch.dtype</code>) – the desired floating point type of the floating point parameters and buffers in this module</li> <li>
<strong>tensor</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li> <li>
<strong>memory_format</strong> (<code>torch.memory_format</code>) – the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.train">
<code>train(mode: bool = True) → T</code> </dt> <dd>
<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <a class="reference internal" href="torch.nn.dropout#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>, <code>BatchNorm</code>, etc.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.type">
<code>type(dst_type: Union[torch.dtype, str]) → T</code> </dt> <dd>
<p>Casts all parameters and buffers to <code>dst_type</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>dst_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.9)">type</a><em> or </em><em>string</em>) – the desired type</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Unflatten.zero_grad">
<code>zero_grad(set_to_none: bool = False) → None</code> </dt> <dd>
<p>Sets gradients of all model parameters to zero. See similar function under <a class="reference internal" href="../optim#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>torch.optim.Optimizer</code></a> for more context.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>set_to_none</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – instead of setting to zero, set the grads to None. See <a class="reference internal" href="../optim#torch.optim.Optimizer.zero_grad" title="torch.optim.Optimizer.zero_grad"><code>torch.optim.Optimizer.zero_grad()</code></a> for details.</p> </dd> </dl> </dd>
</dl> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.7.0/generated/torch.nn.Unflatten.html" class="_attribution-link">https://pytorch.org/docs/1.7.0/generated/torch.nn.Unflatten.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
