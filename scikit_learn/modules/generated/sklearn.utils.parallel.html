
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>utils.Parallel() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Helper class for readable parallel mapping. ">
  <meta name="keywords" content="sklearn, utils, parallel, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/scikit_learn/modules/generated/sklearn.utils.parallel.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="sklearn-utils-parallel">sklearn.utils.Parallel</h1> <dl class="class"> <dt id="sklearn.utils.Parallel">
<code>class sklearn.utils.Parallel(n_jobs=None, backend=None, verbose=0, timeout=None, pre_dispatch=‘2 * n_jobs’, batch_size=’auto’, temp_folder=None, max_nbytes=‘1M’, mmap_mode=’r’, prefer=None, require=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/externals/joblib/parallel.py#L428"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Helper class for readable parallel mapping.</p> <p>Read more in the <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#parallel" title="(in joblib v0.12.6.dev0)"><span class="xref std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first last docutils"> <dt><strong>n_jobs: int, default: None</strong></dt> <dd>
<p class="first last">The maximum number of concurrently running jobs, such as the number of Python worker processes when backend=”multiprocessing” or the size of the thread-pool when backend=”threading”. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for ‘unset’ that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a parallel_backend context manager that sets another value for n_jobs.</p> </dd> <dt><strong>backend: str, ParallelBackendBase instance or None, default: ‘loky’</strong></dt> <dd>
<p class="first">Specify the parallelization backend implementation. Supported backends are:</p> <ul class="simple"> <li>“loky” used by default, can induce some communication and memory overhead when exchanging input and output data with the worker Python processes.</li> <li>“multiprocessing” previous process-based backend based on <code>multiprocessing.Pool</code>. Less robust than <code>loky</code>.</li> <li>“threading” is a very low-overhead backend but it suffers from the Python Global Interpreter Lock if the called function relies a lot on Python objects. “threading” is mostly useful when the execution bottleneck is a compiled extension that explicitly releases the GIL (for instance a Cython loop wrapped in a “with nogil” block or an expensive call to a library such as NumPy).</li> <li>finally, you can register backends by calling register_parallel_backend. This will allow you to implement a backend of your liking.</li> </ul> <p class="last">It is not recommended to hard-code the backend name in a call to Parallel in a library. Instead it is recommended to set soft hints (prefer) or hard constraints (require) so as to make it possible for library users to change the backend from the outside using the parallel_backend context manager.</p> </dd> <dt><strong>prefer: str in {‘processes’, ‘threads’} or None, default: None</strong></dt> <dd>
<p class="first last">Soft hint to choose the default backend if no specific backend was selected with the parallel_backend context manager. The default process-based backend is ‘loky’ and the default thread-based backend is ‘threading’.</p> </dd> <dt><strong>require: ‘sharedmem’ or None, default None</strong></dt> <dd>
<p class="first last">Hard constraint to select the backend. If set to ‘sharedmem’, the selected backend will be single-host and thread-based even if the user asked for a non-thread based backend with parallel_backend.</p> </dd> <dt><strong>verbose: int, optional</strong></dt> <dd>
<p class="first last">The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.</p> </dd> <dt><strong>timeout: float, optional</strong></dt> <dd>
<p class="first last">Timeout limit for each task to complete. If any task takes longer a TimeOutError will be raised. Only applied when n_jobs != 1</p> </dd> <dt><strong>pre_dispatch: {‘all’, integer, or expression, as in ‘3*n_jobs’}</strong></dt> <dd>
<p class="first last">The number of batches (of tasks) to be pre-dispatched. Default is ‘2*n_jobs’. When batch_size=”auto” this is reasonable default and the workers should never starve.</p> </dd> <dt><strong>batch_size: int or ‘auto’, default: ‘auto’</strong></dt> <dd>
<p class="first last">The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, dispatching calls to workers can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The <code>'auto'</code> strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. <code>batch_size="auto"</code> with <code>backend="threading"</code> will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case.</p> </dd> <dt><strong>temp_folder: str, optional</strong></dt> <dd>
<p class="first">Folder to be used by the pool for memmapping large arrays for sharing memory with worker processes. If None, this will try in order:</p> <ul class="simple"> <li>a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,</li> <li>/dev/shm if the folder exists and is writable: this is a RAM disk filesystem available by default on modern Linux distributions,</li> <li>the default system temporary folder that can be overridden with TMP, TMPDIR or TEMP environment variables, typically /tmp under Unix operating systems.</li> </ul> <p class="last">Only active when backend=”loky” or “multiprocessing”.</p> </dd> <dt><strong>max_nbytes int, str, or None, optional, 1M by default</strong></dt> <dd>
<p class="first last">Threshold on the size of arrays passed to the workers that triggers automated memory mapping in temp_folder. Can be an int in Bytes, or a human-readable string, e.g., ‘1M’ for 1 megabyte. Use None to disable memmapping of large arrays. Only active when backend=”loky” or “multiprocessing”.</p> </dd> <dt><strong>mmap_mode: {None, ‘r+’, ‘r’, ‘w+’, ‘c’}</strong></dt> <dd>
<p class="first last">Memmapping mode for numpy arrays passed to workers. See ‘max_nbytes’ parameter documentation for more details.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>This object uses workers to compute in parallel the application of a function to many different arguments. The main functionality it brings in addition to using the raw multiprocessing or concurrent.futures API are (see examples for details):</p> <ul class="simple"> <li>More readable code, in particular since it avoids constructing list of arguments.</li> <li>
<dl class="first docutils"> <dt>Easier debugging:</dt> <dd>
<ul class="first last"> <li>informative tracebacks even when the error happens on the client side</li> <li>using ‘n_jobs=1’ enables to turn off parallel computing for debugging without changing the codepath</li> <li>early capture of pickling errors</li> </ul> </dd> </dl> </li> <li>An optional progress meter.</li> <li>Interruption of multiprocesses jobs with ‘Ctrl-C’</li> <li>Flexible pickling control for the communication to and from the worker processes.</li> <li>Ability to use shared memory efficiently with worker processes for large numpy-based datastructures.</li> </ul> <h4 class="rubric">Examples</h4> <p>A simple example:</p> <pre data-language="python">&gt;&gt;&gt; from math import sqrt
&gt;&gt;&gt; from sklearn.externals.joblib import Parallel, delayed
&gt;&gt;&gt; Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
</pre> <p>Reshaping the output when the function has several return values:</p> <pre data-language="python">&gt;&gt;&gt; from math import modf
&gt;&gt;&gt; from sklearn.externals.joblib import Parallel, delayed
&gt;&gt;&gt; r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))
&gt;&gt;&gt; res, i = zip(*r)
&gt;&gt;&gt; res
(0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)
&gt;&gt;&gt; i
(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)
</pre> <p>The progress meter: the higher the value of <code>verbose</code>, the more messages:</p> <pre data-language="python">&gt;&gt;&gt; from time import sleep
&gt;&gt;&gt; from sklearn.externals.joblib import Parallel, delayed
&gt;&gt;&gt; r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) 
[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s
[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s
[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished
</pre> <p>Traceback example, note how the line of the error is indicated as well as the values of the parameter passed to the function that triggered the exception, even though the traceback happens in the child process:</p> <pre data-language="python">&gt;&gt;&gt; from heapq import nlargest
&gt;&gt;&gt; from sklearn.externals.joblib import Parallel, delayed
&gt;&gt;&gt; Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) 
#...
---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
TypeError                                          Mon Nov 12 11:37:46 2012
PID: 12934                                    Python 2.7.3: /usr/bin/python
...........................................................................
/usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)
    419         if n &gt;= size:
    420             return sorted(iterable, key=key, reverse=True)[:n]
    421
    422     # When key is none, use simpler decoration
    423     if key is None:
--&gt; 424         it = izip(iterable, count(0,-1))                    # decorate
    425         result = _nlargest(n, it)
    426         return map(itemgetter(0), result)                   # undecorate
    427
    428     # General case, slowest method
 TypeError: izip argument #1 must support iteration
___________________________________________________________________________
</pre> <p>Using pre_dispatch in a producer/consumer situation, where the data is generated on the fly. Note how the producer is first called 3 times before the parallel loop is initiated, and then called to generate new data on the fly:</p> <pre data-language="python">&gt;&gt;&gt; from math import sqrt
&gt;&gt;&gt; from sklearn.externals.joblib import Parallel, delayed
&gt;&gt;&gt; def producer():
...     for i in range(6):
...         print('Produced %s' % i)
...         yield i
&gt;&gt;&gt; out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(
...                delayed(sqrt)(i) for i in producer()) 
Produced 0
Produced 1
Produced 2
[Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s
Produced 3
[Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s
Produced 4
[Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s
Produced 5
[Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s
[Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s
[Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils">   <tr>
<td>
<code>__call__</code>(iterable)</td> <td></td> </tr> <tr>
<td>
<a class="reference internal" href="#sklearn.utils.Parallel.dispatch_next" title="sklearn.utils.Parallel.dispatch_next"><code>dispatch_next</code></a>()</td> <td>Dispatch more data for parallel processing</td> </tr> <tr>
<td>
<a class="reference internal" href="#sklearn.utils.Parallel.dispatch_one_batch" title="sklearn.utils.Parallel.dispatch_one_batch"><code>dispatch_one_batch</code></a>(iterator)</td> <td>Prefetch the tasks for the next batch and dispatch them.</td> </tr> <tr>
<td>
<a class="reference internal" href="#sklearn.utils.Parallel.format" title="sklearn.utils.Parallel.format"><code>format</code></a>(obj[, indent])</td> <td>Return the formatted representation of the object.</td> </tr> <tr>
<td>
<a class="reference internal" href="#sklearn.utils.Parallel.print_progress" title="sklearn.utils.Parallel.print_progress"><code>print_progress</code></a>()</td> <td>Display the process of the parallel execution only a fraction of time, controlled by self.verbose.</td> </tr>  </table> <table class="docutils">   <tr>
<td><strong>debug</strong></td> <td> </td> </tr> <tr>
<td><strong>retrieve</strong></td> <td> </td> </tr> <tr>
<td><strong>warn</strong></td> <td> </td> </tr>  </table> <dl class="method"> <dt id="sklearn.utils.Parallel.__init__">
<code>__init__(n_jobs=None, backend=None, verbose=0, timeout=None, pre_dispatch=‘2 * n_jobs’, batch_size=’auto’, temp_folder=None, max_nbytes=‘1M’, mmap_mode=’r’, prefer=None, require=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/externals/joblib/parallel.py#L650"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="sklearn.utils.Parallel.dispatch_next">
<code>dispatch_next()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/externals/joblib/parallel.py#L789"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Dispatch more data for parallel processing</p> <p>This method is meant to be called concurrently by the multiprocessing callback. We rely on the thread-safety of dispatch_one_batch to protect against concurrent consumption of the unprotected iterator.</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.utils.Parallel.dispatch_one_batch">
<code>dispatch_one_batch(iterator)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/externals/joblib/parallel.py#L801"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Prefetch the tasks for the next batch and dispatch them.</p> <p>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</p> <p>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.utils.Parallel.format">
<code>format(obj, indent=0)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/externals/joblib/logger.py#L83"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the formatted representation of the object.</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.utils.Parallel.print_progress">
<code>print_progress()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/externals/joblib/parallel.py#L841"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Display the process of the parallel execution only a fraction of time, controlled by self.verbose.</p> </dd>
</dl> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    © 2007–2018 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.utils.Parallel.html" class="_attribution-link">http://scikit-learn.org/stable/modules/generated/sklearn.utils.Parallel.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
