
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>sklearn.metrics.precision_recall_fscore_support() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Compute precision, recall, F-measure and support for each class ">
  <meta name="keywords" content="sklearn, metrics, precision, recall, fscore, support, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/scikit_learn/modules/generated/sklearn.metrics.precision_recall_fscore_support.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="sklearn-metrics-precision-recall-fscore-support">sklearn.metrics.precision_recall_fscore_support</h1> <dl class="function"> <dt id="sklearn.metrics.precision_recall_fscore_support">
<code>sklearn.metrics.precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=None, warn_for=(‘precision’, ’recall’, ’f-score’), sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/metrics/classification.py#L882"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute precision, recall, F-measure and support for each class</p> <p>The precision is the ratio <code>tp / (tp + fp)</code> where <code>tp</code> is the number of true positives and <code>fp</code> the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</p> <p>The recall is the ratio <code>tp / (tp + fn)</code> where <code>tp</code> is the number of true positives and <code>fn</code> the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</p> <p>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</p> <p>The F-beta score weights recall more than precision by a factor of <code>beta</code>. <code>beta == 1.0</code> means recall and precision are equally important.</p> <p>The support is the number of occurrences of each class in <code>y_true</code>.</p> <p>If <code>pos_label is None</code> and in binary classification, this function returns the average precision, recall and F-measure if <code>average</code> is one of <code>'micro'</code>, <code>'macro'</code>, <code>'weighted'</code> or <code>'samples'</code>.</p> <p>Read more in the <a class="reference internal" href="../model_evaluation#precision-recall-f-measure-metrics"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>y_true : 1d array-like, or label indicator array / sparse matrix</code> </dt> <dd>
<p class="first last">Ground truth (correct) target values.</p> </dd> <dt>
<code>y_pred : 1d array-like, or label indicator array / sparse matrix</code> </dt> <dd>
<p class="first last">Estimated targets as returned by a classifier.</p> </dd> <dt>
<code>beta : float, 1.0 by default</code> </dt> <dd>
<p class="first last">The strength of recall versus precision in the F-score.</p> </dd> <dt>
<code>labels : list, optional</code> </dt> <dd>
<p class="first last">The set of labels to include when <code>average != 'binary'</code>, and their order if <code>average is None</code>. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in <code>y_true</code> and <code>y_pred</code> are used in sorted order.</p> </dd> <dt>
<code>pos_label : str or int, 1 by default</code> </dt> <dd>
<p class="first last">The class to report if <code>average='binary'</code> and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting <code>labels=[pos_label]</code> and <code>average != 'binary'</code> will report scores for that label only.</p> </dd> <dt>
<code>average : string, [None (default), ‘binary’, ‘micro’, ‘macro’, ‘samples’, ‘weighted’]</code> </dt> <dd>
<p class="first">If <code>None</code>, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</p> <dl class="last docutils"> <dt>
<code>'binary':</code> </dt> <dd>
<p class="first last">Only report results for the class specified by <code>pos_label</code>. This is applicable only if targets (<code>y_{true,pred}</code>) are binary.</p> </dd> <dt>
<code>'micro':</code> </dt> <dd>
<p class="first last">Calculate metrics globally by counting the total true positives, false negatives and false positives.</p> </dd> <dt>
<code>'macro':</code> </dt> <dd>
<p class="first last">Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</p> </dd> <dt>
<code>'weighted':</code> </dt> <dd>
<p class="first last">Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.</p> </dd> <dt>
<code>'samples':</code> </dt> <dd>
<p class="first last">Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from <a class="reference internal" href="sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code>accuracy_score</code></a>).</p> </dd> </dl> </dd> <dt>
<code>warn_for : tuple or set, for internal use</code> </dt> <dd>
<p class="first last">This determines which warnings will be made in the case that this function is being used to return only one of its metrics.</p> </dd> <dt>
<code>sample_weight : array-like of shape = [n_samples], optional</code> </dt> <dd>
<p class="first last">Sample weights.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>precision : float (if average is not None) or array of float, shape = [n_unique_labels]</code> </dt>  <dt>
<code>recall : float (if average is not None) or array of float, , shape = [n_unique_labels]</code> </dt>  <dt>
<code>fbeta_score : float (if average is not None) or array of float, shape = [n_unique_labels]</code> </dt>  <dt>
<code>support : int (if average is not None) or array of int, shape = [n_unique_labels]</code> </dt> <dd>
<p class="first last">The number of occurrences of each label in <code>y_true</code>.</p> </dd> </dl> </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r623484488881-1" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">Wikipedia entry for the Precision and recall</a></td>
</tr>  </table> <table class="docutils citation" frame="void" id="r623484488881-2" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id2">[2]</a></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></td>
</tr>  </table> <table class="docutils citation" frame="void" id="r623484488881-3" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id3">[3]</a></td>
<td><a class="reference external" href="http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf">Discriminative Methods for Multi-labeled Classification Advances in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu Godbole, Sunita Sarawagi</a></td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.metrics import precision_recall_fscore_support
&gt;&gt;&gt; y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])
&gt;&gt;&gt; y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])
&gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='macro')
... 
(0.22..., 0.33..., 0.26..., None)
&gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='micro')
... 
(0.33..., 0.33..., 0.33..., None)
&gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='weighted')
... 
(0.22..., 0.33..., 0.26..., None)
</pre> <p>It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:</p> <pre data-language="python">&gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=None,
... labels=['pig', 'dog', 'cat'])
... 
(array([0.        , 0.        , 0.66...]),
 array([0., 0., 1.]), array([0. , 0. , 0.8]),
 array([2, 2, 2]))
</pre> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    © 2007–2018 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html" class="_attribution-link">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
