
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Example&#58; Classification of Text Documents Using Sparse Features - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Note ">
  <meta name="keywords" content="classification, text, documents, using, sparse, features, example, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/scikit_learn/auto_examples/text/plot_document_classification_20newsgroups.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<div class="sphx-glr-download-link-note admonition note"> <p class="first admonition-title">Note</p> <p class="last">Click <a class="reference internal" href="#sphx-glr-download-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">here</span></a> to download the full example code</p> </div>  <h1 id="sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py">Classification of text documents using sparse features</h1> <p id="classification-of-text-documents-using-sparse-features">This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.</p> <p>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</p> <p>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</p> <img alt="../../_images/sphx_glr_plot_document_classification_20newsgroups_001.png" class="sphx-glr-single-img" src="data:image/png;base64,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"> <p class="sphx-glr-script-out">Out:</p> <pre data-language="none">Usage: plot_document_classification_20newsgroups.py [options]

Options:
  -h, --help            show this help message and exit
  --report              Print a detailed classification report.
  --chi2_select=SELECT_CHI2
                        Select some number of features using a chi-squared
                        test
  --confusion_matrix    Print the confusion matrix.
  --top10               Print ten most discriminative terms per class for
                        every classifier.
  --all_categories      Whether to use all categories or not.
  --use_hashing         Use a hashing vectorizer.
  --n_features=N_FEATURES
                        n_features when using the hashing vectorizer.
  --filtered            Remove newsgroup information that is easily overfit:
                        headers, signatures, and quoting.

Loading 20 newsgroups dataset for categories:
['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']
data loaded
2034 documents - 3.980MB (training set)
1353 documents - 2.867MB (test set)
4 categories

Extracting features from the training data using a sparse vectorizer
done in 0.533817s at 7.455MB/s
n_samples: 2034, n_features: 33809

Extracting features from the test data using the same vectorizer
done in 0.494300s at 5.801MB/s
n_samples: 1353, n_features: 33809

================================================================================
Ridge Classifier
________________________________________________________________________________
Training:
RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
        max_iter=None, normalize=False, random_state=None, solver='sag',
        tol=0.01)
train time: 0.324s
test time:  0.002s
accuracy:   0.896
dimensionality: 33809
density: 1.000000


================================================================================
Perceptron
________________________________________________________________________________
Training:
Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,
      fit_intercept=True, max_iter=50, n_iter=None, n_iter_no_change=5,
      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001,
      validation_fraction=0.1, verbose=0, warm_start=False)
train time: 0.030s
test time:  0.003s
accuracy:   0.888
dimensionality: 33809
density: 0.240114


================================================================================
Passive-Aggressive
________________________________________________________________________________
Training:
PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,
              early_stopping=False, fit_intercept=True, loss='hinge',
              max_iter=50, n_iter=None, n_iter_no_change=5, n_jobs=None,
              random_state=None, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
train time: 0.048s
test time:  0.003s
accuracy:   0.900
dimensionality: 33809
density: 0.702069


================================================================================
kNN
________________________________________________________________________________
Training:
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=None, n_neighbors=10, p=2,
           weights='uniform')
train time: 0.002s
test time:  0.273s
accuracy:   0.858

================================================================================
Random forest
________________________________________________________________________________
Training:
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
train time: 1.808s
test time:  0.113s
accuracy:   0.845

================================================================================
L2 penalty
________________________________________________________________________________
Training:
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,
     verbose=0)
train time: 0.167s
test time:  0.002s
accuracy:   0.900
dimensionality: 33809
density: 1.000000


________________________________________________________________________________
Training:
SGDClassifier(alpha=0.0001, average=False, class_weight=None,
       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,
       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
       power_t=0.5, random_state=None, shuffle=True, tol=None,
       validation_fraction=0.1, verbose=0, warm_start=False)
train time: 0.268s
test time:  0.003s
accuracy:   0.902
dimensionality: 33809
density: 0.666213


================================================================================
L1 penalty
________________________________________________________________________________
Training:
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,
     verbose=0)
train time: 0.279s
test time:  0.002s
accuracy:   0.873
dimensionality: 33809
density: 0.005575


________________________________________________________________________________
Training:
SGDClassifier(alpha=0.0001, average=False, class_weight=None,
       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,
       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',
       power_t=0.5, random_state=None, shuffle=True, tol=None,
       validation_fraction=0.1, verbose=0, warm_start=False)
train time: 0.729s
test time:  0.002s
accuracy:   0.888
dimensionality: 33809
density: 0.020128


================================================================================
Elastic-Net penalty
________________________________________________________________________________
Training:
SGDClassifier(alpha=0.0001, average=False, class_weight=None,
       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,
       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='elasticnet',
       power_t=0.5, random_state=None, shuffle=True, tol=None,
       validation_fraction=0.1, verbose=0, warm_start=False)
train time: 0.940s
test time:  0.003s
accuracy:   0.901
dimensionality: 33809
density: 0.186615


================================================================================
NearestCentroid (aka Rocchio classifier)
________________________________________________________________________________
Training:
NearestCentroid(metric='euclidean', shrink_threshold=None)
train time: 0.012s
test time:  0.004s
accuracy:   0.855

================================================================================
Naive Bayes
________________________________________________________________________________
Training:
MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
train time: 0.011s
test time:  0.003s
accuracy:   0.899
dimensionality: 33809
density: 1.000000


________________________________________________________________________________
Training:
BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)
train time: 0.008s
test time:  0.005s
accuracy:   0.884
dimensionality: 33809
density: 1.000000


________________________________________________________________________________
Training:
ComplementNB(alpha=0.1, class_prior=None, fit_prior=True, norm=False)
train time: 0.007s
test time:  0.003s
accuracy:   0.911
dimensionality: 33809
density: 1.000000


================================================================================
LinearSVC with L1-based feature selection
________________________________________________________________________________
Training:
Pipeline(memory=None,
     steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,
     verbose=0),
        max_features=None, no...ax_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0))])
train time: 0.343s
test time:  0.004s
accuracy:   0.880
</pre>  <pre data-language="python"># Author: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com&gt;
#         Olivier Grisel &lt;olivier.grisel@ensta.org&gt;
#         Mathieu Blondel &lt;mathieu@mblondel.org&gt;
#         Lars Buitinck
# License: BSD 3 clause

from __future__ import print_function

import logging
import numpy as np
from optparse import OptionParser
import sys
from time import time
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import RidgeClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.extmath import density
from sklearn import metrics


# Display progress logs on stdout
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)s %(message)s')


# parse commandline arguments
op = OptionParser()
op.add_option("--report",
              action="store_true", dest="print_report",
              help="Print a detailed classification report.")
op.add_option("--chi2_select",
              action="store", type="int", dest="select_chi2",
              help="Select some number of features using a chi-squared test")
op.add_option("--confusion_matrix",
              action="store_true", dest="print_cm",
              help="Print the confusion matrix.")
op.add_option("--top10",
              action="store_true", dest="print_top10",
              help="Print ten most discriminative terms per class"
                   " for every classifier.")
op.add_option("--all_categories",
              action="store_true", dest="all_categories",
              help="Whether to use all categories or not.")
op.add_option("--use_hashing",
              action="store_true",
              help="Use a hashing vectorizer.")
op.add_option("--n_features",
              action="store", type=int, default=2 ** 16,
              help="n_features when using the hashing vectorizer.")
op.add_option("--filtered",
              action="store_true",
              help="Remove newsgroup information that is easily overfit: "
                   "headers, signatures, and quoting.")


def is_interactive():
    return not hasattr(sys.modules['__main__'], '__file__')


# work-around for Jupyter notebook and IPython console
argv = [] if is_interactive() else sys.argv[1:]
(opts, args) = op.parse_args(argv)
if len(args) &gt; 0:
    op.error("this script takes no arguments.")
    sys.exit(1)

print(__doc__)
op.print_help()
print()


# #############################################################################
# Load some categories from the training set
if opts.all_categories:
    categories = None
else:
    categories = [
        'alt.atheism',
        'talk.religion.misc',
        'comp.graphics',
        'sci.space',
    ]

if opts.filtered:
    remove = ('headers', 'footers', 'quotes')
else:
    remove = ()

print("Loading 20 newsgroups dataset for categories:")
print(categories if categories else "all")

data_train = fetch_20newsgroups(subset='train', categories=categories,
                                shuffle=True, random_state=42,
                                remove=remove)

data_test = fetch_20newsgroups(subset='test', categories=categories,
                               shuffle=True, random_state=42,
                               remove=remove)
print('data loaded')

# order of labels in `target_names` can be different from `categories`
target_names = data_train.target_names


def size_mb(docs):
    return sum(len(s.encode('utf-8')) for s in docs) / 1e6


data_train_size_mb = size_mb(data_train.data)
data_test_size_mb = size_mb(data_test.data)

print("%d documents - %0.3fMB (training set)" % (
    len(data_train.data), data_train_size_mb))
print("%d documents - %0.3fMB (test set)" % (
    len(data_test.data), data_test_size_mb))
print("%d categories" % len(categories))
print()

# split a training set and a test set
y_train, y_test = data_train.target, data_test.target

print("Extracting features from the training data using a sparse vectorizer")
t0 = time()
if opts.use_hashing:
    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,
                                   n_features=opts.n_features)
    X_train = vectorizer.transform(data_train.data)
else:
    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
                                 stop_words='english')
    X_train = vectorizer.fit_transform(data_train.data)
duration = time() - t0
print("done in %fs at %0.3fMB/s" % (duration, data_train_size_mb / duration))
print("n_samples: %d, n_features: %d" % X_train.shape)
print()

print("Extracting features from the test data using the same vectorizer")
t0 = time()
X_test = vectorizer.transform(data_test.data)
duration = time() - t0
print("done in %fs at %0.3fMB/s" % (duration, data_test_size_mb / duration))
print("n_samples: %d, n_features: %d" % X_test.shape)
print()

# mapping from integer feature name to original token string
if opts.use_hashing:
    feature_names = None
else:
    feature_names = vectorizer.get_feature_names()

if opts.select_chi2:
    print("Extracting %d best features by a chi-squared test" %
          opts.select_chi2)
    t0 = time()
    ch2 = SelectKBest(chi2, k=opts.select_chi2)
    X_train = ch2.fit_transform(X_train, y_train)
    X_test = ch2.transform(X_test)
    if feature_names:
        # keep selected feature names
        feature_names = [feature_names[i] for i
                         in ch2.get_support(indices=True)]
    print("done in %fs" % (time() - t0))
    print()

if feature_names:
    feature_names = np.asarray(feature_names)


def trim(s):
    """Trim string to fit on terminal (assuming 80-column display)"""
    return s if len(s) &lt;= 80 else s[:77] + "..."


# #############################################################################
# Benchmark classifiers
def benchmark(clf):
    print('_' * 80)
    print("Training: ")
    print(clf)
    t0 = time()
    clf.fit(X_train, y_train)
    train_time = time() - t0
    print("train time: %0.3fs" % train_time)

    t0 = time()
    pred = clf.predict(X_test)
    test_time = time() - t0
    print("test time:  %0.3fs" % test_time)

    score = metrics.accuracy_score(y_test, pred)
    print("accuracy:   %0.3f" % score)

    if hasattr(clf, 'coef_'):
        print("dimensionality: %d" % clf.coef_.shape[1])
        print("density: %f" % density(clf.coef_))

        if opts.print_top10 and feature_names is not None:
            print("top 10 keywords per class:")
            for i, label in enumerate(target_names):
                top10 = np.argsort(clf.coef_[i])[-10:]
                print(trim("%s: %s" % (label, " ".join(feature_names[top10]))))
        print()

    if opts.print_report:
        print("classification report:")
        print(metrics.classification_report(y_test, pred,
                                            target_names=target_names))

    if opts.print_cm:
        print("confusion matrix:")
        print(metrics.confusion_matrix(y_test, pred))

    print()
    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in (
        (RidgeClassifier(tol=1e-2, solver="sag"), "Ridge Classifier"),
        (Perceptron(max_iter=50, tol=1e-3), "Perceptron"),
        (PassiveAggressiveClassifier(max_iter=50, tol=1e-3),
         "Passive-Aggressive"),
        (KNeighborsClassifier(n_neighbors=10), "kNN"),
        (RandomForestClassifier(n_estimators=100), "Random forest")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in ["l2", "l1"]:
    print('=' * 80)
    print("%s penalty" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,
                                       tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print("Elastic-Net penalty")
results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,
                                       penalty="elasticnet")))

# Train NearestCentroid without threshold
print('=' * 80)
print("NearestCentroid (aka Rocchio classifier)")
results.append(benchmark(NearestCentroid()))

# Train sparse Naive Bayes classifiers
print('=' * 80)
print("Naive Bayes")
results.append(benchmark(MultinomialNB(alpha=.01)))
results.append(benchmark(BernoulliNB(alpha=.01)))
results.append(benchmark(ComplementNB(alpha=.1)))

print('=' * 80)
print("LinearSVC with L1-based feature selection")
# The smaller C, the stronger the regularization.
# The more regularization, the more sparsity.
results.append(benchmark(Pipeline([
  ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False,
                                                  tol=1e-3))),
  ('classification', LinearSVC(penalty="l2"))])))

# make some plots

indices = np.arange(len(results))

results = [[x[i] for x in results] for i in range(4)]

clf_names, score, training_time, test_time = results
training_time = np.array(training_time) / np.max(training_time)
test_time = np.array(test_time) / np.max(test_time)

plt.figure(figsize=(12, 8))
plt.title("Score")
plt.barh(indices, score, .2, label="score", color='navy')
plt.barh(indices + .3, training_time, .2, label="training time",
         color='c')
plt.barh(indices + .6, test_time, .2, label="test time", color='darkorange')
plt.yticks(())
plt.legend(loc='best')
plt.subplots_adjust(left=.25)
plt.subplots_adjust(top=.95)
plt.subplots_adjust(bottom=.05)

for i, c in zip(indices, clf_names):
    plt.text(-.3, i, c)

plt.show()
</pre> <p><strong>Total running time of the script:</strong> ( 0 minutes 7.076 seconds)</p> <div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-text-plot-document-classification-20newsgroups-py"> <div class="sphx-glr-download docutils container"> <a class="reference download internal" href="http://scikit-learn.org/stable/_downloads/plot_document_classification_20newsgroups.py" download=""><code>Download Python source code: plot_document_classification_20newsgroups.py</code></a>
</div> <div class="sphx-glr-download docutils container"> <a class="reference download internal" href="http://scikit-learn.org/stable/_downloads/plot_document_classification_20newsgroups.ipynb" download=""><code>Download Jupyter notebook: plot_document_classification_20newsgroups.ipynb</code></a>
</div> </div> <p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2018 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html" class="_attribution-link">http://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
