
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>numpy.einsum() - NumPy 1.17 - W3cubDocs</title>
  
  <meta name="description" content=" Evaluates the Einstein summation convention on the operands. ">
  <meta name="keywords" content="numpy, einsum, numpy~1.17">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/numpy~1.17/generated/numpy.einsum.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/numpy~1.17.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/numpy~1.17/" class="_nav-link" title="" style="margin-left:0;">NumPy 1.17</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="numpy-einsum">numpy.einsum</h1> <dl class="function"> <dt id="numpy.einsum">
<code>numpy.einsum(subscripts, *operands, out=None, dtype=None, order='K', casting='safe', optimize=False)</code> <a class="reference external" href="https://github.com/numpy/numpy/blob/v1.17.0/numpy/core/einsumfunc.py#L1004-L1432"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Evaluates the Einstein summation convention on the operands.</p> <p>Using the Einstein summation convention, many common multi-dimensional, linear algebraic array operations can be represented in a simple fashion. In <em>implicit</em> mode <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> computes these values.</p> <p>In <em>explicit</em> mode, <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> provides further flexibility to compute other array operations that might not be considered classical Einstein summation operations, by disabling, or forcing summation over specified subscript labels.</p> <p>See the notes and examples for clarification.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<dl class="first docutils"> <dt>
<code>subscripts : str</code> </dt> <dd>
<p class="first last">Specifies the subscripts for summation as comma separated list of subscript labels. An implicit (classical Einstein summation) calculation is performed unless the explicit indicator ‘-&gt;’ is included as well as subscript labels of the precise output form.</p> </dd> <dt>
<code>operands : list of array_like</code> </dt> <dd>
<p class="first last">These are the arrays for the operation.</p> </dd> <dt>
<code>out : ndarray, optional</code> </dt> <dd>
<p class="first last">If provided, the calculation is done into this array.</p> </dd> <dt>
<code>dtype : {data-type, None}, optional</code> </dt> <dd>
<p class="first last">If provided, forces the calculation to use the data type specified. Note that you may have to also give a more liberal <code>casting</code> parameter to allow the conversions. Default is None.</p> </dd> <dt>
<code>order : {‘C’, ‘F’, ‘A’, ‘K’}, optional</code> </dt> <dd>
<p class="first last">Controls the memory layout of the output. ‘C’ means it should be C contiguous. ‘F’ means it should be Fortran contiguous, ‘A’ means it should be ‘F’ if the inputs are all ‘F’, ‘C’ otherwise. ‘K’ means it should be as close to the layout as the inputs as is possible, including arbitrarily permuted axes. Default is ‘K’.</p> </dd> <dt>
<code>casting : {‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’}, optional</code> </dt> <dd>
<p class="first">Controls what kind of data casting may occur. Setting this to ‘unsafe’ is not recommended, as it can adversely affect accumulations.</p>  <ul class="simple"> <li>‘no’ means the data types should not be cast at all.</li> <li>‘equiv’ means only byte-order changes are allowed.</li> <li>‘safe’ means only casts which can preserve values are allowed.</li> <li>‘same_kind’ means only safe casts or casts within a kind, like float64 to float32, are allowed.</li> <li>‘unsafe’ means any data conversions may be done.</li> </ul>  <p class="last">Default is ‘safe’.</p> </dd> <dt>
<code>optimize : {False, True, ‘greedy’, ‘optimal’}, optional</code> </dt> <dd>
<p class="first last">Controls if intermediate optimization should occur. No optimization will occur if False and True will default to the ‘greedy’ algorithm. Also accepts an explicit contraction list from the <code>np.einsum_path</code> function. See <code>np.einsum_path</code> for more details. Defaults to False.</p> </dd> </dl> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<dl class="first last docutils"> <dt>
<code>output : ndarray</code> </dt> <dd>
<p class="first last">The calculation based on the Einstein summation convention.</p> </dd> </dl> </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="numpy.einsum_path#numpy.einsum_path" title="numpy.einsum_path"><code>einsum_path</code></a>, <a class="reference internal" href="numpy.dot#numpy.dot" title="numpy.dot"><code>dot</code></a>, <a class="reference internal" href="numpy.inner#numpy.inner" title="numpy.inner"><code>inner</code></a>, <a class="reference internal" href="numpy.outer#numpy.outer" title="numpy.outer"><code>outer</code></a>, <a class="reference internal" href="numpy.tensordot#numpy.tensordot" title="numpy.tensordot"><code>tensordot</code></a>, <a class="reference internal" href="numpy.linalg.multi_dot#numpy.linalg.multi_dot" title="numpy.linalg.multi_dot"><code>linalg.multi_dot</code></a></p> </div> <h4 class="rubric">Notes</h4> <div class="versionadded"> <p><span class="versionmodified">New in version 1.6.0.</span></p> </div> <p>The Einstein summation convention can be used to compute many multi-dimensional, linear algebraic array operations. <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> provides a succinct way of representing these.</p> <p>A non-exhaustive list of these operations, which can be computed by <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a>, is shown below along with examples:</p> <ul class="simple"> <li>Trace of an array, <a class="reference internal" href="numpy.trace#numpy.trace" title="numpy.trace"><code>numpy.trace</code></a>.</li> <li>Return a diagonal, <a class="reference internal" href="numpy.diag#numpy.diag" title="numpy.diag"><code>numpy.diag</code></a>.</li> <li>Array axis summations, <a class="reference internal" href="numpy.sum#numpy.sum" title="numpy.sum"><code>numpy.sum</code></a>.</li> <li>Transpositions and permutations, <a class="reference internal" href="numpy.transpose#numpy.transpose" title="numpy.transpose"><code>numpy.transpose</code></a>.</li> <li>Matrix multiplication and dot product, <a class="reference internal" href="numpy.matmul#numpy.matmul" title="numpy.matmul"><code>numpy.matmul</code></a> <a class="reference internal" href="numpy.dot#numpy.dot" title="numpy.dot"><code>numpy.dot</code></a>.</li> <li>Vector inner and outer products, <a class="reference internal" href="numpy.inner#numpy.inner" title="numpy.inner"><code>numpy.inner</code></a> <a class="reference internal" href="numpy.outer#numpy.outer" title="numpy.outer"><code>numpy.outer</code></a>.</li> <li>Broadcasting, element-wise and scalar multiplication, <a class="reference internal" href="numpy.multiply#numpy.multiply" title="numpy.multiply"><code>numpy.multiply</code></a>.</li> <li>Tensor contractions, <a class="reference internal" href="numpy.tensordot#numpy.tensordot" title="numpy.tensordot"><code>numpy.tensordot</code></a>.</li> <li>Chained array operations, in efficient calculation order, <a class="reference internal" href="numpy.einsum_path#numpy.einsum_path" title="numpy.einsum_path"><code>numpy.einsum_path</code></a>.</li> </ul> <p>The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so <code>np.einsum('i,i', a, b)</code> is equivalent to <a class="reference internal" href="numpy.inner#numpy.inner" title="numpy.inner"><code>np.inner(a,b)</code></a>. If a label appears only once, it is not summed, so <code>np.einsum('i', a)</code> produces a view of <code>a</code> with no changes. A further example <code>np.einsum('ij,jk', a, b)</code> describes traditional matrix multiplication and is equivalent to <a class="reference internal" href="numpy.matmul#numpy.matmul" title="numpy.matmul"><code>np.matmul(a,b)</code></a>. Repeated subscript labels in one operand take the diagonal. For example, <code>np.einsum('ii', a)</code> is equivalent to <a class="reference internal" href="numpy.trace#numpy.trace" title="numpy.trace"><code>np.trace(a)</code></a>.</p> <p>In <em>implicit mode</em>, the chosen subscripts are important since the axes of the output are reordered alphabetically. This means that <code>np.einsum('ij', a)</code> doesn’t affect a 2D array, while <code>np.einsum('ji', a)</code> takes its transpose. Additionally, <code>np.einsum('ij,jk', a, b)</code> returns a matrix multiplication, while, <code>np.einsum('ij,jh', a, b)</code> returns the transpose of the multiplication since subscript ‘h’ precedes subscript ‘i’.</p> <p>In <em>explicit mode</em> the output can be directly controlled by specifying output subscript labels. This requires the identifier ‘-&gt;’ as well as the list of output subscript labels. This feature increases the flexibility of the function since summing can be disabled or forced when required. The call <code>np.einsum('i-&gt;', a)</code> is like <a class="reference internal" href="numpy.sum#numpy.sum" title="numpy.sum"><code>np.sum(a, axis=-1)</code></a>, and <code>np.einsum('ii-&gt;i', a)</code> is like <a class="reference internal" href="numpy.diag#numpy.diag" title="numpy.diag"><code>np.diag(a)</code></a>. The difference is that <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> does not allow broadcasting by default. Additionally <code>np.einsum('ij,jh-&gt;ih', a, b)</code> directly specifies the order of the output subscript labels and therefore returns matrix multiplication, unlike the example above in implicit mode.</p> <p>To enable and control broadcasting, use an ellipsis. Default NumPy-style broadcasting is done by adding an ellipsis to the left of each term, like <code>np.einsum('...ii-&gt;...i', a)</code>. To take the trace along the first and last axes, you can do <code>np.einsum('i...i', a)</code>, or to do a matrix-matrix product with the left-most indices instead of rightmost, one can do <code>np.einsum('ij...,jk...-&gt;ik...', a, b)</code>.</p> <p>When there is only one operand, no axes are summed, and no output parameter is provided, a view into the operand is returned instead of a new array. Thus, taking the diagonal as <code>np.einsum('ii-&gt;i', a)</code> produces a view (changed in version 1.10.0).</p> <p><a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> also provides an alternative way to provide the subscripts and operands as <code>einsum(op0, sublist0, op1, sublist1, ..., [sublistout])</code>. If the output shape is not provided in this format <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> will be calculated in implicit mode, otherwise it will be performed explicitly. The examples below have corresponding <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> calls with the two parameter methods.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 1.10.0.</span></p> </div> <p>Views returned from einsum are now writeable whenever the input array is writeable. For example, <code>np.einsum('ijk...-&gt;kji...', a)</code> will now have the same effect as <a class="reference internal" href="numpy.swapaxes#numpy.swapaxes" title="numpy.swapaxes"><code>np.swapaxes(a, 0, 2)</code></a> and <code>np.einsum('ii-&gt;i', a)</code> will return a writeable view of the diagonal of a 2D array.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 1.12.0.</span></p> </div> <p>Added the <code>optimize</code> argument which will optimize the contraction order of an einsum expression. For a contraction with three or more operands this can greatly increase the computational efficiency at the cost of a larger memory footprint during computation.</p> <p>Typically a ‘greedy’ algorithm is applied which empirical tests have shown returns the optimal path in the majority of cases. In some cases ‘optimal’ will return the superlative path through a more expensive, exhaustive search. For iterative calculations it may be advisable to calculate the optimal path once and reuse that path by supplying it as an argument. An example is given below.</p> <p>See <a class="reference internal" href="numpy.einsum_path#numpy.einsum_path" title="numpy.einsum_path"><code>numpy.einsum_path</code></a> for more details.</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; a = np.arange(25).reshape(5,5)
&gt;&gt;&gt; b = np.arange(5)
&gt;&gt;&gt; c = np.arange(6).reshape(2,3)
</pre> <p>Trace of a matrix:</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('ii', a)
60
&gt;&gt;&gt; np.einsum(a, [0,0])
60
&gt;&gt;&gt; np.trace(a)
60
</pre> <p>Extract the diagonal (requires explicit form):</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('ii-&gt;i', a)
array([ 0,  6, 12, 18, 24])
&gt;&gt;&gt; np.einsum(a, [0,0], [0])
array([ 0,  6, 12, 18, 24])
&gt;&gt;&gt; np.diag(a)
array([ 0,  6, 12, 18, 24])
</pre> <p>Sum over an axis (requires explicit form):</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('ij-&gt;i', a)
array([ 10,  35,  60,  85, 110])
&gt;&gt;&gt; np.einsum(a, [0,1], [0])
array([ 10,  35,  60,  85, 110])
&gt;&gt;&gt; np.sum(a, axis=1)
array([ 10,  35,  60,  85, 110])
</pre> <p>For higher dimensional arrays summing a single axis can be done with ellipsis:</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('...j-&gt;...', a)
array([ 10,  35,  60,  85, 110])
&gt;&gt;&gt; np.einsum(a, [Ellipsis,1], [Ellipsis])
array([ 10,  35,  60,  85, 110])
</pre> <p>Compute a matrix transpose, or reorder any number of axes:</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('ji', c)
array([[0, 3],
       [1, 4],
       [2, 5]])
&gt;&gt;&gt; np.einsum('ij-&gt;ji', c)
array([[0, 3],
       [1, 4],
       [2, 5]])
&gt;&gt;&gt; np.einsum(c, [1,0])
array([[0, 3],
       [1, 4],
       [2, 5]])
&gt;&gt;&gt; np.transpose(c)
array([[0, 3],
       [1, 4],
       [2, 5]])
</pre> <p>Vector inner products:</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('i,i', b, b)
30
&gt;&gt;&gt; np.einsum(b, [0], b, [0])
30
&gt;&gt;&gt; np.inner(b,b)
30
</pre> <p>Matrix vector multiplication:</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('ij,j', a, b)
array([ 30,  80, 130, 180, 230])
&gt;&gt;&gt; np.einsum(a, [0,1], b, [1])
array([ 30,  80, 130, 180, 230])
&gt;&gt;&gt; np.dot(a, b)
array([ 30,  80, 130, 180, 230])
&gt;&gt;&gt; np.einsum('...j,j', a, b)
array([ 30,  80, 130, 180, 230])
</pre> <p>Broadcasting and scalar multiplication:</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('..., ...', 3, c)
array([[ 0,  3,  6],
       [ 9, 12, 15]])
&gt;&gt;&gt; np.einsum(',ij', 3, c)
array([[ 0,  3,  6],
       [ 9, 12, 15]])
&gt;&gt;&gt; np.einsum(3, [Ellipsis], c, [Ellipsis])
array([[ 0,  3,  6],
       [ 9, 12, 15]])
&gt;&gt;&gt; np.multiply(3, c)
array([[ 0,  3,  6],
       [ 9, 12, 15]])
</pre> <p>Vector outer product:</p> <pre data-language="python">&gt;&gt;&gt; np.einsum('i,j', np.arange(2)+1, b)
array([[0, 1, 2, 3, 4],
       [0, 2, 4, 6, 8]])
&gt;&gt;&gt; np.einsum(np.arange(2)+1, [0], b, [1])
array([[0, 1, 2, 3, 4],
       [0, 2, 4, 6, 8]])
&gt;&gt;&gt; np.outer(np.arange(2)+1, b)
array([[0, 1, 2, 3, 4],
       [0, 2, 4, 6, 8]])
</pre> <p>Tensor contraction:</p> <pre data-language="python">&gt;&gt;&gt; a = np.arange(60.).reshape(3,4,5)
&gt;&gt;&gt; b = np.arange(24.).reshape(4,3,2)
&gt;&gt;&gt; np.einsum('ijk,jil-&gt;kl', a, b)
array([[4400., 4730.],
       [4532., 4874.],
       [4664., 5018.],
       [4796., 5162.],
       [4928., 5306.]])
&gt;&gt;&gt; np.einsum(a, [0,1,2], b, [1,0,3], [2,3])
array([[4400., 4730.],
       [4532., 4874.],
       [4664., 5018.],
       [4796., 5162.],
       [4928., 5306.]])
&gt;&gt;&gt; np.tensordot(a,b, axes=([1,0],[0,1]))
array([[4400., 4730.],
       [4532., 4874.],
       [4664., 5018.],
       [4796., 5162.],
       [4928., 5306.]])
</pre> <p>Writeable returned arrays (since version 1.10.0):</p> <pre data-language="python">&gt;&gt;&gt; a = np.zeros((3, 3))
&gt;&gt;&gt; np.einsum('ii-&gt;i', a)[:] = 1
&gt;&gt;&gt; a
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
</pre> <p>Example of ellipsis use:</p> <pre data-language="python">&gt;&gt;&gt; a = np.arange(6).reshape((3,2))
&gt;&gt;&gt; b = np.arange(12).reshape((4,3))
&gt;&gt;&gt; np.einsum('ki,jk-&gt;ij', a, b)
array([[10, 28, 46, 64],
       [13, 40, 67, 94]])
&gt;&gt;&gt; np.einsum('ki,...k-&gt;i...', a, b)
array([[10, 28, 46, 64],
       [13, 40, 67, 94]])
&gt;&gt;&gt; np.einsum('k...,jk', a, b)
array([[10, 28, 46, 64],
       [13, 40, 67, 94]])
</pre> <p>Chained array operations. For more complicated contractions, speed ups might be achieved by repeatedly computing a ‘greedy’ path or pre-computing the ‘optimal’ path and repeatedly applying it, using an <a class="reference internal" href="numpy.einsum_path#numpy.einsum_path" title="numpy.einsum_path"><code>einsum_path</code></a> insertion (since version 1.12.0). Performance improvements can be particularly significant with larger arrays:</p> <pre data-language="python">&gt;&gt;&gt; a = np.ones(64).reshape(2,4,8)
</pre> <p>Basic <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a>: ~1520ms (benchmarked on 3.1GHz Intel i5.)</p> <pre data-language="python">&gt;&gt;&gt; for iteration in range(500):
...     _ = np.einsum('ijk,ilm,njm,nlk,abc-&gt;',a,a,a,a,a)
</pre> <p>Sub-optimal <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> (due to repeated path calculation time): ~330ms</p> <pre data-language="python">&gt;&gt;&gt; for iteration in range(500):
...     _ = np.einsum('ijk,ilm,njm,nlk,abc-&gt;',a,a,a,a,a, optimize='optimal')
</pre> <p>Greedy <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> (faster optimal path approximation): ~160ms</p> <pre data-language="python">&gt;&gt;&gt; for iteration in range(500):
...     _ = np.einsum('ijk,ilm,njm,nlk,abc-&gt;',a,a,a,a,a, optimize='greedy')
</pre> <p>Optimal <a class="reference internal" href="#numpy.einsum" title="numpy.einsum"><code>einsum</code></a> (best usage pattern in some use cases): ~110ms</p> <pre data-language="python">&gt;&gt;&gt; path = np.einsum_path('ijk,ilm,njm,nlk,abc-&gt;',a,a,a,a,a, optimize='optimal')[0]
&gt;&gt;&gt; for iteration in range(500):
...     _ = np.einsum('ijk,ilm,njm,nlk,abc-&gt;',a,a,a,a,a, optimize=path)
</pre> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2005–2019 NumPy Developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://docs.scipy.org/doc/numpy-1.17.0/reference/generated/numpy.einsum.html" class="_attribution-link">https://docs.scipy.org/doc/numpy-1.17.0/reference/generated/numpy.einsum.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
