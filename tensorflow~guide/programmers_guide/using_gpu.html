
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Using GPUs - TensorFlow Guide - W3cubDocs</title>
  
  <meta name="description" content="On a typical system, there are multiple computing devices. In TensorFlow, the supported device types are CPU and GPU. They are represented as &hellip;">
  <meta name="keywords" content="using, gpus, tensorflow, guide, tensorflow~guide">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~guide/programmers_guide/using_gpu.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-4205f3012478879f8c56fe996c2028fb79885cdfa2a555f0246a77154592eefd5cbfab91f37f1715fe7ccfe78d4f5ebf2a64b27344118d69549f74bb7bb03769.css">
  <script src="/assets/application-6642e8a44fdf75b0cdac9f6b93996611b714089b67b28678421f881289dbb80a0bed9f9975268d4499e9e1498c5157b8af1460631c66e1efd65cea387e868f4e.js" type="text/javascript"></script>
  <script src="/json/tensorflow~guide.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~guide/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Guide</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> Using GPUs </h1>     <h2 id="supported_devices">Supported devices</h2> <p>On a typical system, there are multiple computing devices. In TensorFlow, the supported device types are <code>CPU</code> and <code>GPU</code>. They are represented as <code>strings</code>. For example:</p> <ul> <li>
<code>"/cpu:0"</code>: The CPU of your machine.</li> <li>
<code>"/device:GPU:0"</code>: The GPU of your machine, if you have one.</li> <li>
<code>"/device:GPU:1"</code>: The second GPU of your machine, etc.</li> </ul> <p>If a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device. For example, <code>matmul</code> has both CPU and GPU kernels. On a system with devices <code>cpu:0</code> and <code>gpu:0</code>, <code>gpu:0</code> will be selected to run <code>matmul</code>.</p> <h2 id="logging_device_placement">Logging Device placement</h2> <p>To find out which devices your operations and tensors are assigned to, create the session with <code>log_device_placement</code> configuration option set to <code>True</code>.</p> <pre class="prettyprint lang-python" data-language="python"># Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))
</pre> <p>You should see the following output:</p> <pre class="prettyprint" data-language="cpp">Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K40c, pci bus
id: 0000:05:00.0
b: /job:localhost/replica:0/task:0/device:GPU:0
a: /job:localhost/replica:0/task:0/device:GPU:0
MatMul: /job:localhost/replica:0/task:0/device:GPU:0
[[ 22.  28.]
 [ 49.  64.]]

</pre> <h2 id="manual_device_placement">Manual device placement</h2> <p>If you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use <code>with tf.device</code> to create a device context such that all the operations within that context will have the same device assignment.</p> <pre class="prettyprint lang-python" data-language="python"># Creates a graph.
with tf.device('/cpu:0'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))
</pre> <p>You will see that now <code>a</code> and <code>b</code> are assigned to <code>cpu:0</code>. Since a device was not explicitly specified for the <code>MatMul</code> operation, the TensorFlow runtime will choose one based on the operation and available devices (<code>gpu:0</code> in this example) and automatically copy tensors between devices if required.</p> <pre class="prettyprint" data-language="cpp">Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K40c, pci bus
id: 0000:05:00.0
b: /job:localhost/replica:0/task:0/cpu:0
a: /job:localhost/replica:0/task:0/cpu:0
MatMul: /job:localhost/replica:0/task:0/device:GPU:0
[[ 22.  28.]
 [ 49.  64.]]
</pre> <h2 id="allowing_gpu_memory_growth">Allowing GPU memory growth</h2> <p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars"><code>CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing <a href="https://en.wikipedia.org/wiki/Fragmentation_(computing)">memory fragmentation</a>.</p> <p>In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as is needed by the process. TensorFlow provides two Config options on the Session to control this.</p> <p>The first is the <code>allow_growth</code> option, which attempts to allocate only as much GPU memory based on runtime allocations: it starts out allocating very little memory, and as Sessions get run and more GPU memory is needed, we extend the GPU memory region needed by the TensorFlow process. Note that we do not release memory, since that can lead to even worse memory fragmentation. To turn this option on, set the option in the ConfigProto by:</p> <pre class="prettyprint lang-python" data-language="python">config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config, ...)
</pre> <p>The second method is the <code>per_process_gpu_memory_fraction</code> option, which determines the fraction of the overall amount of memory that each visible GPU should be allocated. For example, you can tell TensorFlow to only allocate 40% of the total memory of each GPU by:</p> <pre class="prettyprint lang-python" data-language="python">config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
session = tf.Session(config=config, ...)
</pre> <p>This is useful if you want to truly bound the amount of GPU memory available to the TensorFlow process.</p> <h2 id="using_a_single_gpu_on_a_multi-gpu_system">Using a single GPU on a multi-GPU system</h2> <p>If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. If you would like to run on a different GPU, you will need to specify the preference explicitly:</p> <pre class="prettyprint lang-python" data-language="python"># Creates a graph.
with tf.device('/device:GPU:2'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
  c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))
</pre> <p>If the device you have specified does not exist, you will get <code>InvalidArgumentError</code>:</p> <pre class="prettyprint" data-language="cpp">InvalidArgumentError: Invalid argument: Cannot assign a device to node 'b':
Could not satisfy explicit device specification '/device:GPU:2'
   [[Node: b = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [3,2]
   values: 1 2 3...&gt;, _device="/device:GPU:2"]()]]
</pre> <p>If you would like TensorFlow to automatically choose an existing and supported device to run the operations in case the specified one doesn't exist, you can set <code>allow_soft_placement</code> to <code>True</code> in the configuration option when creating the session.</p> <pre class="prettyprint lang-python" data-language="python"># Creates a graph.
with tf.device('/device:GPU:2'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
  c = tf.matmul(a, b)
# Creates a session with allow_soft_placement and log_device_placement set
# to True.
sess = tf.Session(config=tf.ConfigProto(
      allow_soft_placement=True, log_device_placement=True))
# Runs the op.
print(sess.run(c))
</pre> <h2 id="using_multiple_gpus">Using multiple GPUs</h2> <p>If you would like to run TensorFlow on multiple GPUs, you can construct your model in a multi-tower fashion where each tower is assigned to a different GPU. For example:</p> <pre class="prettyprint lang-python" data-language="python"># Creates a graph.
c = []
for d in ['/device:GPU:2', '/device:GPU:3']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(sum))
</pre> <p>You will see the following output.</p> <pre class="prettyprint" data-language="cpp">Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K20m, pci bus
id: 0000:02:00.0
/job:localhost/replica:0/task:0/device:GPU:1 -&gt; device: 1, name: Tesla K20m, pci bus
id: 0000:03:00.0
/job:localhost/replica:0/task:0/device:GPU:2 -&gt; device: 2, name: Tesla K20m, pci bus
id: 0000:83:00.0
/job:localhost/replica:0/task:0/device:GPU:3 -&gt; device: 3, name: Tesla K20m, pci bus
id: 0000:84:00.0
Const_3: /job:localhost/replica:0/task:0/device:GPU:3
Const_2: /job:localhost/replica:0/task:0/device:GPU:3
MatMul_1: /job:localhost/replica:0/task:0/device:GPU:3
Const_1: /job:localhost/replica:0/task:0/device:GPU:2
Const: /job:localhost/replica:0/task:0/device:GPU:2
MatMul: /job:localhost/replica:0/task:0/device:GPU:2
AddN: /job:localhost/replica:0/task:0/cpu:0
[[  44.   56.]
 [  98.  128.]]
</pre> <p>The <a href="../tutorials/deep_cnn">cifar10 tutorial</a> is a good example demonstrating how to do training with multiple GPUs.</p>
<div class="_attribution">
  <p class="_attribution-p">
    © 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/programmers_guide/using_gpu" class="_attribution-link">https://www.tensorflow.org/programmers_guide/using_gpu</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
