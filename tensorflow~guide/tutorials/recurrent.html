
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Recurrent Neural Networks - TensorFlow Guide - W3cubDocs</title>
  
  <meta name="description" content=" Take a look at this great article for an introduction to recurrent neural networks and LSTMs in particular. ">
  <meta name="keywords" content="recurrent, neural, networks, tensorflow, guide, tensorflow~guide">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~guide/tutorials/recurrent.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-vWZ9W/y+fPvxx895gHec4ryHsIkFG4WK5PEliF/Zcs/yOrkA7rGz0CvUiVIfyko1uWktxJ1G7nYlkIn7C9Qpvw==" crossorigin="anonymous" src="/assets/application-bd667d5bfcbe7cfbf1c7cf7980779ce2bc87b089051b858ae4f125885fd972cff23ab900eeb1b3d02bd489521fca4a35b9692dc49d46ee76259089fb0bd429bf.js"></script>
  <script src="/json/tensorflow~guide.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~guide/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Guide</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> Recurrent Neural Networks </h1>     <h2 id="introduction">Introduction</h2> <p>Take a look at <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">this great article</a> for an introduction to recurrent neural networks and LSTMs in particular.</p> <h2 id="language_modeling">Language Modeling</h2> <p>In this tutorial we will show how to train a recurrent neural network on a challenging task of language modeling. The goal of the problem is to fit a probabilistic model which assigns probabilities to sentences. It does so by predicting next words in a text given a history of previous words. For this purpose we will use the <a href="https://catalog.ldc.upenn.edu/ldc99t42">Penn Tree Bank</a> (PTB) dataset, which is a popular benchmark for measuring the quality of these models, whilst being small and relatively fast to train.</p> <p>Language modeling is key to many interesting problems such as speech recognition, machine translation, or image captioning. It is also fun -- take a look <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">here</a>.</p> <p>For the purpose of this tutorial, we will reproduce the results from <a href="https://arxiv.org/abs/1409.2329">Zaremba et al., 2014</a> (<a href="https://arxiv.org/pdf/1409.2329.pdf">pdf</a>), which achieves very good quality on the PTB dataset.</p> <h2 id="tutorial_files">Tutorial Files</h2> <p>This tutorial references the following files from <code>models/tutorials/rnn/ptb</code> in the <a href="https://github.com/tensorflow/models">TensorFlow models repo</a>:</p> <table> <thead> <tr> <th>File</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td><code>ptb_word_lm.py</code></td> <td>The code to train a language model on the PTB dataset.</td> </tr> <tr> <td><code>reader.py</code></td> <td>The code to read the dataset.</td> </tr> </tbody> </table> <h2 id="download_and_prepare_the_data">Download and Prepare the Data</h2> <p>The data required for this tutorial is in the <code>data/</code> directory of the <a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz">PTB dataset from Tomas Mikolov's webpage</a>.</p> <p>The dataset is already preprocessed and contains overall 10000 different words, including the end-of-sentence marker and a special symbol (\&lt;unk&gt;) for rare words. In <code>reader.py</code>, we convert each word to a unique integer identifier, in order to make it easy for the neural network to process the data.</p> <h2 id="the_model">The Model</h2> <h3 id="lstm">LSTM</h3> <p>The core of the model consists of an LSTM cell that processes one word at a time and computes probabilities of the possible values for the next word in the sentence. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word. For computational reasons, we will process data in mini-batches of size <code>batch_size</code>. In this example, it is important to note that <code>current_batch_of_words</code> does not correspond to a "sentence" of words. Every word in a batch should correspond to a time t. TensorFlow will automatically sum the gradients of each batch for you.</p> <p>For example:</p> <pre class="prettyprint" data-language="cpp"> t=0  t=1    t=2  t=3     t=4
[The, brown, fox, is,     quick]
[The, red,   fox, jumped, high]

words_in_dataset[0] = [The, The]
words_in_dataset[1] = [brown, red]
words_in_dataset[2] = [fox, fox]
words_in_dataset[3] = [is, jumped]
words_in_dataset[4] = [quick, high]
batch_size = 2, time_steps = 5
</pre> <p>The basic pseudocode is as follows:</p> <pre class="prettyprint lang-python" data-language="python">words_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])
lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
hidden_state = tf.zeros([batch_size, lstm.state_size])
current_state = tf.zeros([batch_size, lstm.state_size])
state = hidden_state, current_state
probabilities = []
loss = 0.0
for current_batch_of_words in words_in_dataset:
    # The value of state is updated after processing each batch of words.
    output, state = lstm(current_batch_of_words, state)

    # The LSTM output can be used to make next word predictions
    logits = tf.matmul(output, softmax_w) + softmax_b
    probabilities.append(tf.nn.softmax(logits))
    loss += loss_function(probabilities, target_words)
</pre> <h3 id="truncated_backpropagation">Truncated Backpropagation</h3> <p>By design, the output of a recurrent neural network (RNN) depends on arbitrarily distant inputs. Unfortunately, this makes backpropagation computation difficult. In order to make the learning process tractable, it is common practice to create an "unrolled" version of the network, which contains a fixed number (<code>num_steps</code>) of LSTM inputs and outputs. The model is then trained on this finite approximation of the RNN. This can be implemented by feeding inputs of length <code>num_steps</code> at a time and performing a backward pass after each such input block.</p> <p>Here is a simplified block of code for creating a graph which performs truncated backpropagation:</p> <pre class="prettyprint lang-python" data-language="python"># Placeholder for the inputs in a given iteration.
words = tf.placeholder(tf.int32, [batch_size, num_steps])

lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
initial_state = state = tf.zeros([batch_size, lstm.state_size])

for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)

    # The rest of the code.
    # ...

final_state = state
</pre> <p>And this is how to implement an iteration over the whole dataset:</p> <pre class="prettyprint lang-python" data-language="python"># A numpy array holding the state of LSTM after each batch of words.
numpy_state = initial_state.eval()
total_loss = 0.0
for current_batch_of_words in words_in_dataset:
    numpy_state, current_loss = session.run([final_state, loss],
        # Initialize the LSTM state from the previous iteration.
        feed_dict={initial_state: numpy_state, words: current_batch_of_words})
    total_loss += current_loss
</pre> <h3 id="inputs">Inputs</h3> <p>The word IDs will be embedded into a dense representation (see the <a href="word2vec">Vector Representations Tutorial</a>) before feeding to the LSTM. This allows the model to efficiently represent the knowledge about particular words. It is also easy to write:</p> <pre class="prettyprint lang-python" data-language="python"># embedding_matrix is a tensor of shape [vocabulary_size, embedding size]
word_embeddings = tf.nn.embedding_lookup(embedding_matrix, word_ids)
</pre> <p>The embedding matrix will be initialized randomly and the model will learn to differentiate the meaning of words just by looking at the data.</p> <h3 id="loss_function">Loss Function</h3> <p>We want to minimize the average negative log probability of the target words:</p> <div> $$ \text{loss} = -\frac{1}{N}\sum_{i=1}^{N} \ln p_{\text{target}_i} $$ </div> <p>It is not very difficult to implement but the function <code>sequence_loss_by_example</code> is already available, so we can just use it here.</p> <p>The typical measure reported in the papers is average per-word perplexity (often just called perplexity), which is equal to</p> <div> $$e^{-\frac{1}{N}\sum_{i=1}^{N} \ln p_{\text{target}_i}} = e^{\text{loss}} $$ </div> <p>and we will monitor its value throughout the training process.</p> <h3 id="stacking_multiple_lstms">Stacking multiple LSTMs</h3> <p>To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.</p> <p>We have a class called <code>MultiRNNCell</code> that makes the implementation seamless:</p> <pre class="prettyprint lang-python" data-language="python">def lstm_cell():
  return tf.contrib.rnn.BasicLSTMCell(lstm_size)
stacked_lstm = tf.contrib.rnn.MultiRNNCell(
    [lstm_cell() for _ in range(number_of_layers)])

initial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)
for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = stacked_lstm(words[:, i], state)

    # The rest of the code.
    # ...

final_state = state
</pre> <h2 id="run_the_code">Run the Code</h2> <p>Before running the code, download the PTB dataset, as discussed at the beginning of this tutorial. Then, extract the PTB dataset underneath your home directory as follows:</p> <pre class="prettyprint lang-bsh" data-language="cpp">tar xvfz simple-examples.tgz -C $HOME
</pre> <p><em>(Note: On Windows, you may need to use <a href="https://wiki.haskell.org/How_to_unpack_a_tar_file_in_Windows">other tools</a>.)</em></p> <p>Now, clone the <a href="https://github.com/tensorflow/models">TensorFlow models repo</a> from GitHub. Run the following commands:</p> <pre class="prettyprint lang-bsh" data-language="cpp">cd models/tutorials/rnn/ptb
python ptb_word_lm.py --data_path=$HOME/simple-examples/data/ --model=small
</pre> <p>There are 3 supported model configurations in the tutorial code: "small", "medium" and "large". The difference between them is in size of the LSTMs and the set of hyperparameters used for training.</p> <p>The larger the model, the better results it should get. The <code>small</code> model should be able to reach perplexity below 120 on the test set and the <code>large</code> one below 80, though it might take several hours to train.</p> <h2 id="what_next">What Next?</h2> <p>There are several tricks that we haven't mentioned that make the model better, including:</p> <ul> <li>decreasing learning rate schedule,</li> <li>dropout between the LSTM layers.</li> </ul> <p>Study the code and modify it to improve the model even further.</p>
<div class="_attribution">
  <p class="_attribution-p">
    © 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/tutorials/recurrent" class="_attribution-link">https://www.tensorflow.org/tutorials/recurrent</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
