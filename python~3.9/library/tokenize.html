
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Tokenize - Python 3.9 - W3cubDocs</title>
  
  <meta name="description" content=" Source code&#58; Lib&#47;tokenize.py ">
  <meta name="keywords" content="tokenize, —, tokenizer, for, python, source, python~3.9">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/python~3.9/library/tokenize.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/python~3.9.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/python~3.9/" class="_nav-link" title="" style="margin-left:0;">Python 3.9</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _python">
				
				
<h1 id="tokenize-tokenizer-for-python-source">tokenize — Tokenizer for Python source</h1> <p id="module-tokenize"><strong>Source code:</strong> <a class="reference external" href="https://github.com/python/cpython/tree/3.9/Lib/tokenize.py">Lib/tokenize.py</a></p>  <p>The <a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code>tokenize</code></a> module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it useful for implementing “pretty-printers”, including colorizers for on-screen displays.</p> <p>To simplify token stream handling, all <a class="reference internal" href="https://docs.python.org/3.9/reference/lexical_analysis.html#operators"><span class="std std-ref">operator</span></a> and <a class="reference internal" href="https://docs.python.org/3.9/reference/lexical_analysis.html#delimiters"><span class="std std-ref">delimiter</span></a> tokens and <a class="reference internal" href="constants#Ellipsis" title="Ellipsis"><code>Ellipsis</code></a> are returned using the generic <a class="reference internal" href="token#token.OP" title="token.OP"><code>OP</code></a> token type. The exact type can be determined by checking the <code>exact_type</code> property on the <a class="reference internal" href="https://docs.python.org/3.9/glossary.html#term-named-tuple"><span class="xref std std-term">named tuple</span></a> returned from <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize.tokenize()</code></a>.</p>  <h2 id="tokenizing-input">Tokenizing Input</h2> <p>The primary entry point is a <a class="reference internal" href="https://docs.python.org/3.9/glossary.html#term-generator"><span class="xref std std-term">generator</span></a>:</p> <dl class="function"> <dt id="tokenize.tokenize">
<code>tokenize.tokenize(readline)</code> </dt> <dd>
<p>The <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a> generator requires one argument, <em>readline</em>, which must be a callable object which provides the same interface as the <a class="reference internal" href="io#io.IOBase.readline" title="io.IOBase.readline"><code>io.IOBase.readline()</code></a> method of file objects. Each call to the function should return one line of input as bytes.</p> <p>The generator produces 5-tuples with these members: the token type; the token string; a 2-tuple <code>(srow, scol)</code> of ints specifying the row and column where the token begins in the source; a 2-tuple <code>(erow, ecol)</code> of ints specifying the row and column where the token ends in the source; and the line on which the token was found. The line passed (the last tuple item) is the <em>physical</em> line. The 5 tuple is returned as a <a class="reference internal" href="https://docs.python.org/3.9/glossary.html#term-named-tuple"><span class="xref std std-term">named tuple</span></a> with the field names: <code>type string start end line</code>.</p> <p>The returned <a class="reference internal" href="https://docs.python.org/3.9/glossary.html#term-named-tuple"><span class="xref std std-term">named tuple</span></a> has an additional property named <code>exact_type</code> that contains the exact operator type for <a class="reference internal" href="token#token.OP" title="token.OP"><code>OP</code></a> tokens. For all other token types <code>exact_type</code> equals the named tuple <code>type</code> field.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 3.1: </span>Added support for named tuples.</p> </div> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 3.3: </span>Added support for <code>exact_type</code>.</p> </div> <p><a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a> determines the source encoding of the file by looking for a UTF-8 BOM or encoding cookie, according to <a class="pep reference external" href="https://www.python.org/dev/peps/pep-0263" id="index-0"><strong>PEP 263</strong></a>.</p> </dd>
</dl> <dl class="function"> <dt id="tokenize.generate_tokens">
<code>tokenize.generate_tokens(readline)</code> </dt> <dd>
<p>Tokenize a source reading unicode strings instead of bytes.</p> <p>Like <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a>, the <em>readline</em> argument is a callable returning a single line of input. However, <a class="reference internal" href="#tokenize.generate_tokens" title="tokenize.generate_tokens"><code>generate_tokens()</code></a> expects <em>readline</em> to return a str object rather than bytes.</p> <p>The result is an iterator yielding named tuples, exactly like <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a>. It does not yield an <a class="reference internal" href="token#token.ENCODING" title="token.ENCODING"><code>ENCODING</code></a> token.</p> </dd>
</dl> <p>All constants from the <a class="reference internal" href="token#module-token" title="token: Constants representing terminal nodes of the parse tree."><code>token</code></a> module are also exported from <a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code>tokenize</code></a>.</p> <p>Another function is provided to reverse the tokenization process. This is useful for creating tools that tokenize a script, modify the token stream, and write back the modified script.</p> <dl class="function"> <dt id="tokenize.untokenize">
<code>tokenize.untokenize(iterable)</code> </dt> <dd>
<p>Converts tokens back into Python source code. The <em>iterable</em> must return sequences with at least two elements, the token type and the token string. Any additional sequence elements are ignored.</p> <p>The reconstructed script is returned as a single string. The result is guaranteed to tokenize back to match the input so that the conversion is lossless and round-trips are assured. The guarantee applies only to the token type and token string as the spacing between tokens (column positions) may change.</p> <p>It returns bytes, encoded using the <a class="reference internal" href="token#token.ENCODING" title="token.ENCODING"><code>ENCODING</code></a> token, which is the first token sequence output by <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a>. If there is no encoding token in the input, it returns a str instead.</p> </dd>
</dl> <p><a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a> needs to detect the encoding of source files it tokenizes. The function it uses to do this is available:</p> <dl class="function"> <dt id="tokenize.detect_encoding">
<code>tokenize.detect_encoding(readline)</code> </dt> <dd>
<p>The <a class="reference internal" href="#tokenize.detect_encoding" title="tokenize.detect_encoding"><code>detect_encoding()</code></a> function is used to detect the encoding that should be used to decode a Python source file. It requires one argument, readline, in the same way as the <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a> generator.</p> <p>It will call readline a maximum of twice, and return the encoding used (as a string) and a list of any lines (not decoded from bytes) it has read in.</p> <p>It detects the encoding from the presence of a UTF-8 BOM or an encoding cookie as specified in <a class="pep reference external" href="https://www.python.org/dev/peps/pep-0263" id="index-1"><strong>PEP 263</strong></a>. If both a BOM and a cookie are present, but disagree, a <a class="reference internal" href="exceptions#SyntaxError" title="SyntaxError"><code>SyntaxError</code></a> will be raised. Note that if the BOM is found, <code>'utf-8-sig'</code> will be returned as an encoding.</p> <p>If no encoding is specified, then the default of <code>'utf-8'</code> will be returned.</p> <p>Use <a class="reference internal" href="#tokenize.open" title="tokenize.open"><code>open()</code></a> to open Python source files: it uses <a class="reference internal" href="#tokenize.detect_encoding" title="tokenize.detect_encoding"><code>detect_encoding()</code></a> to detect the file encoding.</p> </dd>
</dl> <dl class="function"> <dt id="tokenize.open">
<code>tokenize.open(filename)</code> </dt> <dd>
<p>Open a file in read only mode using the encoding detected by <a class="reference internal" href="#tokenize.detect_encoding" title="tokenize.detect_encoding"><code>detect_encoding()</code></a>.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 3.2.</span></p> </div> </dd>
</dl> <dl class="exception"> <dt id="tokenize.TokenError">
<code>exception tokenize.TokenError</code> </dt> <dd>
<p>Raised when either a docstring or expression that may be split over several lines is not completed anywhere in the file, for example:</p> <pre data-language="python">"""Beginning of
docstring
</pre> <p>or:</p> <pre data-language="python">[1,
 2,
 3
</pre> </dd>
</dl> <p>Note that unclosed single-quoted strings do not cause an error to be raised. They are tokenized as <a class="reference internal" href="token#token.ERRORTOKEN" title="token.ERRORTOKEN"><code>ERRORTOKEN</code></a>, followed by the tokenization of their contents.</p>   <h2 id="tokenize-cli">Command-Line Usage</h2> <div class="versionadded" id="command-line-usage"> <p><span class="versionmodified added">New in version 3.3.</span></p> </div> <p>The <a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code>tokenize</code></a> module can be executed as a script from the command line. It is as simple as:</p> <pre data-language="sh">python -m tokenize [-e] [filename.py]
</pre> <p>The following options are accepted:</p> <dl class="cmdoption"> <dt id="cmdoption-tokenize-h">
<code>-h, --help</code> </dt> <dd>
<p>show this help message and exit</p> </dd>
</dl> <dl class="cmdoption"> <dt id="cmdoption-tokenize-e">
<code>-e, --exact</code> </dt> <dd>
<p>display token names using the exact type</p> </dd>
</dl> <p>If <code>filename.py</code> is specified its contents are tokenized to stdout. Otherwise, tokenization is performed on stdin.</p>   <h2 id="examples">Examples</h2> <p>Example of a script rewriter that transforms float literals into Decimal objects:</p> <pre data-language="python">from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP
from io import BytesIO

def decistmt(s):
    """Substitute Decimals for floats in a string of statements.

    &gt;&gt;&gt; from decimal import Decimal
    &gt;&gt;&gt; s = 'print(+21.3e-5*-.1234/81.7)'
    &gt;&gt;&gt; decistmt(s)
    "print (+Decimal ('21.3e-5')*-Decimal ('.1234')/Decimal ('81.7'))"

    The format of the exponent is inherited from the platform C library.
    Known cases are "e-007" (Windows) and "e-07" (not Windows).  Since
    we're only showing 12 digits, and the 13th isn't close to 5, the
    rest of the output should be platform-independent.

    &gt;&gt;&gt; exec(s)  #doctest: +ELLIPSIS
    -3.21716034272e-0...7

    Output from calculations with Decimal should be identical across all
    platforms.

    &gt;&gt;&gt; exec(decistmt(s))
    -3.217160342717258261933904529E-7
    """
    result = []
    g = tokenize(BytesIO(s.encode('utf-8')).readline)  # tokenize the string
    for toknum, tokval, _, _, _ in g:
        if toknum == NUMBER and '.' in tokval:  # replace NUMBER tokens
            result.extend([
                (NAME, 'Decimal'),
                (OP, '('),
                (STRING, repr(tokval)),
                (OP, ')')
            ])
        else:
            result.append((toknum, tokval))
    return untokenize(result).decode('utf-8')
</pre> <p>Example of tokenizing from the command line. The script:</p> <pre data-language="python">def say_hello():
    print("Hello, World!")

say_hello()
</pre> <p>will be tokenized to the following output where the first column is the range of the line/column coordinates where the token is found, the second column is the name of the token, and the final column is the value of the token (if any)</p> <pre data-language="shell">$ python -m tokenize hello.py
0,0-0,0:            ENCODING       'utf-8'
1,0-1,3:            NAME           'def'
1,4-1,13:           NAME           'say_hello'
1,13-1,14:          OP             '('
1,14-1,15:          OP             ')'
1,15-1,16:          OP             ':'
1,16-1,17:          NEWLINE        '\n'
2,0-2,4:            INDENT         '    '
2,4-2,9:            NAME           'print'
2,9-2,10:           OP             '('
2,10-2,25:          STRING         '"Hello, World!"'
2,25-2,26:          OP             ')'
2,26-2,27:          NEWLINE        '\n'
3,0-3,1:            NL             '\n'
4,0-4,0:            DEDENT         ''
4,0-4,9:            NAME           'say_hello'
4,9-4,10:           OP             '('
4,10-4,11:          OP             ')'
4,11-4,12:          NEWLINE        '\n'
5,0-5,0:            ENDMARKER      ''
</pre> <p>The exact token type names can be displayed using the <a class="reference internal" href="#cmdoption-tokenize-e"><code>-e</code></a> option:</p> <pre data-language="shell">$ python -m tokenize -e hello.py
0,0-0,0:            ENCODING       'utf-8'
1,0-1,3:            NAME           'def'
1,4-1,13:           NAME           'say_hello'
1,13-1,14:          LPAR           '('
1,14-1,15:          RPAR           ')'
1,15-1,16:          COLON          ':'
1,16-1,17:          NEWLINE        '\n'
2,0-2,4:            INDENT         '    '
2,4-2,9:            NAME           'print'
2,9-2,10:           LPAR           '('
2,10-2,25:          STRING         '"Hello, World!"'
2,25-2,26:          RPAR           ')'
2,26-2,27:          NEWLINE        '\n'
3,0-3,1:            NL             '\n'
4,0-4,0:            DEDENT         ''
4,0-4,9:            NAME           'say_hello'
4,9-4,10:           LPAR           '('
4,10-4,11:          RPAR           ')'
4,11-4,12:          NEWLINE        '\n'
5,0-5,0:            ENDMARKER      ''
</pre> <p>Example of tokenizing a file programmatically, reading unicode strings instead of bytes with <a class="reference internal" href="#tokenize.generate_tokens" title="tokenize.generate_tokens"><code>generate_tokens()</code></a>:</p> <pre data-language="python">import tokenize

with tokenize.open('hello.py') as f:
    tokens = tokenize.generate_tokens(f.readline)
    for token in tokens:
        print(token)
</pre> <p>Or reading bytes directly with <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a>:</p> <pre data-language="python">import tokenize

with open('hello.py', 'rb') as f:
    tokens = tokenize.tokenize(f.readline)
    for token in tokens:
        print(token)
</pre>
<div class="_attribution">
  <p class="_attribution-p">
    © 2001–2020 Python Software Foundation<br>Licensed under the PSF License.<br>
    <a href="https://docs.python.org/3.9/library/tokenize.html" class="_attribution-link">https://docs.python.org/3.9/library/tokenize.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
