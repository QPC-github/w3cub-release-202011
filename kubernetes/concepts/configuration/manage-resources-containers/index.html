
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Resource Management for Pods and Containers - Kubernetes - W3cubDocs</title>
  
  <meta name="description" content="When you specify a Pod, you can optionally specify how much of each resource a container needs. The most common resources to specify are CPU and &hellip;">
  <meta name="keywords" content="resource, management, for, pods, and, containers, kubernetes">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/kubernetes/concepts/configuration/manage-resources-containers/">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/kubernetes.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/kubernetes/" class="_nav-link" title="" style="margin-left:0;">Kubernetes</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _kubernetes">
				
				
<h1>Resource Management for Pods and Containers</h1>  <p>When you specify a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="../../workloads/pods/index" target="_blank" aria-label="Pod">Pod</a>, you can optionally specify how much of each resource a <a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." data-toggle="tooltip" data-placement="top" href="../../containers/index" target="_blank" aria-label="container">container</a> needs. The most common resources to specify are CPU and memory (RAM); there are others.</p> <p>When you specify the resource <em>request</em> for containers in a Pod, the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="../../../reference/command-line-tools-reference/kube-scheduler/index" target="_blank" aria-label="kube-scheduler">kube-scheduler</a> uses this information to decide which node to place the Pod on. When you specify a resource <em>limit</em> for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set. The kubelet also reserves at least the <em>request</em> amount of that system resource specifically for that container to use.</p>  <h2 id="requests-and-limits">Requests and limits</h2> <p>If the node where a Pod is running has enough of a resource available, it's possible (and allowed) for a container to use more resource than its <code>request</code> for that resource specifies. However, a container is not allowed to use more than its resource <code>limit</code>.</p> <p>For example, if you set a <code>memory</code> request of 256 MiB for a container, and that container is in a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use more RAM.</p> <p>If you set a <code>memory</code> limit of 4GiB for that container, the kubelet (and <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="../../../setup/production-environment/container-runtimes/index" target="_blank" aria-label="container runtime">container runtime</a>) enforce the limit. The runtime prevents the container from using more than the configured resource limit. For example: when a process in the container tries to consume more than the allowed amount of memory, the system kernel terminates the process that attempted the allocation, with an out of memory (OOM) error.</p> <p>Limits can be implemented either reactively (the system intervenes once it sees a violation) or by enforcement (the system prevents the container from ever exceeding the limit). Different runtimes can have different ways to implement the same restrictions.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> If a container specifies its own memory limit, but does not specify a memory request, Kubernetes automatically assigns a memory request that matches the limit. Similarly, if a container specifies its own CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches the limit. </div> <h2 id="resource-types">Resource types</h2> <p><em>CPU</em> and <em>memory</em> are each a <em>resource type</em>. A resource type has a base unit. CPU represents compute processing and is specified in units of <a href="#meaning-of-cpu">Kubernetes CPUs</a>. Memory is specified in units of bytes. For Linux workloads, you can specify <em>huge page</em> resources. Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory that are much larger than the default page size.</p> <p>For example, on a system where the default page size is 4KiB, you could specify a limit, <code>hugepages-2Mi: 80Mi</code>. If the container tries allocating over 40 2MiB huge pages (a total of 80 MiB), that allocation fails.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> You cannot overcommit <code>hugepages-*</code> resources. This is different from the <code>memory</code> and <code>cpu</code> resources. </div> <p>CPU and memory are collectively referred to as <em>compute resources</em>, or <em>resources</em>. Compute resources are measurable quantities that can be requested, allocated, and consumed. They are distinct from <a href="../../overview/kubernetes-api/index">API resources</a>. API resources, such as Pods and <a href="../../services-networking/service/index">Services</a> are objects that can be read and modified through the Kubernetes API server.</p> <h2 id="resource-requests-and-limits-of-pod-and-container">Resource requests and limits of Pod and container</h2> <p>For each container, you can specify resource limits and requests, including the following:</p> <ul> <li><code>spec.containers[].resources.limits.cpu</code></li> <li><code>spec.containers[].resources.limits.memory</code></li> <li><code>spec.containers[].resources.limits.hugepages-&lt;size&gt;</code></li> <li><code>spec.containers[].resources.requests.cpu</code></li> <li><code>spec.containers[].resources.requests.memory</code></li> <li><code>spec.containers[].resources.requests.hugepages-&lt;size&gt;</code></li> </ul> <p>Although you can only specify requests and limits for individual containers, it is also useful to think about the overall resource requests and limits for a Pod. For a particular resource, a <em>Pod resource request/limit</em> is the sum of the resource requests/limits of that type for each container in the Pod.</p> <h2 id="resource-units-in-kubernetes">Resource units in Kubernetes</h2> <h3 id="meaning-of-cpu">CPU resource units</h3> <p>Limits and requests for CPU resources are measured in <em>cpu</em> units. In Kubernetes, 1 CPU unit is equivalent to <strong>1 physical CPU core</strong>, or <strong>1 virtual core</strong>, depending on whether the node is a physical host or a virtual machine running inside a physical machine.</p> <p>Fractional requests are allowed. When you define a container with <code>spec.containers[].resources.requests.cpu</code> set to <code>0.5</code>, you are requesting half as much CPU time compared to if you asked for <code>1.0</code> CPU. For CPU resource units, the <a href="../../../reference/kubernetes-api/common-definitions/quantity/index">quantity</a> expression <code>0.1</code> is equivalent to the expression <code>100m</code>, which can be read as "one hundred millicpu". Some people say "one hundred millicores", and this is understood to mean the same thing.</p> <p>CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example, <code>500m</code> CPU represents the roughly same amount of computing power whether that container runs on a single-core, dual-core, or 48-core machine.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> Kubernetes doesn't allow you to specify CPU resources with a precision finer than <code>1m</code>. Because of this, it's useful to specify CPU units less than <code>1.0</code> or <code>1000m</code> using the milliCPU form; for example, <code>5m</code> rather than <code>0.005</code>. </div> <h3 id="meaning-of-memory">Memory resource units</h3> <p>Limits and requests for <code>memory</code> are measured in bytes. You can express memory as a plain integer or as a fixed-point number using one of these <a href="../../../reference/kubernetes-api/common-definitions/quantity/index">quantity</a> suffixes: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">128974848, 129e6, 129M,  128974848000m, 123Mi
</pre></div>
<p>Take care about case for suffixes. If you request <code>400m</code> of memory, this is a request for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (<code>400Mi</code>) or 400 megabytes (<code>400M</code>).</p> <h2 id="example-1">Container resources example</h2> <p>The following Pod has two containers. Both containers are defined with a request for 0.25 CPU and 64MiB (2<sup>26</sup> bytes) of memory. Each container has a limit of 0.5 CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128 MiB of memory, and a limit of 1 CPU and 256MiB of memory.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">---
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
</pre></div>
<h2 id="how-pods-with-resource-requests-are-scheduled">How Pods with resource requests are scheduled</h2> <p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum capacity for each of the resource types: the amount of CPU and memory it can provide for Pods. The scheduler ensures that, for each resource type, the sum of the resource requests of the scheduled containers is less than the capacity of the node. Note that although actual memory or CPU resource usage on nodes is very low, the scheduler still refuses to place a Pod on a node if the capacity check fails. This protects against a resource shortage on a node when resource usage later increases, for example, during a daily peak in request rate.</p> <h2 id="how-pods-with-resource-limits-are-run">How Kubernetes applies resource requests and limits</h2> <p>When the kubelet starts a container as part of a Pod, the kubelet passes that container's requests and limits for memory and CPU to the container runtime.</p> <p>On Linux, the container runtime typically configures kernel <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." data-toggle="tooltip" data-placement="top" href="https://kubernetes.io/docs/reference/glossary/?all=true#term-cgroup" target="_blank" aria-label="cgroups">cgroups</a> that apply and enforce the limits you defined.</p> <ul> <li>The CPU limit defines a hard ceiling on how much CPU time that the container can use. During each scheduling interval (time slice), the Linux kernel checks to see if this limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.</li> <li>The CPU request typically defines a weighting. If several different containers (cgroups) want to run on a contended system, workloads with larger CPU requests are allocated more CPU time than workloads with small requests.</li> <li>The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses cgroups v2, the container runtime might use the memory request as a hint to set <code>memory.min</code> and <code>memory.low</code>.</li> <li>The memory limit defines a memory limit for that cgroup. If the container tries to allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates and, typically, intervenes by stopping one of the processes in the container that tried to allocate memory. If that process is the container's PID 1, and the container is marked as restartable, Kubernetes restarts the container.</li> <li>The memory limit for the Pod or container can also apply to pages in memory backed volumes, such as an <code>emptyDir</code>. The kubelet tracks <code>tmpfs</code> emptyDir volumes as container memory use, rather than as local ephemeral storage.</li> </ul> <p>If a container exceeds its memory request and the node that it runs on becomes short of memory overall, it is likely that the Pod the container belongs to will be <a class="glossary-tooltip" title="Process of terminating one or more Pods on Nodes" data-toggle="tooltip" data-placement="top" href="../../scheduling-eviction/index" target="_blank" aria-label="evicted">evicted</a>.</p> <p>A container might or might not be allowed to exceed its CPU limit for extended periods of time. However, container runtimes don't terminate Pods or containers for excessive CPU usage.</p> <p>To determine whether a container cannot be scheduled or is being killed due to resource limits, see the <a href="#troubleshooting">Troubleshooting</a> section.</p> <h3 id="monitoring-compute-memory-resource-usage">Monitoring compute &amp; memory resource usage</h3> <p>The kubelet reports the resource usage of a Pod as part of the Pod <a href="../../overview/working-with-objects/kubernetes-objects/index#object-spec-and-status"><code>status</code></a>.</p> <p>If optional <a href="../../../tasks/debug-application-cluster/resource-usage-monitoring/index">tools for monitoring</a> are available in your cluster, then Pod resource usage can be retrieved either from the <a href="../../../tasks/debug-application-cluster/resource-metrics-pipeline/index#the-metrics-api">Metrics API</a> directly or from your monitoring tools.</p> <h2 id="local-ephemeral-storage">Local ephemeral storage</h2>  <div style="margin-top: 10px; margin-bottom: 10px;"> <b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code> </div> <p>Nodes have local ephemeral storage, backed by locally-attached writeable devices or, sometimes, by RAM. "Ephemeral" means that there is no long-term guarantee about durability.</p> <p>Pods use ephemeral local storage for scratch space, caching, and for logs. The kubelet can provide scratch space to Pods using local ephemeral storage to mount <a href="../../storage/volumes/index#emptydir"><code>emptyDir</code></a> <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="../../storage/volumes/index" target="_blank" aria-label="volumes">volumes</a> into containers.</p> <p>The kubelet also uses this kind of storage to hold <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>, container images, and the writable layers of running containers.</p> <div class="alert alert-warning caution callout" role="alert"> <strong>Caution:</strong> If a node fails, the data in its ephemeral storage can be lost. Your applications cannot expect any performance SLAs (disk IOPS for example) from local ephemeral storage. </div> <p>As a beta feature, Kubernetes lets you track, reserve and limit the amount of ephemeral local storage a Pod can consume.</p> <h3 id="configurations-for-local-ephemeral-storage">Configurations for local ephemeral storage</h3> <p>Kubernetes supports two ways to configure local ephemeral storage on a node: </p>
<ul class="nav nav-tabs" id="local-storage-configurations" role="tablist">
<li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#local-storage-configurations-0" role="tab" aria-controls="local-storage-configurations-0" aria-selected="true">Single filesystem</a></li> <li class="nav-item"><a data-toggle="tab" class="nav-link" href="#local-storage-configurations-1" role="tab" aria-controls="local-storage-configurations-1">Two filesystems</a></li>
</ul> <div class="tab-content" id="local-storage-configurations">
<div id="local-storage-configurations-0" class="tab-pane show active" role="tabpanel" aria-labelledby="local-storage-configurations-0"> 
<p>In this configuration, you place all different kinds of ephemeral local data (<code>emptyDir</code> volumes, writeable layers, container images, logs) into one filesystem. The most effective way to configure the kubelet means dedicating this filesystem to Kubernetes (kubelet) data.</p> <p>The kubelet also writes <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a> and treats these similarly to ephemeral local storage.</p> <p>The kubelet writes logs to files inside its configured log directory (<code>/var/log</code> by default); and has a base directory for other locally stored data (<code>/var/lib/kubelet</code> by default).</p> <p>Typically, both <code>/var/lib/kubelet</code> and <code>/var/log</code> are on the system root filesystem, and the kubelet is designed with that layout in mind.</p> <p>Your node can have as many other filesystems, not used for Kubernetes, as you like.</p> </div> <div id="local-storage-configurations-1" class="tab-pane" role="tabpanel" aria-labelledby="local-storage-configurations-1"> 
<p>You have a filesystem on the node that you're using for ephemeral data that comes from running Pods: logs, and <code>emptyDir</code> volumes. You can use this filesystem for other data (for example: system logs not related to Kubernetes); it can even be the root filesystem.</p> <p>The kubelet also writes <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a> into the first filesystem, and treats these similarly to ephemeral local storage.</p> <p>You also use a separate filesystem, backed by a different logical storage device. In this configuration, the directory where you tell the kubelet to place container image layers and writeable layers is on this second filesystem.</p> <p>The first filesystem does not hold any image layers or writeable layers.</p> <p>Your node can have as many other filesystems, not used for Kubernetes, as you like.</p> </div>
</div> <p>The kubelet can measure how much local storage it is using. It does this provided that:</p> <ul> <li>the <code>LocalStorageCapacityIsolation</code> <a href="../../../reference/command-line-tools-reference/feature-gates/index">feature gate</a> is enabled (the feature is on by default), and</li> <li>you have set up the node using one of the supported configurations for local ephemeral storage.</li> </ul> <p>If you have a different configuration, then the kubelet does not apply resource limits for ephemeral local storage.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> The kubelet tracks <code>tmpfs</code> emptyDir volumes as container memory use, rather than as local ephemeral storage. </div> <h3 id="setting-requests-and-limits-for-local-ephemeral-storage">Setting requests and limits for local ephemeral storage</h3> <p>You can specify <code>ephemeral-storage</code> for managing local ephemeral storage. Each container of a Pod can specify either or both of the following:</p> <ul> <li><code>spec.containers[].resources.limits.ephemeral-storage</code></li> <li><code>spec.containers[].resources.requests.ephemeral-storage</code></li> </ul> <p>Limits and requests for <code>ephemeral-storage</code> are measured in byte quantities. You can express storage as a plain integer or as a fixed-point number using one of these suffixes: E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following quantities all represent roughly the same value:</p> <ul> <li><code>128974848</code></li> <li><code>129e6</code></li> <li><code>129M</code></li> <li><code>123Mi</code></li> </ul> <p>In the following example, the Pod has two containers. Each container has a request of 2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and a limit of 8GiB of local ephemeral storage.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        ephemeral-storage: "2Gi"
      limits:
        ephemeral-storage: "4Gi"
    volumeMounts:
    - name: ephemeral
      mountPath: "/tmp"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        ephemeral-storage: "2Gi"
      limits:
        ephemeral-storage: "4Gi"
    volumeMounts:
    - name: ephemeral
      mountPath: "/tmp"
  volumes:
    - name: ephemeral
      emptyDir: {}
</pre></div>
<h3 id="how-pods-with-ephemeral-storage-requests-are-scheduled">How Pods with ephemeral-storage requests are scheduled</h3> <p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods. For more information, see <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">Node Allocatable</a>.</p> <p>The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.</p> <h3 id="resource-emphemeralstorage-consumption">Ephemeral storage consumption management</h3> <p>If the kubelet is managing local ephemeral storage as a resource, then the kubelet measures storage use in:</p> <ul> <li>
<code>emptyDir</code> volumes, except <em>tmpfs</em> <code>emptyDir</code> volumes</li> <li>directories holding node-level logs</li> <li>writeable container layers</li> </ul> <p>If a Pod is using more ephemeral storage than you allow it to, the kubelet sets an eviction signal that triggers Pod eviction.</p> <p>For container-level isolation, if a container's writable layer and log usage exceeds its storage limit, the kubelet marks the Pod for eviction.</p> <p>For pod-level isolation the kubelet works out an overall Pod storage limit by summing the limits for the containers in that Pod. In this case, if the sum of the local ephemeral storage usage from all containers and also the Pod's <code>emptyDir</code> volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod for eviction.</p> <div class="alert alert-warning caution callout" role="alert"> <strong>Caution:</strong> <p>If the kubelet is not measuring local ephemeral storage, then a Pod that exceeds its local storage limit will not be evicted for breaching local storage resource limits.</p> <p>However, if the filesystem space for writeable container layers, node-level logs, or <code>emptyDir</code> volumes falls low, the node <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." data-toggle="tooltip" data-placement="top" href="../../scheduling-eviction/taint-and-toleration/index" target="_blank" aria-label="taints">taints</a> itself as short on local storage and this taint triggers eviction for any Pods that don't specifically tolerate the taint.</p> <p>See the supported <a href="#configurations-for-local-ephemeral-storage">configurations</a> for ephemeral local storage.</p> </div> <p>The kubelet supports different ways to measure Pod storage use:</p> <ul class="nav nav-tabs" id="resource-emphemeralstorage-measurement" role="tablist">
<li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#resource-emphemeralstorage-measurement-0" role="tab" aria-controls="resource-emphemeralstorage-measurement-0" aria-selected="true">Periodic scanning</a></li> <li class="nav-item"><a data-toggle="tab" class="nav-link" href="#resource-emphemeralstorage-measurement-1" role="tab" aria-controls="resource-emphemeralstorage-measurement-1">Filesystem project quota</a></li>
</ul> <div class="tab-content" id="resource-emphemeralstorage-measurement">
<div id="resource-emphemeralstorage-measurement-0" class="tab-pane show active" role="tabpanel" aria-labelledby="resource-emphemeralstorage-measurement-0"> 
<p>The kubelet performs regular, scheduled checks that scan each <code>emptyDir</code> volume, container log directory, and writeable container layer.</p> <p>The scan measures how much space is used.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> <p>In this mode, the kubelet does not track open file descriptors for deleted files.</p> <p>If you (or a container) create a file inside an <code>emptyDir</code> volume, something then opens that file, and you delete the file while it is still open, then the inode for the deleted file stays until you close that file but the kubelet does not categorize the space as in use.</p> </div> </div> <div id="resource-emphemeralstorage-measurement-1" class="tab-pane" role="tabpanel" aria-labelledby="resource-emphemeralstorage-measurement-1"> 
<div style="margin-top: 10px; margin-bottom: 10px;"> <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [alpha]</code> </div> <p>Project quotas are an operating-system level feature for managing storage use on filesystems. With Kubernetes, you can enable project quotas for monitoring storage use. Make sure that the filesystem backing the <code>emptyDir</code> volumes, on the node, provides project quota support. For example, XFS and ext4fs offer project quotas.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> Project quotas let you monitor storage use; they do not enforce limits. </div> <p>Kubernetes uses project IDs starting from <code>1048576</code>. The IDs in use are registered in <code>/etc/projects</code> and <code>/etc/projid</code>. If project IDs in this range are used for other purposes on the system, those project IDs must be registered in <code>/etc/projects</code> and <code>/etc/projid</code> so that Kubernetes does not use them.</p> <p>Quotas are faster and more accurate than directory scanning. When a directory is assigned to a project, all files created under a directory are created in that project, and the kernel merely has to keep track of how many blocks are in use by files in that project. If a file is created and deleted, but has an open file descriptor, it continues to consume space. Quota tracking records that space accurately whereas directory scans overlook the storage used by deleted files.</p> <p>If you want to use project quotas, you should:</p> <ul> <li> <p>Enable the <code>LocalStorageCapacityIsolationFSQuotaMonitoring=true</code> <a href="../../../reference/command-line-tools-reference/feature-gates/index">feature gate</a> using the <code>featureGates</code> field in the <a href="../../../reference/config-api/kubelet-config.v1beta1/index">kubelet configuration</a> or the <code>--feature-gates</code> command line flag.</p> </li> <li> <p>Ensure that the root filesystem (or optional runtime filesystem) has project quotas enabled. All XFS filesystems support project quotas. For ext4 filesystems, you need to enable the project quota tracking feature while the filesystem is not mounted.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="bash"># For ext4, with /dev/block-device not mounted
sudo tune2fs -O project -Q prjquota /dev/block-device
</pre></div>
</li> <li> <p>Ensure that the root filesystem (or optional runtime filesystem) is mounted with project quotas enabled. For both XFS and ext4fs, the mount option is named <code>prjquota</code>.</p> </li> </ul> </div>
</div> <h2 id="extended-resources">Extended resources</h2> <p>Extended resources are fully-qualified resource names outside the <code>kubernetes.io</code> domain. They allow cluster operators to advertise and users to consume the non-Kubernetes-built-in resources.</p> <p>There are two steps required to use Extended Resources. First, the cluster operator must advertise an Extended Resource. Second, users must request the Extended Resource in Pods.</p> <h3 id="managing-extended-resources">Managing extended resources</h3> <h4 id="node-level-extended-resources">Node-level extended resources</h4> <p>Node-level extended resources are tied to nodes.</p> <h5 id="device-plugin-managed-resources">Device plugin managed resources</h5> <p>See <a href="../../extend-kubernetes/compute-storage-net/device-plugins/index">Device Plugin</a> for how to advertise device plugin managed resources on each node.</p> <h5 id="other-resources">Other resources</h5> <p>To advertise a new node-level extended resource, the cluster operator can submit a <code>PATCH</code> HTTP request to the API server to specify the available quantity in the <code>status.capacity</code> for a node in the cluster. After this operation, the node's <code>status.capacity</code> will include a new resource. The <code>status.allocatable</code> field is updated automatically with the new resource asynchronously by the kubelet.</p> <p>Because the scheduler uses the node's <code>status.allocatable</code> value when evaluating Pod fitness, the scheduler only takes account of the new value after that asynchronous update. There may be a short delay between patching the node capacity with a new resource and the time when the first Pod that requests the resource can be scheduled on that node.</p> <p><strong>Example:</strong></p> <p>Here is an example showing how to use <code>curl</code> to form an HTTP request that advertises five "example.com/foo" resources on node <code>k8s-node-1</code> whose master is <code>k8s-master</code>.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]' \
http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
</pre></div>
<div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> In the preceding request, <code>~1</code> is the encoding for the character <code>/</code> in the patch path. The operation path value in JSON-Patch is interpreted as a JSON-Pointer. For more details, see <a href="https://tools.ietf.org/html/rfc6901#section-3">IETF RFC 6901, section 3</a>. </div> <h4 id="cluster-level-extended-resources">Cluster-level extended resources</h4> <p>Cluster-level extended resources are not tied to nodes. They are usually managed by scheduler extenders, which handle the resource consumption and resource quota.</p> <p>You can specify the extended resources that are handled by scheduler extenders in <a href="../../../reference/config-api/kube-scheduler-config.v1beta3/index">scheduler configuration</a></p> <p><strong>Example:</strong></p> <p>The following configuration for a scheduler policy indicates that the cluster-level extended resource "example.com/foo" is handled by the scheduler extender.</p> <ul> <li>The scheduler sends a Pod to the scheduler extender only if the Pod requests "example.com/foo".</li> <li>The <code>ignoredByScheduler</code> field specifies that the scheduler does not check the "example.com/foo" resource in its <code>PodFitsResources</code> predicate.</li> </ul> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="json">{
  "kind": "Policy",
  "apiVersion": "v1",
  "extenders": [
    {
      "urlPrefix":"&lt;extender-endpoint&gt;",
      "bindVerb": "bind",
      "managedResources": [
        {
          "name": "example.com/foo",
          "ignoredByScheduler": true
        }
      ]
    }
  ]
}
</pre></div>
<h3 id="consuming-extended-resources">Consuming extended resources</h3> <p>Users can consume extended resources in Pod specs like CPU and memory. The scheduler takes care of the resource accounting so that no more than the available amount is simultaneously allocated to Pods.</p> <p>The API server restricts quantities of extended resources to whole numbers. Examples of <em>valid</em> quantities are <code>3</code>, <code>3000m</code> and <code>3Ki</code>. Examples of <em>invalid</em> quantities are <code>0.5</code> and <code>1500m</code>.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> Extended resources replace Opaque Integer Resources. Users can use any domain name prefix other than <code>kubernetes.io</code> which is reserved. </div> <p>To consume an extended resource in a Pod, include the resource name as a key in the <code>spec.containers[].resources.limits</code> map in the container spec.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> Extended resources cannot be overcommitted, so request and limit must be equal if both are present in a container spec. </div> <p>A Pod is scheduled only if all of the resource requests are satisfied, including CPU, memory and any extended resources. The Pod remains in the <code>PENDING</code> state as long as the resource request cannot be satisfied.</p> <p><strong>Example:</strong></p> <p>The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: myimage
    resources:
      requests:
        cpu: 2
        example.com/foo: 1
      limits:
        example.com/foo: 1
</pre></div>
<h2 id="pid-limiting">PID limiting</h2> <p>Process ID (PID) limits allow for the configuration of a kubelet to limit the number of PIDs that a given Pod can consume. See <a href="../../policy/pid-limiting/index">PID Limiting</a> for information.</p> <h2 id="troubleshooting">Troubleshooting</h2> <h3 id="my-pods-are-pending-with-event-message-failedscheduling">My Pods are pending with event message <code>FailedScheduling</code>
</h3> <p>If the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. An <a href="../../../reference/kubernetes-api/cluster-resources/event-v1/index">Event</a> is produced each time the scheduler fails to find a place for the Pod. You can use <code>kubectl</code> to view the events for a Pod; for example:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl describe pod frontend | grep -A 9999999999 Events
</pre></div>
<pre><code>Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu
</code></pre>
<p>In the preceding example, the Pod named "frontend" fails to be scheduled due to insufficient CPU resource on any node. Similar error messages can also suggest failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod is pending with a message of this type, there are several things to try:</p> <ul> <li>Add more nodes to the cluster.</li> <li>Terminate unneeded Pods to make room for pending Pods.</li> <li>Check that the Pod is not larger than all the nodes. For example, if all the nodes have a capacity of <code>cpu: 1</code>, then a Pod with a request of <code>cpu: 1.1</code> will never be scheduled.</li> <li>Check for node taints. If most of your nodes are tainted, and the new Pod does not tolerate that taint, the scheduler only considers placements onto the remaining nodes that don't have that taint.</li> </ul> <p>You can check node capacities and amounts allocated with the <code>kubectl describe nodes</code> command. For example:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl describe nodes e2e-test-node-pool-4lw4
</pre></div>
<pre><code>Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
</code></pre>
<p>In the preceding output, you can see that if a Pod requests more than 1.120 CPUs, or more than 6.23Gi of memory, that Pod will not fit on the node.</p> <p>By looking at the “Pods” section, you can see which Pods are taking up space on the node.</p> <p>The amount of resources available to Pods is less than the node capacity, because system daemons use a portion of the available resources. Within the Kubernetes API, each Node has a <code>.status.allocatable</code> field (see <a href="../../../reference/kubernetes-api/cluster-resources/node-v1/index#NodeStatus">NodeStatus</a> for details).</p> <p>The <code>.status.allocatable</code> field describes the amount of resources that are available to Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory). For more information on node allocatable resources in Kubernetes, see <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons</a>.</p> <p>You can configure <a href="../../policy/resource-quotas/index">resource quotas</a> to limit the total amount of resources that a namespace can consume. Kubernetes enforces quotas for objects in particular namespace when there is a ResourceQuota in that namespace. For example, if you assign specific namespaces to different teams, you can add ResourceQuotas into those namespaces. Setting resource quotas helps to prevent one team from using so much of any resource that this over-use affects other teams.</p> <p>You should also consider what access you grant to that namespace: <strong>full</strong> write access to a namespace allows someone with that access to remove any resource, include a configured ResourceQuota.</p> <h3 id="my-container-is-terminated">My container is terminated</h3> <p>Your container might get terminated because it is resource-starved. To check whether a container is being killed because it is hitting a resource limit, call <code>kubectl describe pod</code> on the Pod of interest:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl describe pod simmemleak-hra99
</pre></div>
<p>The output is similar to:</p> <pre><code>Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak:latest
    Limits:
      cpu:          100m
      memory:       50Mi
    State:          Running
      Started:      Tue, 07 Jul 2019 12:54:41 -0700
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Fri, 07 Jul 2019 12:54:30 -0700
      Finished:     Fri, 07 Jul 2019 12:54:33 -0700
    Ready:          False
    Restart Count:  5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image "saadali/simmemleak:latest" already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod
</code></pre>
<p>In the preceding example, the <code>Restart Count:  5</code> indicates that the <code>simmemleak</code> container in the Pod was terminated and restarted five times (so far). The <code>OOMKilled</code> reason shows that the container tried to use more memory than its limit.</p> <p>Your next step might be to check the application code for a memory leak. If you find that the application is behaving how you expect, consider setting a higher memory limit (and possibly request) for that container.</p> <h2 id="what-s-next">What's next</h2> <ul> <li>Get hands-on experience <a href="../../../tasks/configure-pod-container/assign-memory-resource/index">assigning Memory resources to containers and Pods</a>.</li> <li>Get hands-on experience <a href="../../../tasks/configure-pod-container/assign-cpu-resource/index">assigning CPU resources to containers and Pods</a>.</li> <li>Read how the API reference defines a <a href="../../../reference/kubernetes-api/workload-resources/pod-v1/index#Container">container</a> and its <a href="../../../reference/kubernetes-api/workload-resources/pod-v1/index#resources">resource requirements</a>
</li> <li>Read about <a href="https://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/en-US/html/xfs-quotas.html">project quotas</a> in XFS</li> <li>Read more about the <a href="../../../reference/config-api/kube-scheduler-config.v1beta3/index">kube-scheduler configuration reference (v1beta3)</a>
</li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    © 2022 The Kubernetes Authors<br>Documentation Distributed under CC BY 4.0.<br>
    <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/" class="_attribution-link">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
