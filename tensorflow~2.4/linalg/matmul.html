
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.linalg.matmul - TensorFlow 2.4 - W3cubDocs</title>
  
  <meta name="description" content=" Multiplies matrix a by matrix b, producing a &#42; b. ">
  <meta name="keywords" content="tf, linalg, matmul, tensorflow, tensorflow~2.4">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~2.4/linalg/matmul.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-9352e6910fe51dd10a2c4166dc5a7847892ebc84c0bb52d7e40dcdccbf262592790483d09e6dbfd62c689609a706227727ca9acb967a718e944c111f43ca96a0.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~2.4.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~2.4/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 2.4</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.linalg.matmul</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L3123-L3315">  View source on GitHub </a> </td> </table> <p>Multiplies matrix <code translate="no" dir="ltr">a</code> by matrix <code translate="no" dir="ltr">b</code>, producing <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code>.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Main aliases</b> </p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul"><code translate="no" dir="ltr">tf.matmul</code></a></p> <b>Compat aliases for migration</b> <p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul"><code translate="no" dir="ltr">tf.compat.v1.linalg.matmul</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul"><code translate="no" dir="ltr">tf.compat.v1.matmul</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.linalg.matmul(
    a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False,
    a_is_sparse=False, b_is_sparse=False, name=None
)
</pre>  <p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication dimensions, and any further outer dimensions specify matching batch size.</p> <p>Both matrices must be of the same type. The supported types are: <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code>.</p> <p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code translate="no" dir="ltr">True</code>. These are <code translate="no" dir="ltr">False</code> by default.</p> <p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code translate="no" dir="ltr">a_is_sparse</code> or <code translate="no" dir="ltr">b_is_sparse</code> flag to <code translate="no" dir="ltr">True</code>. These are <code translate="no" dir="ltr">False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code translate="no" dir="ltr">bfloat16</code> or <code translate="no" dir="ltr">float32</code>.</p> <p>A simple 2-D tensor matrix multiplication:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
a  # 2-D tensor
&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)&gt;
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
b  # 2-D tensor
&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)&gt;
c = tf.matmul(a, b)
c  # `a` * `b`
&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)&gt;
</pre> <p>A batch matrix multiplication with batch shape [2]:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])
a  # 3-D tensor
&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=
array([[[ 1,  2,  3],
        [ 4,  5,  6]],
       [[ 7,  8,  9],
        [10, 11, 12]]], dtype=int32)&gt;
b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])
b  # 3-D tensor
&lt;tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=
array([[[13, 14],
        [15, 16],
        [17, 18]],
       [[19, 20],
        [21, 22],
        [23, 24]]], dtype=int32)&gt;
c = tf.matmul(a, b)
c  # `a` * `b`
&lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=
array([[[ 94, 100],
        [229, 244]],
       [[508, 532],
        [697, 730]]], dtype=int32)&gt;
</pre> <p>Since python &gt;= 3.5 the @ operator is supported (see <a href="https://www.python.org/dev/peps/pep-0465/">PEP 465</a>). In TensorFlow, it simply calls the <a href="matmul"><code translate="no" dir="ltr">tf.matmul()</code></a> function, so the following lines are equivalent:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
d = a @ b @ [[10], [11]]
d = tf.matmul(tf.matmul(a, b), [[10], [11]])
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">a</code> </td> <td> <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> of type <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code> and rank &gt; 1. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">b</code> </td> <td> <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> with same type and rank as <code translate="no" dir="ltr">a</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">transpose_a</code> </td> <td> If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is transposed before multiplication. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">transpose_b</code> </td> <td> If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is transposed before multiplication. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">adjoint_a</code> </td> <td> If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is conjugated and transposed before multiplication. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">adjoint_b</code> </td> <td> If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is conjugated and transposed before multiplication. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">a_is_sparse</code> </td> <td> If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is treated as a sparse matrix. Notice, this <strong>does not support <a href="../sparse/sparsetensor"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a></strong>, it just makes optimizations that assume most values in <code translate="no" dir="ltr">a</code> are zero. See <a href="../sparse/sparse_dense_matmul"><code translate="no" dir="ltr">tf.sparse.sparse_dense_matmul</code></a> for some support for <a href="../sparse/sparsetensor"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a> multiplication. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">b_is_sparse</code> </td> <td> If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is treated as a sparse matrix. Notice, this <strong>does not support <a href="../sparse/sparsetensor"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a></strong>, it just makes optimizations that assume most values in <code translate="no" dir="ltr">a</code> are zero. See <a href="../sparse/sparse_dense_matmul"><code translate="no" dir="ltr">tf.sparse.sparse_dense_matmul</code></a> for some support for <a href="../sparse/sparsetensor"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a> multiplication. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> of the same type as <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code> where each inner-most matrix is the product of the corresponding matrices in <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code>, e.g. if all transpose or adjoint attributes are <code translate="no" dir="ltr">False</code>: <p><code translate="no" dir="ltr">output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])</code>, for all indices <code translate="no" dir="ltr">i</code>, <code translate="no" dir="ltr">j</code>. </p>
</td> </tr> <tr> <td> <code translate="no" dir="ltr">Note</code> </td> <td> This is matrix product, not element-wise product. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If <code translate="no" dir="ltr">transpose_a</code> and <code translate="no" dir="ltr">adjoint_a</code>, or <code translate="no" dir="ltr">transpose_b</code> and <code translate="no" dir="ltr">adjoint_b</code> are both set to <code translate="no" dir="ltr">True</code>. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    © 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/linalg/matmul" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/linalg/matmul</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
