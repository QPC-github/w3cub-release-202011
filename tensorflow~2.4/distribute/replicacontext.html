
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.distribute.ReplicaContext - TensorFlow 2.4 - W3cubDocs</title>
  
  <meta name="description" content=" A class with a collection of APIs that can be called in a replica context. ">
  <meta name="keywords" content="tf, distribute, replicacontext, tensorflow, tensorflow~2.4">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~2.4/distribute/replicacontext.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-9352e6910fe51dd10a2c4166dc5a7847892ebc84c0bb52d7e40dcdccbf262592790483d09e6dbfd62c689609a706227727ca9acb967a718e944c111f43ca96a0.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~2.4.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~2.4/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 2.4</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.distribute.ReplicaContext</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L3108-L3268">  View source on GitHub </a> </td> </table> <p>A class with a collection of APIs that can be called in a replica context.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.distribute.ReplicaContext(
    strategy, replica_id_in_sync_group
)
</pre>  <p>You can use <a href="get_replica_context"><code translate="no" dir="ltr">tf.distribute.get_replica_context</code></a> to get an instance of <code translate="no" dir="ltr">ReplicaContext</code>, which can only be called inside the function passed to <a href="strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])
def func():
  replica_context = tf.distribute.get_replica_context()
  return replica_context.replica_id_in_sync_group
strategy.run(func)
PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;
}
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">strategy</code> </td> <td> A <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">replica_id_in_sync_group</code> </td> <td> An integer, a <code translate="no" dir="ltr">Tensor</code> or None. Prefer an integer whenever possible to avoid issues with nested <a href="../function"><code translate="no" dir="ltr">tf.function</code></a>. It accepts a <code translate="no" dir="ltr">Tensor</code> only to be compatible with <code translate="no" dir="ltr">tpu.replicate</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">devices</code> </td> <td> Returns the devices this replica is to be executed on, as a tuple of strings. (deprecated) <aside class="warning"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please avoid relying on devices property.</span></aside><blockquote class="note">
<strong>Note:</strong><span> For <a href="mirroredstrategy"><code translate="no" dir="ltr">tf.distribute.MirroredStrategy</code></a> and <a href="experimental/multiworkermirroredstrategy"><code translate="no" dir="ltr">tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a>, this returns a nested list of device strings, e.g, [["GPU:0"]]. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_replicas_in_sync</code> </td> <td> Returns number of replicas that are kept in sync. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">replica_id_in_sync_group</code> </td> <td> Returns the id of the replica. <p>This identifies the replica among all replicas that are kept in sync. The value of the replica id can range from 0 to <a href="replicacontext#num_replicas_in_sync"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.num_replicas_in_sync</code></a> - 1.</p> <blockquote class="note">
<strong>Note:</strong><span> This is not guaranteed to be the same ID as the XLA replica ID use for low-level operations such as collective_permute. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">strategy</code> </td> <td> The current <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="all_gather" data-text="all_gather"><code translate="no" dir="ltr">all_gather</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L3112-L3268">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
all_gather(
    value, axis, options=None
)
</pre> <p>All-gathers <code translate="no" dir="ltr">value</code> across all replicas along <code translate="no" dir="ltr">axis</code>.</p> <blockquote class="note">
<strong>Note:</strong><span> An <code translate="no" dir="ltr">all_gather</code> method can only be called in replica context. For a cross-replica context counterpart, see <a href="strategy#gather"><code translate="no" dir="ltr">tf.distribute.Strategy.gather</code></a>. All replicas need to participate in the all-gather, otherwise this operation hangs. So if <code translate="no" dir="ltr">all_gather</code> is called in any replica, it must be called in all replicas.</span>
</blockquote>
<blockquote class="note">
<strong>Note:</strong><span> If there are multiple <code translate="no" dir="ltr">all_gather</code> calls, they need to be executed in the same order on all replicas. Dispatching <code translate="no" dir="ltr">all_gather</code> based on conditions is usually error-prone.</span>
</blockquote> <p>For all strategies except <a href="tpustrategy"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>, the input <code translate="no" dir="ltr">value</code> on different replicas must have the same rank, and their shapes must be the same in all dimensions except the <code translate="no" dir="ltr">axis</code>-th dimension. In other words, their shapes cannot be different in a dimension <code translate="no" dir="ltr">d</code> where <code translate="no" dir="ltr">d</code> does not equal to the <code translate="no" dir="ltr">axis</code> argument. For example, given a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> with component tensors of shape <code translate="no" dir="ltr">(1, 2, 3)</code> and <code translate="no" dir="ltr">(1, 3, 3)</code> on two replicas, you can call <code translate="no" dir="ltr">all_gather(..., axis=1, ...)</code> on it, but not <code translate="no" dir="ltr">all_gather(..., axis=0, ...)</code> or <code translate="no" dir="ltr">all_gather(..., axis=2, ...)</code>. However, with <a href="tpustrategy"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>, all tensors must have exactly the same rank and same shape.</p> <blockquote class="note">
<strong>Note:</strong><span> The input <code translate="no" dir="ltr">value</code> must have a non-zero rank. Otherwise, consider using <a href="../expand_dims"><code translate="no" dir="ltr">tf.expand_dims</code></a> before gathering them.</span>
</blockquote> <p>You can pass in a single tensor to all-gather:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
@tf.function
def gather_value():
  ctx = tf.distribute.get_replica_context()
  local_value = tf.constant([1, 2, 3])
  return ctx.all_gather(local_value, axis=0)
result = strategy.run(gather_value)
result
PerReplica:{
  0: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;
}
strategy.experimental_local_results(result)
(&lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3],
dtype=int32)&gt;,
&lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3],
dtype=int32)&gt;)
</pre> <p>You can also pass in a nested structure of tensors to all-gather, say, a list:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
@tf.function
def gather_nest():
  ctx = tf.distribute.get_replica_context()
  value_1 = tf.constant([1, 2, 3])
  value_2 = tf.constant([[1, 2], [3, 4]])
  # all_gather a nest of `tf.distribute.DistributedValues`
  return ctx.all_gather([value_1, value_2], axis=0)
result = strategy.run(gather_nest)
result
[PerReplica:{
  0: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;
}, PerReplica:{
  0: &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]], dtype=int32)&gt;
}]
strategy.experimental_local_results(result)
([PerReplica:{
  0: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;
}, PerReplica:{
  0: &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]], dtype=int32)&gt;
}],)
</pre> <p>What if you are all-gathering tensors with different shapes on different replicas? Consider the following example with two replicas, where you have <code translate="no" dir="ltr">value</code> as a nested structure consisting of two items to all-gather, <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code>.</p> <p>On Replica 0, <code translate="no" dir="ltr">value</code> is {'a': [0], 'b': [[0, 1]]} On Replica 1, <code translate="no" dir="ltr">value</code> is {'a': [1], 'b': [[2, 3], [4, 5]]}</p> <p>Result for <code translate="no" dir="ltr">all_gather</code> with <code translate="no" dir="ltr">axis</code>=0: (on each of the replicas): {'a': [1, 2], 'b': [[0, 1], [2, 3], [4, 5]]}</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> a nested structure of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> which <a href="../nest/flatten"><code translate="no" dir="ltr">tf.nest.flatten</code></a> accepts, or a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> instance. The structure of the <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> need to be same on all replicas. The underlying tensor constructs can only be dense tensors with non-zero rank, NOT <a href="../indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">axis</code> </td> <td> 0-D int32 Tensor. Dimension along which to gather. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A nested structure of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> with the gathered values. The structure is the same as <code translate="no" dir="ltr">value</code>. </td> </tr> 
</table> <h3 id="all_reduce" data-text="all_reduce"><code translate="no" dir="ltr">all_reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L3009-L3094">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
all_reduce(
    reduce_op, value, options=None
)
</pre> <p>All-reduces <code translate="no" dir="ltr">value</code> across all replicas.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
def step_fn():
  ctx = tf.distribute.get_replica_context()
  value = tf.identity(1.)
  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, value)
strategy.experimental_local_results(strategy.run(step_fn))
(&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;)
</pre> <p>It supports batched operations. You can pass a list of values and it attempts to batch them when possible. You can also specify <code translate="no" dir="ltr">options</code> to indicate the desired batching behavior, e.g. batch the values into multiple packs so that they can better overlap with computations.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
def step_fn():
  ctx = tf.distribute.get_replica_context()
  value1 = tf.identity(1.)
  value2 = tf.identity(2.)
  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, [value1, value2])
strategy.experimental_local_results(strategy.run(step_fn))
([PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;
}, PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;
}],)
</pre> <p>Note that all replicas need to participate in the all-reduce, otherwise this operation hangs. Note that if there're multiple all-reduces, they need to execute in the same order on all replicas. Dispatching all-reduce based on conditions is usually error-prone.</p> <p>This API currently can only be called in the replica context. Other variants to reduce values across replicas are:</p> <ul> <li>
<a href="strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a>: the reduce and all-reduce API in the cross-replica context.</li> <li>
<a href="strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>: the batched reduce and all-reduce API in the cross-replica context.</li> <li>
<a href="strategy#reduce"><code translate="no" dir="ltr">tf.distribute.Strategy.reduce</code></a>: a more convenient method to reduce to the host in cross-replica context.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined. Allows using string representation of the enum such as "SUM", "MEAN". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> a nested structure of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> which <a href="../nest/flatten"><code translate="no" dir="ltr">tf.nest.flatten</code></a> accepts. The structure and the shapes of the <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> need to be same on all replicas. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A nested structure of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> with the reduced values. The structure is the same as <code translate="no" dir="ltr">value</code>. </td> </tr> 
</table> <h3 id="merge_call" data-text="merge_call"><code translate="no" dir="ltr">merge_call</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2908-L2941">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
merge_call(
    merge_fn, args=(), kwargs=None
)
</pre> <p>Merge args across replicas and run <code translate="no" dir="ltr">merge_fn</code> in a cross-replica context.</p> <p>This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to <code translate="no" dir="ltr">strategy.run(step_fn, ...)</code>.</p> <p>See <a href="strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a> for an explanation.</p> <p>If not inside a distributed scope, this is equivalent to:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">strategy = tf.distribute.get_strategy()
with cross-replica-context(strategy):
  return merge_fn(strategy, *args, **kwargs)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">merge_fn</code> </td> <td> Function that joins arguments from threads that are given as PerReplica. It accepts <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object as the first argument. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> List or tuple with positional per-thread arguments for <code translate="no" dir="ltr">merge_fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword per-thread arguments for <code translate="no" dir="ltr">merge_fn</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The return value of <code translate="no" dir="ltr">merge_fn</code>, except for <code translate="no" dir="ltr">PerReplica</code> values which are unpacked. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    © 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/ReplicaContext" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/ReplicaContext</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
