
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.keras.optimizers.SGD - TensorFlow 2.3 - W3cubDocs</title>
  
  <meta name="description" content=" Gradient descent (with momentum) optimizer. ">
  <meta name="keywords" content="tf, keras, optimizers, sgd, tensorflow, tensorflow~2.3">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~2.3/keras/optimizers/sgd.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/tensorflow~2.3.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~2.3/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 2.3</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.keras.optimizers.SGD</h1>      <table class="tfo-notebook-buttons tfo-api" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/gradient_descent.py#L30-L189">  View source on GitHub </a> </td> </table> <p>Gradient descent (with momentum) optimizer.</p> <p>Inherits From: <a href="optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="0">View aliases</h4> <p> <b>Main aliases</b> </p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD"><code translate="no" dir="ltr">tf.optimizers.SGD</code></a></p> <b>Compat aliases for migration</b> <p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD"><code translate="no" dir="ltr">tf.compat.v1.keras.optimizers.SGD</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs
)
</pre>  <p>Update rule for parameter <code translate="no" dir="ltr">w</code> with gradient <code translate="no" dir="ltr">g</code> when <code translate="no" dir="ltr">momentum</code> is 0:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">w = w - learning_rate * g
</pre> <p>Update rule when <code translate="no" dir="ltr">momentum</code> is larger than 0:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">velocity = momentum * velocity - learning_rate * g
w = w * velocity
</pre> <p>When <code translate="no" dir="ltr">nesterov=False</code>, this rule becomes:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">velocity = momentum * velocity - learning_rate * g
w = w + momentum * velocity - learning_rate * g
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code>, floating point value, or a schedule that is a <a href="schedules/learningrateschedule"><code translate="no" dir="ltr">tf.keras.optimizers.schedules.LearningRateSchedule</code></a>, or a callable that takes no arguments and returns the actual value to use. The learning rate. Defaults to 0.01. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">momentum</code> </td> <td> float hyperparameter &gt;= 0 that accelerates gradient descent in the relevant direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient descent. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">nesterov</code> </td> <td> boolean. Whether to apply Nesterov momentum. Defaults to <code translate="no" dir="ltr">False</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name prefix for the operations created when applying gradients. Defaults to <code translate="no" dir="ltr">"SGD"</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Keyword arguments. Allowed to be one of <code translate="no" dir="ltr">"clipnorm"</code> or <code translate="no" dir="ltr">"clipvalue"</code>. <code translate="no" dir="ltr">"clipnorm"</code> (float) clips gradients by norm; <code translate="no" dir="ltr">"clipvalue"</code> (float) clips gradients by value. </td> </tr> </table> <h4 id="usage" data-text="Usage:" tabindex="0">Usage:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt = tf.keras.optimizers.SGD(learning_rate=0.1)
var = tf.Variable(1.0)
loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1
step_count = opt.minimize(loss, [var]).numpy()
# Step is `- learning_rate * grad`
var.numpy()
0.9
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
var = tf.Variable(1.0)
val0 = var.value()
loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1
# First step is `- learning_rate * grad`
step_count = opt.minimize(loss, [var]).numpy()
val1 = var.value()
(val0 - val1).numpy()
0.1
# On later steps, step-size increases because of momentum
step_count = opt.minimize(loss, [var]).numpy()
val2 = var.value()
(val1 - val2).numpy()
0.18
</pre> <h4 id="reference" data-text="Reference:" tabindex="0">Reference:</h4> <ul> <li>For <code translate="no" dir="ltr">nesterov=True</code>, See <a href="http://jmlr.org/proceedings/papers/v28/sutskever13.pdf">Sutskever et al., 2013</a>.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A non-empty string. The name to use for accumulators created for the optimizer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> keyword arguments. Allowed to be {<code translate="no" dir="ltr">clipnorm</code>, <code translate="no" dir="ltr">clipvalue</code>, <code translate="no" dir="ltr">lr</code>, <code translate="no" dir="ltr">decay</code>}. <code translate="no" dir="ltr">clipnorm</code> is clip gradients by norm; <code translate="no" dir="ltr">clipvalue</code> is clip gradients by value, <code translate="no" dir="ltr">decay</code> is included for backward compatibility to allow time inverse decay of learning rate. <code translate="no" dir="ltr">lr</code> is included for backward compatibility, recommended to use <code translate="no" dir="ltr">learning_rate</code> instead. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If name is malformed. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">iterations</code> </td> <td> Variable. The number of training steps this Optimizer has run. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> Returns variables of this Optimizer based on the order created. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="add_slot" data-text="add_slot" tabindex="0"><code translate="no" dir="ltr">add_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L735-L771">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_slot(
    var, slot_name, initializer='zeros'
)
</pre> <p>Add a new slot variable for <code translate="no" dir="ltr">var</code>.</p> <h3 id="add_weight" data-text="add_weight" tabindex="0"><code translate="no" dir="ltr">add_weight</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1003-L1043">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_weight(
    name, shape, dtype=None, initializer='zeros', trainable=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
</pre> <h3 id="apply_gradients" data-text="apply_gradients" tabindex="0"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L473-L550">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
apply_gradients(
    grads_and_vars, name=None, experimental_aggregate_gradients=True
)
</pre> <p>Apply gradients to variables.</p> <p>This is the second part of <code translate="no" dir="ltr">minimize()</code>. It returns an <code translate="no" dir="ltr">Operation</code> that applies gradients.</p> <p>The method sums gradients from all replicas in the presence of <a href="../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> by default. You can aggregate gradients yourself by passing <code translate="no" dir="ltr">experimental_aggregate_gradients=False</code>.</p> <h4 id="example" data-text="Example:" tabindex="0">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">grads = tape.gradient(loss, vars)
grads = tf.distribute.get_replica_context().all_reduce('sum', grads)
# Processing aggregated gradients.
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)

</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">grads_and_vars</code> </td> <td> List of (gradient, variable) pairs. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. Default to the name passed to the <code translate="no" dir="ltr">Optimizer</code> constructor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_aggregate_gradients</code> </td> <td> Whether to sum gradients from different replicas in the presense of <a href="../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>. If False, it's user responsibility to aggregate the gradients. Default to True. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">Operation</code> that applies the specified gradients. The <code translate="no" dir="ltr">iterations</code> will be automatically increased by 1. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">grads_and_vars</code> is malformed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If none of the variables have gradients. </td> </tr> </table> <h3 id="from_config" data-text="from_config" tabindex="0"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L879-L902">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@classmethod
from_config(
    config, custom_objects=None
)
</pre> <p>Creates an optimizer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same optimizer from the config dictionary.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">custom_objects</code> </td> <td> A Python dictionary mapping names to additional Python objects used to create this optimizer, such as a function used for a hyperparameter. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An optimizer instance. </td> </tr> 
</table> <h3 id="get_config" data-text="get_config" tabindex="0"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/gradient_descent.py#L181-L189">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_config()
</pre> <p>Returns the config of the optimizer.</p> <p>An optimizer config is a Python dictionary (serializable) containing the configuration of an optimizer. The same optimizer can be reinstantiated later (without any saved state) from this configuration.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Python dictionary. </td> </tr> 
</table> <h3 id="get_gradients" data-text="get_gradients" tabindex="0"><code translate="no" dir="ltr">get_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L445-L471">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_gradients(
    loss, params
)
</pre> <p>Returns gradients of <code translate="no" dir="ltr">loss</code> with respect to <code translate="no" dir="ltr">params</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> Loss tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">params</code> </td> <td> List of variables. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> List of gradient tensors. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> In case any gradient cannot be computed (e.g. if gradient function not implemented). </td> </tr> </table> <h3 id="get_slot" data-text="get_slot" tabindex="0"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L773-L776">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot(
    var, slot_name
)
</pre> <h3 id="get_slot_names" data-text="get_slot_names" tabindex="0"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L731-L733">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot_names()
</pre> <p>A list of names for this optimizer's slots.</p> <h3 id="get_updates" data-text="get_updates" tabindex="0"><code translate="no" dir="ltr">get_updates</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L647-L654">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_updates(
    loss, params
)
</pre> <h3 id="get_weights" data-text="get_weights" tabindex="0"><code translate="no" dir="ltr">get_weights</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L924-L952">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_weights()
</pre> <p>Returns the current weights of the optimizer.</p> <p>The weights of an optimizer are its state (ie, variables). This function returns the weight values associated with this optimizer as a list of Numpy arrays. The first value is always the iterations count of the optimizer, followed by the optimizer's state variables in the order they were created. The returned list can in turn be used to load state into similarly parameterized optimizers.</p> <p>For example, the RMSprop optimizer for this simple model returns a list of three values-- the iteration count, followed by the root-mean-square value of the kernel and bias of the single Dense layer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt = tf.keras.optimizers.RMSprop()
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss='mse')
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)
print('Training'); results = m.fit(data, labels)
Training ...
len(opt.get_weights())
3
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Weights values as a list of numpy arrays. </td> </tr> 
</table> <h3 id="minimize" data-text="minimize" tabindex="0"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L348-L377">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
minimize(
    loss, var_list, grad_loss=None, name=None
)
</pre> <p>Minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply computes gradient using <a href="../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> and calls <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying then call <a href="../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> A callable taking no arguments which returns the value to minimize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>, or a callable returning the list or tuple of <code translate="no" dir="ltr">Variable</code> objects. Use callable when the variable list would otherwise be incomplete before <code translate="no" dir="ltr">minimize</code> since the variables are created at the first time <code translate="no" dir="ltr">loss</code> is called. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">grad_loss</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">Operation</code> that updates the variables in <code translate="no" dir="ltr">var_list</code>. The <code translate="no" dir="ltr">iterations</code> will be automatically increased by 1. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects. </td> </tr> </table> <h3 id="set_weights" data-text="set_weights" tabindex="0"><code translate="no" dir="ltr">set_weights</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L955-L1001">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_weights(
    weights
)
</pre> <p>Set the weights of the optimizer.</p> <p>The weights of an optimizer are its state (ie, variables). This function takes the weight values associated with this optimizer as a list of Numpy arrays. The first value is always the iterations count of the optimizer, followed by the optimizer's state variables in the order they are created. The passed values are used to set the new state of the optimizer.</p> <p>For example, the RMSprop optimizer for this simple model takes a list of three values-- the iteration count, followed by the root-mean-square value of the kernel and bias of the single Dense layer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt = tf.keras.optimizers.RMSprop()
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss='mse')
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)
print('Training'); results = m.fit(data, labels)
Training ...
new_weights = [np.array(10), np.ones([20, 10]), np.zeros([10])]
opt.set_weights(new_weights)
opt.iterations
&lt;tf.Variable 'RMSprop/iter:0' shape=() dtype=int64, numpy=10&gt;
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> weight values as a list of numpy arrays. </td> </tr> </table> <h3 id="variables" data-text="variables" tabindex="0"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L915-L917">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
variables()
</pre> <p>Returns variables of this Optimizer based on the order created.</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    © 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/optimizers/SGD" class="_attribution-link">https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/optimizers/SGD</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
