
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Tokenize - Python 2.7 - W3cubDocs</title>
  
  <meta name="description" content=" Source code&#58; Lib&#47;tokenize.py ">
  <meta name="keywords" content="tokenize, —, tokenizer, for, python, source, python~2.7">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/python~2.7/library/tokenize.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/python~2.7.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/python~2.7/" class="_nav-link" title="" style="margin-left:0;">Python 2.7</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _python">
				
				
<h1 id="tokenize-tokenizer-for-python-source"> tokenize — Tokenizer for Python source</h1> <p id="module-tokenize"><strong>Source code:</strong> <a class="reference external" href="https://github.com/python/cpython/tree/2.7/Lib/tokenize.py">Lib/tokenize.py</a></p>  <p>The <a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code>tokenize</code></a> module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it useful for implementing “pretty-printers,” including colorizers for on-screen displays.</p> <p>To simplify token stream handling, all <a class="reference internal" href="https://docs.python.org/2.7/reference/lexical_analysis.html#operators"><span class="std std-ref">Operators</span></a> and <a class="reference internal" href="https://docs.python.org/2.7/reference/lexical_analysis.html#delimiters"><span class="std std-ref">Delimiters</span></a> tokens are returned using the generic <a class="reference internal" href="token#token.OP" title="token.OP"><code>token.OP</code></a> token type. The exact type can be determined by checking the second field (containing the actual token string matched) of the tuple returned from <a class="reference internal" href="#tokenize.generate_tokens" title="tokenize.generate_tokens"><code>tokenize.generate_tokens()</code></a> for the character sequence that identifies a specific operator token.</p> <p>The primary entry point is a <a class="reference internal" href="https://docs.python.org/2.7/glossary.html#term-generator"><span class="xref std std-term">generator</span></a>:</p> <dl class="function"> <dt id="tokenize.generate_tokens">
<code>tokenize.generate_tokens(readline)</code> </dt> <dd>
<p>The <a class="reference internal" href="#tokenize.generate_tokens" title="tokenize.generate_tokens"><code>generate_tokens()</code></a> generator requires one argument, <em>readline</em>, which must be a callable object which provides the same interface as the <a class="reference internal" href="stdtypes#file.readline" title="file.readline"><code>readline()</code></a> method of built-in file objects (see section <a class="reference internal" href="stdtypes#bltin-file-objects"><span class="std std-ref">File Objects</span></a>). Each call to the function should return one line of input as a string. Alternately, <em>readline</em> may be a callable object that signals completion by raising <a class="reference internal" href="exceptions#exceptions.StopIteration" title="exceptions.StopIteration"><code>StopIteration</code></a>.</p> <p>The generator produces 5-tuples with these members: the token type; the token string; a 2-tuple <code>(srow, scol)</code> of ints specifying the row and column where the token begins in the source; a 2-tuple <code>(erow, ecol)</code> of ints specifying the row and column where the token ends in the source; and the line on which the token was found. The line passed (the last tuple item) is the <em>logical</em> line; continuation lines are included.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 2.2.</span></p> </div> </dd>
</dl> <p>An older entry point is retained for backward compatibility:</p> <dl class="function"> <dt id="tokenize.tokenize">
<code>tokenize.tokenize(readline[, tokeneater])</code> </dt> <dd>
<p>The <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a> function accepts two parameters: one representing the input stream, and one providing an output mechanism for <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a>.</p> <p>The first parameter, <em>readline</em>, must be a callable object which provides the same interface as the <a class="reference internal" href="stdtypes#file.readline" title="file.readline"><code>readline()</code></a> method of built-in file objects (see section <a class="reference internal" href="stdtypes#bltin-file-objects"><span class="std std-ref">File Objects</span></a>). Each call to the function should return one line of input as a string. Alternately, <em>readline</em> may be a callable object that signals completion by raising <a class="reference internal" href="exceptions#exceptions.StopIteration" title="exceptions.StopIteration"><code>StopIteration</code></a>.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 2.5: </span>Added <a class="reference internal" href="exceptions#exceptions.StopIteration" title="exceptions.StopIteration"><code>StopIteration</code></a> support.</p> </div> <p>The second parameter, <em>tokeneater</em>, must also be a callable object. It is called once for each token, with five arguments, corresponding to the tuples generated by <a class="reference internal" href="#tokenize.generate_tokens" title="tokenize.generate_tokens"><code>generate_tokens()</code></a>.</p> </dd>
</dl> <p>All constants from the <a class="reference internal" href="token#module-token" title="token: Constants representing terminal nodes of the parse tree."><code>token</code></a> module are also exported from <a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code>tokenize</code></a>, as are two additional token type values that might be passed to the <em>tokeneater</em> function by <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code>tokenize()</code></a>:</p> <dl class="data"> <dt id="tokenize.COMMENT">
<code>tokenize.COMMENT</code> </dt> <dd>
<p>Token value used to indicate a comment.</p> </dd>
</dl> <dl class="data"> <dt id="tokenize.NL">
<code>tokenize.NL</code> </dt> <dd>
<p>Token value used to indicate a non-terminating newline. The NEWLINE token indicates the end of a logical line of Python code; NL tokens are generated when a logical line of code is continued over multiple physical lines.</p> </dd>
</dl> <p>Another function is provided to reverse the tokenization process. This is useful for creating tools that tokenize a script, modify the token stream, and write back the modified script.</p> <dl class="function"> <dt id="tokenize.untokenize">
<code>tokenize.untokenize(iterable)</code> </dt> <dd>
<p>Converts tokens back into Python source code. The <em>iterable</em> must return sequences with at least two elements, the token type and the token string. Any additional sequence elements are ignored.</p> <p>The reconstructed script is returned as a single string. The result is guaranteed to tokenize back to match the input so that the conversion is lossless and round-trips are assured. The guarantee applies only to the token type and token string as the spacing between tokens (column positions) may change.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 2.5.</span></p> </div> </dd>
</dl> <dl class="exception"> <dt id="tokenize.TokenError">
<code>exception tokenize.TokenError</code> </dt> <dd>
<p>Raised when either a docstring or expression that may be split over several lines is not completed anywhere in the file, for example:</p> <pre data-language="python">"""Beginning of
docstring
</pre> <p>or:</p> <pre data-language="python">[1,
 2,
 3
</pre> </dd>
</dl> <p>Note that unclosed single-quoted strings do not cause an error to be raised. They are tokenized as <code>ERRORTOKEN</code>, followed by the tokenization of their contents.</p> <p>Example of a script re-writer that transforms float literals into Decimal objects:</p> <pre data-language="python">def decistmt(s):
    """Substitute Decimals for floats in a string of statements.

    &gt;&gt;&gt; from decimal import Decimal
    &gt;&gt;&gt; s = 'print +21.3e-5*-.1234/81.7'
    &gt;&gt;&gt; decistmt(s)
    "print +Decimal ('21.3e-5')*-Decimal ('.1234')/Decimal ('81.7')"

    &gt;&gt;&gt; exec(s)
    -3.21716034272e-007
    &gt;&gt;&gt; exec(decistmt(s))
    -3.217160342717258261933904529E-7

    """
    result = []
    g = generate_tokens(StringIO(s).readline)   # tokenize the string
    for toknum, tokval, _, _, _  in g:
        if toknum == NUMBER and '.' in tokval:  # replace NUMBER tokens
            result.extend([
                (NAME, 'Decimal'),
                (OP, '('),
                (STRING, repr(tokval)),
                (OP, ')')
            ])
        else:
            result.append((toknum, tokval))
    return untokenize(result)
</pre>
<div class="_attribution">
  <p class="_attribution-p">
    © 2001–2020 Python Software Foundation<br>Licensed under the PSF License.<br>
    <a href="https://docs.python.org/2.7/library/tokenize.html" class="_attribution-link">https://docs.python.org/2.7/library/tokenize.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
