
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.function - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" Creates a callable TensorFlow graph from a Python function. ">
  <meta name="keywords" content="tf, function, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/function.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.function</h1>      <table class="tfo-notebook-buttons tfo-api" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/eager/def_function.py#L759-L1078">  View source on GitHub </a> </td> </table> <p>Creates a callable TensorFlow graph from a Python function.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="0">View aliases</h4> <p> <b>Main aliases</b> </p>
<p>`tf.contrib.eager.function`</p> <b>Compat aliases for migration</b> <p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/function"><code translate="no" dir="ltr">tf.compat.v1.function</code></a>, `tf.compat.v2.function`</p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.function(
    func=None, input_signature=None, autograph=True,
    experimental_autograph_options=None, experimental_relax_shapes=False,
    experimental_compile=None
)
</pre>  <p><code translate="no" dir="ltr">function</code> constructs a callable that executes a TensorFlow graph (<a href="graph"><code translate="no" dir="ltr">tf.Graph</code></a>) created by tracing the TensorFlow operations in <code translate="no" dir="ltr">func</code>. This allows the TensorFlow runtime to apply optimizations and exploit parallelism in the computation defined by <code translate="no" dir="ltr">func</code>.</p> <p><em>Example Usage</em></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def f(x, y):
  return tf.reduce_mean(tf.multiply(x ** 2, 3) + y)

g = tf.function(f)

x = tf.constant([[2.0, 3.0]])
y = tf.constant([[3.0, -2.0]])

# `f` and `g` will return the same value, but `g` will be executed as a
# TensorFlow graph.
assert f(x, y).numpy() == g(x, y).numpy()

# Tensors and tf.Variables used by the Python function are captured in the
# graph.
@tf.function
def h():
  return f(x, y)

assert (h().numpy() == f(x, y).numpy()).all()

# Data-dependent control flow is also captured in the graph. Supported
# control flow statements include `if`, `for`, `while`, `break`, `continue`,
# `return`.
@tf.function
def g(x):
  if tf.reduce_sum(x) &gt; 0:
    return x * x
  else:
    return -x // 2

# print and TensorFlow side effects are supported, but exercise caution when
# using Python side effects like mutating objects, saving to files, etc.
l = []

@tf.function
def g(x):
  for i in x:
    print(i)                              # Works
    tf.compat.v1.assign(v, i)                       # Works
    tf.compat.v1.py_func(lambda i: l.append(i))(i)  # Works
    l.append(i)                           # Caution! Doesn't work.
</pre> <p>Note that unlike other TensorFlow operations, we don't convert python numerical inputs to tensors. Moreover, a new graph is generated for each distinct python numerical value, for example calling <code translate="no" dir="ltr">g(2)</code> and <code translate="no" dir="ltr">g(3)</code> will generate two new graphs (while only one is generated if you call <code translate="no" dir="ltr">g(tf.constant(2))</code> and <code translate="no" dir="ltr">g(tf.constant(3))</code>). Therefore, python numerical inputs should be restricted to arguments that will have few distinct values, such as hyperparameters like the number of layers in a neural network. This allows TensorFlow to optimize each variant of the neural network.</p> <p><em>Referencing <a href="variable"><code translate="no" dir="ltr">tf.Variable</code></a>s</em></p> <p>The Python function <code translate="no" dir="ltr">func</code> may reference stateful objects (such as <a href="variable"><code translate="no" dir="ltr">tf.Variable</code></a>). These are captured as implicit inputs to the callable returned by <code translate="no" dir="ltr">function</code>. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">c = tf.Variable(0)

@tf.function
def f(x):
  c.assign_add(1)
  return x + tf.compat.v1.to_float(c)

assert int(c) == 0
assert f(1.0) == 2.0
assert int(c) == 1
assert f(1.0) == 3.0
assert int(c) == 2
</pre> <p><code translate="no" dir="ltr">function</code> can be applied to methods of an object. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class Dense(object):
  def __init__(self):
    self.W = tf.Variable(tf.compat.v1.glorot_uniform_initializer()((10, 10)))
    self.b = tf.Variable(tf.zeros(10))

  @tf.function
  def compute(self, x):
    return tf.matmul(x, self.W) + self.b

d1 = Dense()
d2 = Dense()
x = tf.random.uniform((10, 10))
# d1 and d2 are using distinct variables
assert not (d1.compute(x).numpy() == d2.compute(x).numpy()).all()
</pre> <p><em>Usage with <a href="keras"><code translate="no" dir="ltr">tf.keras</code></a></em></p> <p>The <code translate="no" dir="ltr">call</code> methods of a <a href="keras/model"><code translate="no" dir="ltr">tf.keras.Model</code></a> subclass can be decorated with <code translate="no" dir="ltr">function</code> in order to apply graph execution optimizations on it. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class MyModel(tf.keras.Model):
  def __init__(self, keep_probability=0.2):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4)
    self.dense2 = tf.keras.layers.Dense(5)
    self.keep_probability = keep_probability

  @tf.function
  def call(self, inputs, training=True):
    y = self.dense2(self.dense1(inputs))
    if training:
      return tf.nn.dropout(y, self.keep_probability)
    else:
      return y

model = MyModel()
model(x, training=True)  # executes a graph, with dropout
model(x, training=False) # executes a graph, without dropout
</pre> <p><em>Input Signatures</em></p> <p><code translate="no" dir="ltr">function</code> instantiates a separate graph for every unique set of input shapes and datatypes. For example, the following code snippet will result in three distinct graphs being traced, as each input has a different shape.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.function
def f(x): return tf.add(x, 1.)

scalar = tf.constant(1.0)
vector = tf.constant([1.0, 1.0])
matrix = tf.constant([[3.0]])

f(scalar)
f(vector)
f(matrix)
</pre> <p>An "input signature" can be optionally provided to <code translate="no" dir="ltr">function</code> to control the graphs traced. The input signature specifies the shape and type of each <code translate="no" dir="ltr">Tensor</code> argument to the function using a <a href="tensorspec"><code translate="no" dir="ltr">tf.TensorSpec</code></a> object. For example, the following code snippet ensures that a single graph is created where the input <code translate="no" dir="ltr">Tensor</code> is required to be a floating point tensor with no restrictions on shape.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])
def f(x): return tf.add(x, 1.)
</pre> <p>When an <code translate="no" dir="ltr">input_signature</code> is specified, the callable will convert the inputs to the specified TensorSpecs.</p> <p><em>Tracing and staging</em></p> <p>When <code translate="no" dir="ltr">autograph</code> is <code translate="no" dir="ltr">True</code>, all Python control flow that depends on <code translate="no" dir="ltr">Tensor</code> values is staged into a TensorFlow graph. When <code translate="no" dir="ltr">autograph</code> is <code translate="no" dir="ltr">False</code>, the function is traced and control flow is not allowed to depend on data.</p> <p>Note that <code translate="no" dir="ltr">function</code> only stages TensorFlow operations, all Python code that <code translate="no" dir="ltr">func</code> executes and does not depend on data will shape the <em>construction</em> of the graph. For example, consider the following:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">import numpy as np

def add_noise():
  return tf.eye(5) + np.random.randn(5, 5)

traced = tf.function(add_noise)
</pre> <p><code translate="no" dir="ltr">add_noise()</code> will return a different output every time it is invoked. However, <code translate="no" dir="ltr">traced()</code> will return the same value every time it is called, since a particular random value generated by the <code translate="no" dir="ltr">np.random.randn</code> call will be inserted in the traced/staged TensorFlow graph as a constant. In this particular example, replacing <code translate="no" dir="ltr">np.random.randn(5, 5)</code> with <code translate="no" dir="ltr">tf.random.normal((5, 5))</code> will result in the same behavior for <code translate="no" dir="ltr">add_noise()</code> and <code translate="no" dir="ltr">traced()</code>.</p> <p><em>Python Side-Effects</em></p> <p>A corollary of the previous discussion on tracing is the following: If a Python function <code translate="no" dir="ltr">func</code> has Python side-effects, then executing <code translate="no" dir="ltr">func</code> multiple times may not be semantically equivalent to executing <code translate="no" dir="ltr">F = tf.function(func)</code> multiple times; this difference is due to the fact that <code translate="no" dir="ltr">function</code> only captures the subgraph of TensorFlow operations that is constructed when <code translate="no" dir="ltr">func</code> is invoked to trace a graph.</p> <p>The same is true if code with Python side effects is used inside control flow, such as a loop. If your code uses side effects that are not intended to control graph construction, wrap them inside <a href="py_func"><code translate="no" dir="ltr">tf.compat.v1.py_func</code></a>.</p> <p><em>Retracing</em></p> <p>A single tf.function object might need to map to multiple computation graphs under the hood. This should be visible only as performance (tracing graphs has a nonzero computational and memory cost) but should not affect the correctness of the program. A traced function should return the same result as it would when run eagerly, assuming no unintended Python side-effects.</p> <p>Calling a <a href="function"><code translate="no" dir="ltr">tf.function</code></a> with tensor arguments of different dtypes should lead to at least one computational graph per distinct set of dtypes. Alternatively, always calling a <a href="function"><code translate="no" dir="ltr">tf.function</code></a> with tensor arguments of the same shapes and dtypes and the same non-tensor arguments should not lead to additional retracings of your function.</p> <p>Other than that, TensorFlow reserves the right to retrace functions as many times as needed, to ensure that traced functions behave as they would when run eagerly and to provide the best end-to-end performance. For example, the behavior of how many traces TensorFlow will do when the function is repeatedly called with different python scalars as arguments is left undefined to allow for future optimizations.</p> <p>To control the tracing behavior, use the following tools:</p> <ul> <li>different <a href="function"><code translate="no" dir="ltr">tf.function</code></a> objects are guaranteed to not share traces; and</li> <li>specifying a signature or using concrete function objects returned from get_concrete_function() guarantees that only one function graph will be built.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">func</code> </td> <td> function to be compiled. If <code translate="no" dir="ltr">func</code> is None, returns a decorator that can be invoked with a single argument - <code translate="no" dir="ltr">func</code>. The end result is equivalent to providing all the arguments up front. In other words, <code translate="no" dir="ltr">tf.function(input_signature=...)(func)</code> is equivalent to <a href="function"><code translate="no" dir="ltr">tf.function(func, input_signature=...)</code></a>. The former can be used to decorate Python functions, for example: @tf.function(input_signature=...) def foo(...): ... </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_signature</code> </td> <td> A possibly nested sequence of <a href="tensorspec"><code translate="no" dir="ltr">tf.TensorSpec</code></a> objects specifying the shapes and dtypes of the Tensors that will be supplied to this function. If <code translate="no" dir="ltr">None</code>, a separate function is instantiated for each inferred input signature. If input_signature is specified, every input to <code translate="no" dir="ltr">func</code> must be a <code translate="no" dir="ltr">Tensor</code>, and <code translate="no" dir="ltr">func</code> cannot accept <code translate="no" dir="ltr">**kwargs</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">autograph</code> </td> <td> Whether autograph should be applied on <code translate="no" dir="ltr">func</code> before tracing a graph. This allows for dynamic control flow (Python if's, loops etc.) in the traced graph. See https://www.tensorflow.org/guide/autograph for more information. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_autograph_options</code> </td> <td> Experimental knobs (in the form of a tuple of tensorflow.autograph.Feature values) to control behavior when autograph=True. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_relax_shapes</code> </td> <td> When true, argument shapes may be relaxed to avoid unecessary retracing. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_compile</code> </td> <td> If false, execute the function in a regular way. The function is optimized by some graph rewrite passes (some ops might be clustered into a single op) and interpreted by the standard TensorFlow executor, which dispatches op kernels one by one as they become executable. Set it to false when directly running a multi-device function on TPUs (e.g. two TPU cores, one TPU core and its host CPU). If True, the function is compiled directly by XLA (https://www.tensorflow.org/xla). XLA would fuse all the ops and emit more efficient code to run for some devices (e.g. TPU, XLA_GPU) and some use cases (e.g. dense tensor computation). It requires that the whole function is compilable by XLA (e.g. static tensor shape, a subset of operations, no string, compile-time constant input, etc). If None (default), compile the function with XLA when running on TPU and go through the regular function execution path when running on other devices. Note: TensorArrays on TPU don't work with standard TensorFlow executor. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> If <code translate="no" dir="ltr">func</code> is not None, returns a callable that will execute the compiled function (and return zero or more <a href="tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> objects). If <code translate="no" dir="ltr">func</code> is None, returns a decorator that, when invoked with a single <code translate="no" dir="ltr">func</code> argument, returns a callable equivalent to the case above. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">input_signature</code> is neither <code translate="no" dir="ltr">None</code> nor a sequence of <code translate="no" dir="ltr">TensorSpec</code> objects. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    © 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/function" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/function</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
