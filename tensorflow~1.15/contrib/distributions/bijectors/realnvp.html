
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.contrib.distributions.bijectors.RealNVP - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" RealNVP &#34;affine coupling layer&#34; for vector-valued events. ">
  <meta name="keywords" content="tf, contrib, distributions, bijectors, realnvp, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/contrib/distributions/bijectors/realnvp.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.contrib.distributions.bijectors.RealNVP</h1>       <p>RealNVP "affine coupling layer" for vector-valued events.</p> <p>Inherits From: <a href="bijector"><code translate="no" dir="ltr">Bijector</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.contrib.distributions.bijectors.RealNVP(
    num_masked, shift_and_log_scale_fn, is_constant_jacobian=False,
    validate_args=False, name=None
)
</pre>  <p>Real NVP models a normalizing flow on a <code translate="no" dir="ltr">D</code>-dimensional distribution via a single <code translate="no" dir="ltr">D-d</code>-dimensional conditional distribution [(Dinh et al., 2017)][1]:</p> <p><code translate="no" dir="ltr">y[d:D] = y[d:D] * math_ops.exp(log_scale_fn(y[d:D])) + shift_fn(y[d:D])</code> <code translate="no" dir="ltr">y[0:d] = x[0:d]</code></p> <p>The last <code translate="no" dir="ltr">D-d</code> units are scaled and shifted based on the first <code translate="no" dir="ltr">d</code> units only, while the first <code translate="no" dir="ltr">d</code> units are 'masked' and left unchanged. Real NVP's <code translate="no" dir="ltr">shift_and_log_scale_fn</code> computes vector-valued quantities. For scale-and-shift transforms that do not depend on any masked units, i.e. <code translate="no" dir="ltr">d=0</code>, use the <code translate="no" dir="ltr">tfb.Affine</code> bijector with learned parameters instead.</p> <p>Masking is currently only supported for base distributions with <code translate="no" dir="ltr">event_ndims=1</code>. For more sophisticated masking schemes like checkerboard or channel-wise masking [(Papamakarios et al., 2016)[4], use the <code translate="no" dir="ltr">tfb.Permute</code> bijector to re-order desired masked units into the first <code translate="no" dir="ltr">d</code> units. For base distributions with <code translate="no" dir="ltr">event_ndims &gt; 1</code>, use the <code translate="no" dir="ltr">tfb.Reshape</code> bijector to flatten the event shape.</p> <p>Recall that the MAF bijector [(Papamakarios et al., 2016)][4] implements a normalizing flow via an autoregressive transformation. MAF and IAF have opposite computational tradeoffs - MAF can train all units in parallel but must sample units sequentially, while IAF must train units sequentially but can sample in parallel. In contrast, Real NVP can compute both forward and inverse computations in parallel. However, the lack of an autoregressive transformations makes it less expressive on a per-bijector basis.</p> <p>A "valid" <code translate="no" dir="ltr">shift_and_log_scale_fn</code> must compute each <code translate="no" dir="ltr">shift</code> (aka <code translate="no" dir="ltr">loc</code> or "mu" in [Papamakarios et al. (2016)][4]) and <code translate="no" dir="ltr">log(scale)</code> (aka "alpha" in [Papamakarios et al. (2016)][4]) such that each are broadcastable with the arguments to <code translate="no" dir="ltr">forward</code> and <code translate="no" dir="ltr">inverse</code>, i.e., such that the calculations in <code translate="no" dir="ltr">forward</code>, <code translate="no" dir="ltr">inverse</code> [below] are possible. For convenience, <code translate="no" dir="ltr">real_nvp_default_nvp</code> is offered as a possible <code translate="no" dir="ltr">shift_and_log_scale_fn</code> function.</p> <p>NICE [(Dinh et al., 2014)][2] is a special case of the Real NVP bijector which discards the scale transformation, resulting in a constant-time inverse-log-determinant-Jacobian. To use a NICE bijector instead of Real NVP, <code translate="no" dir="ltr">shift_and_log_scale_fn</code> should return <code translate="no" dir="ltr">(shift, None)</code>, and <code translate="no" dir="ltr">is_constant_jacobian</code> should be set to <code translate="no" dir="ltr">True</code> in the <code translate="no" dir="ltr">RealNVP</code> constructor. Calling <code translate="no" dir="ltr">real_nvp_default_template</code> with <code translate="no" dir="ltr">shift_only=True</code> returns one such NICE-compatible <code translate="no" dir="ltr">shift_and_log_scale_fn</code>.</p> <p>Caching: the scalar input depth <code translate="no" dir="ltr">D</code> of the base distribution is not known at construction time. The first call to any of <code translate="no" dir="ltr">forward(x)</code>, <code translate="no" dir="ltr">inverse(x)</code>, <code translate="no" dir="ltr">inverse_log_det_jacobian(x)</code>, or <code translate="no" dir="ltr">forward_log_det_jacobian(x)</code> memoizes <code translate="no" dir="ltr">D</code>, which is re-used in subsequent calls. This shape must be known prior to graph execution (which is the case if using tf.layers).</p> <h4 id="example_use" data-text="Example Use" tabindex="0">Example Use</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">import tensorflow_probability as tfp
tfd = tfp.distributions
tfb = tfp.bijectors

# A common choice for a normalizing flow is to use a Gaussian for the base
# distribution. (However, any continuous distribution would work.) E.g.,
num_dims = 3
num_samples = 1
nvp = tfd.TransformedDistribution(
    distribution=tfd.MultivariateNormalDiag(loc=np.zeros(num_dims)),
    bijector=tfb.RealNVP(
        num_masked=2,
        shift_and_log_scale_fn=tfb.real_nvp_default_template(
            hidden_layers=[512, 512])))

x = nvp.sample(num_samples)
nvp.log_prob(x)
nvp.log_prob(np.zeros([num_samples, num_dims]))
</pre> <p>For more examples, see [Jang (2018)][3].</p> <h4 id="references" data-text="References" tabindex="0">References</h4> <p>[1]: Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation using Real NVP. In <em>International Conference on Learning Representations</em>, 2017. <a href="https://arxiv.org/abs/1605.08803">https://arxiv.org/abs/1605.08803</a></p> <p>[2]: Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Components Estimation. <em>arXiv preprint arXiv:1410.8516</em>, 2014. <a href="https://arxiv.org/abs/1410.8516">https://arxiv.org/abs/1410.8516</a></p> <p>[3]: Eric Jang. Normalizing Flows Tutorial, Part 2: Modern Normalizing Flows. <em>Technical Report</em>, 2018. <a href="http://blog.evjang.com/2018/01/nf2.html">http://blog.evjang.com/2018/01/nf2.html</a></p> <p>[4]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017. <a href="https://arxiv.org/abs/1705.07057">https://arxiv.org/abs/1705.07057</a></p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">num_masked</code> </td> <td> Python <code translate="no" dir="ltr">int</code> indicating that the first <code translate="no" dir="ltr">d</code> units of the event should be masked. Must be in the closed interval <code translate="no" dir="ltr">[1, D-1]</code>, where <code translate="no" dir="ltr">D</code> is the event size of the base distribution. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shift_and_log_scale_fn</code> </td> <td> Python <code translate="no" dir="ltr">callable</code> which computes <code translate="no" dir="ltr">shift</code> and <code translate="no" dir="ltr">log_scale</code> from both the forward domain (<code translate="no" dir="ltr">x</code>) and the inverse domain (<code translate="no" dir="ltr">y</code>). Calculation must respect the "autoregressive property" (see class docstring). Suggested default <code translate="no" dir="ltr">masked_autoregressive_default_template(hidden_layers=...)</code>. Typically the function contains <code translate="no" dir="ltr">tf.Variables</code> and is wrapped using <a href="../../../make_template"><code translate="no" dir="ltr">tf.compat.v1.make_template</code></a>. Returning <code translate="no" dir="ltr">None</code> for either (both) <code translate="no" dir="ltr">shift</code>, <code translate="no" dir="ltr">log_scale</code> is equivalent to (but more efficient than) returning zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">is_constant_jacobian</code> </td> <td> Python <code translate="no" dir="ltr">bool</code>. Default: <code translate="no" dir="ltr">False</code>. When <code translate="no" dir="ltr">True</code> the implementation assumes <code translate="no" dir="ltr">log_scale</code> does not depend on the forward domain (<code translate="no" dir="ltr">x</code>) or inverse domain (<code translate="no" dir="ltr">y</code>) values. (No validation is made; <code translate="no" dir="ltr">is_constant_jacobian=False</code> is always safe but possibly computationally inefficient.) </td> </tr>
<tr> <td> <code translate="no" dir="ltr">validate_args</code> </td> <td> Python <code translate="no" dir="ltr">bool</code> indicating whether arguments should be checked for correctness. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Python <code translate="no" dir="ltr">str</code>, name given to ops managed by this object. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If num_masked &lt; 1. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> dtype of <code translate="no" dir="ltr">Tensor</code>s transformable by this distribution. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">forward_min_event_ndims</code> </td> <td> Returns the minimal number of dimensions bijector.forward operates on. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">graph_parents</code> </td> <td> Returns this <code translate="no" dir="ltr">Bijector</code>'s graph_parents as a Python list. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">inverse_min_event_ndims</code> </td> <td> Returns the minimal number of dimensions bijector.inverse operates on. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">is_constant_jacobian</code> </td> <td> Returns true iff the Jacobian matrix is not a function of x. <blockquote class="note">
<strong>Note:</strong><span> Jacobian matrix is either constant for both forward and inverse or neither. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Returns the string name of this <code translate="no" dir="ltr">Bijector</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">validate_args</code> </td> <td> Returns True if Tensor arguments will be validated. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="forward" data-text="forward" tabindex="0"><code translate="no" dir="ltr">forward</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L753-L768">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
forward(
    x, name='forward'
)
</pre> <p>Returns the forward <code translate="no" dir="ltr">Bijector</code> evaluation, i.e., X = g(Y).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>. The input to the "forward" evaluation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name to give this op. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">Tensor</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">self.dtype</code> is specified and <code translate="no" dir="ltr">x.dtype</code> is not <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> if <code translate="no" dir="ltr">_forward</code> is not implemented. </td> </tr> </table> <h3 id="forward_event_shape" data-text="forward_event_shape" tabindex="0"><code translate="no" dir="ltr">forward_event_shape</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L677-L690">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
forward_event_shape(
    input_shape
)
</pre> <p>Shape of a single sample from a single batch as a <code translate="no" dir="ltr">TensorShape</code>.</p> <p>Same meaning as <code translate="no" dir="ltr">forward_event_shape_tensor</code>. May be only partially defined.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_shape</code> </td> <td> <code translate="no" dir="ltr">TensorShape</code> indicating event-portion shape passed into <code translate="no" dir="ltr">forward</code> function. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">forward_event_shape_tensor</code> </td> <td> <code translate="no" dir="ltr">TensorShape</code> indicating event-portion shape after applying <code translate="no" dir="ltr">forward</code>. Possibly unknown. </td> </tr> </table> <h3 id="forward_event_shape_tensor" data-text="forward_event_shape_tensor" tabindex="0"><code translate="no" dir="ltr">forward_event_shape_tensor</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L653-L670">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
forward_event_shape_tensor(
    input_shape, name='forward_event_shape_tensor'
)
</pre> <p>Shape of a single sample from a single batch as an <code translate="no" dir="ltr">int32</code> 1D <code translate="no" dir="ltr">Tensor</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_shape</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">int32</code> vector indicating event-portion shape passed into <code translate="no" dir="ltr">forward</code> function. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> name to give to the op </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">forward_event_shape_tensor</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">int32</code> vector indicating event-portion shape after applying <code translate="no" dir="ltr">forward</code>. </td> </tr> </table> <h3 id="forward_log_det_jacobian" data-text="forward_log_det_jacobian" tabindex="0"><code translate="no" dir="ltr">forward_log_det_jacobian</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L973-L997">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
forward_log_det_jacobian(
    x, event_ndims, name='forward_log_det_jacobian'
)
</pre> <p>Returns both the forward_log_det_jacobian.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>. The input to the "forward" Jacobian determinant evaluation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">event_ndims</code> </td> <td> Number of dimensions in the probabilistic events being transformed. Must be greater than or equal to <code translate="no" dir="ltr">self.forward_min_event_ndims</code>. The result is summed over the final dimensions to produce a scalar Jacobian determinant for each event, i.e. it has shape <code translate="no" dir="ltr">x.shape.ndims - event_ndims</code> dimensions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name to give this op. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">Tensor</code>, if this bijector is injective. If not injective this is not implemented. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">self.dtype</code> is specified and <code translate="no" dir="ltr">y.dtype</code> is not <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> if neither <code translate="no" dir="ltr">_forward_log_det_jacobian</code> nor {<code translate="no" dir="ltr">_inverse</code>, <code translate="no" dir="ltr">_inverse_log_det_jacobian</code>} are implemented, or this is a non-injective bijector. </td> </tr> </table> <h3 id="inverse" data-text="inverse" tabindex="0"><code translate="no" dir="ltr">inverse</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L787-L804">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
inverse(
    y, name='inverse'
)
</pre> <p>Returns the inverse <code translate="no" dir="ltr">Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">y</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>. The input to the "inverse" evaluation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name to give this op. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">Tensor</code>, if this bijector is injective. If not injective, returns the k-tuple containing the unique <code translate="no" dir="ltr">k</code> points <code translate="no" dir="ltr">(x1, ..., xk)</code> such that <code translate="no" dir="ltr">g(xi) = y</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">self.dtype</code> is specified and <code translate="no" dir="ltr">y.dtype</code> is not <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> if <code translate="no" dir="ltr">_inverse</code> is not implemented. </td> </tr> </table> <h3 id="inverse_event_shape" data-text="inverse_event_shape" tabindex="0"><code translate="no" dir="ltr">inverse_event_shape</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L721-L734">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
inverse_event_shape(
    output_shape
)
</pre> <p>Shape of a single sample from a single batch as a <code translate="no" dir="ltr">TensorShape</code>.</p> <p>Same meaning as <code translate="no" dir="ltr">inverse_event_shape_tensor</code>. May be only partially defined.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">output_shape</code> </td> <td> <code translate="no" dir="ltr">TensorShape</code> indicating event-portion shape passed into <code translate="no" dir="ltr">inverse</code> function. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inverse_event_shape_tensor</code> </td> <td> <code translate="no" dir="ltr">TensorShape</code> indicating event-portion shape after applying <code translate="no" dir="ltr">inverse</code>. Possibly unknown. </td> </tr> </table> <h3 id="inverse_event_shape_tensor" data-text="inverse_event_shape_tensor" tabindex="0"><code translate="no" dir="ltr">inverse_event_shape_tensor</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L697-L714">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
inverse_event_shape_tensor(
    output_shape, name='inverse_event_shape_tensor'
)
</pre> <p>Shape of a single sample from a single batch as an <code translate="no" dir="ltr">int32</code> 1D <code translate="no" dir="ltr">Tensor</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">output_shape</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">int32</code> vector indicating event-portion shape passed into <code translate="no" dir="ltr">inverse</code> function. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> name to give to the op </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inverse_event_shape_tensor</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">int32</code> vector indicating event-portion shape after applying <code translate="no" dir="ltr">inverse</code>. </td> </tr> </table> <h3 id="inverse_log_det_jacobian" data-text="inverse_log_det_jacobian" tabindex="0"><code translate="no" dir="ltr">inverse_log_det_jacobian</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L871-L900">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
inverse_log_det_jacobian(
    y, event_ndims, name='inverse_log_det_jacobian'
)
</pre> <p>Returns the (log o det o Jacobian o inverse)(y).</p> <p>Mathematically, returns: <code translate="no" dir="ltr">log(det(dX/dY))(Y)</code>. (Recall that: <code translate="no" dir="ltr">X=g^{-1}(Y)</code>.)</p> <p>Note that <code translate="no" dir="ltr">forward_log_det_jacobian</code> is the negative of this function, evaluated at <code translate="no" dir="ltr">g^{-1}(y)</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">y</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>. The input to the "inverse" Jacobian determinant evaluation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">event_ndims</code> </td> <td> Number of dimensions in the probabilistic events being transformed. Must be greater than or equal to <code translate="no" dir="ltr">self.inverse_min_event_ndims</code>. The result is summed over the final dimensions to produce a scalar Jacobian determinant for each event, i.e. it has shape <code translate="no" dir="ltr">y.shape.ndims - event_ndims</code> dimensions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name to give this op. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">Tensor</code>, if this bijector is injective. If not injective, returns the tuple of local log det Jacobians, <code translate="no" dir="ltr">log(det(Dg_i^{-1}(y)))</code>, where <code translate="no" dir="ltr">g_i</code> is the restriction of <code translate="no" dir="ltr">g</code> to the <code translate="no" dir="ltr">ith</code> partition <code translate="no" dir="ltr">Di</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">self.dtype</code> is specified and <code translate="no" dir="ltr">y.dtype</code> is not <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> if <code translate="no" dir="ltr">_inverse_log_det_jacobian</code> is not implemented. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    © 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/distributions/bijectors/RealNVP" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/distributions/bijectors/RealNVP</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
