
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.contrib.opt.ElasticAverageOptimizer - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" Wrapper optimizer that implements the Elastic Average SGD algorithm. ">
  <meta name="keywords" content="tf, contrib, opt, elasticaverageoptimizer, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/contrib/opt/elasticaverageoptimizer.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.contrib.opt.ElasticAverageOptimizer</h1>       <p>Wrapper optimizer that implements the Elastic Average SGD algorithm.</p> <p>Inherits From: <a href="../../train/optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.contrib.opt.ElasticAverageOptimizer(
    opt, num_worker, ea_custom_getter, communication_period=10, moving_rate=None,
    rho=None, use_locking=True, synchronous=False, name='ElasticAverageOptimizer'
)
</pre>  <p>This is an async optimizer. During the training, Each worker will update the local variables and maintains its own local_step, which starts from 0 and is incremented by 1 after each update of local variables. Whenever the communication period divides the local step, the worker requests the current global center variables and then computed the elastic difference between global center variables and local variables. The elastic difference then be used to update both local variables and global variables.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">opt</code> </td> <td> The actual optimizer that will be used to update local variables. Must be one of the Optimizer classes. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_worker</code> </td> <td> The number of workers </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ea_custom_getter</code> </td> <td> The ElasticAverageCustomGetter </td> </tr>
<tr> <td> <code translate="no" dir="ltr">communication_period</code> </td> <td> An int point value to controls the frequency of the communication between every worker and the ps. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">moving_rate</code> </td> <td> A floating point value to control the elastic difference. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">rho</code> </td> <td> the amount of exploration we allow in the model. The default value is moving_rate/learning_rate rho=0.0 is suggested in async mode. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_locking</code> </td> <td> If True use locks for update operations. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">synchronous</code> </td> <td> Add_sync_queues_and_barrier or not. True: all workers will wait for each other before start training False: worker can start training when its initilization is done, no need to wait for everyone is ready. in case one worker is restarted, it can join and continue training without being blocked. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name prefix for the operations created when applying gradients. Defaults to "ElasticAverageOptimizer". </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="apply_gradients" data-text="apply_gradients" tabindex="0"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/opt/python/training/elastic_average_optimizer.py#L268-L351">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
apply_gradients(
    grads_and_vars, global_step=None, name=None
)
</pre> <p>Apply gradients to global variables.</p> <p>This is the second part of <code translate="no" dir="ltr">minimize()</code>. It returns an <code translate="no" dir="ltr">Operation</code> that applies gradients.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">grads_and_vars</code> </td> <td> List of (gradient, variable) pairs as returned by <code translate="no" dir="ltr">compute_gradients()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. Default to the name passed to the <code translate="no" dir="ltr">Optimizer</code> constructor. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">Operation</code> that applies the specified gradients. If <code translate="no" dir="ltr">global_step</code> was not None, that operation also increments <code translate="no" dir="ltr">global_step</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">grads_and_vars</code> is malformed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If none of the variables have gradients. </td> </tr> </table> <h3 id="compute_gradients" data-text="compute_gradients" tabindex="0"><code translate="no" dir="ltr">compute_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/opt/python/training/elastic_average_optimizer.py#L214-L266">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compute_gradients(
    loss, var_list=None, gate_gradients=optimizer.Optimizer.GATE_OP,
    aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None
)
</pre> <p>Compute gradients of <code translate="no" dir="ltr">loss</code> for the variables in <code translate="no" dir="ltr">var_list</code>.</p> <p>Add rho*elastic_difference to loss to control the exploration This is the first part of <code translate="no" dir="ltr">minimize()</code>. It returns a list of (gradient, variable) pairs where "gradient" is the gradient for "variable". Note that "gradient" can be a <code translate="no" dir="ltr">Tensor</code>, an <code translate="no" dir="ltr">IndexedSlices</code>, or <code translate="no" dir="ltr">None</code> if there is no gradient for the given variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> A Tensor containing the value to minimize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> Optional list or tuple of <a href="../../variable"><code translate="no" dir="ltr">tf.Variable</code></a> to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKey.TRAINABLE_VARIABLES</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gate_gradients</code> </td> <td> How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation_method</code> </td> <td> Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">colocate_gradients_with_ops</code> </td> <td> If True, try colocating gradients with the corresponding op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">grad_loss</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of (gradient, variable) pairs. Variable is always present, but gradient can be <code translate="no" dir="ltr">None</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">var_list</code> contains anything else than <code translate="no" dir="ltr">Variable</code> objects. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If some arguments are invalid. </td> </tr> </table> <h3 id="get_init_op" data-text="get_init_op" tabindex="0"><code translate="no" dir="ltr">get_init_op</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/opt/python/training/elastic_average_optimizer.py#L353-L398">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_init_op(
    task_index
)
</pre> <p>Returns the op to let all the local variables and local center</p> <p>variables equal to the global center variables before the training begins</p> <h3 id="get_name" data-text="get_name" tabindex="0"><code translate="no" dir="ltr">get_name</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/optimizer.py#L352-L353">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_name()
</pre> <h3 id="get_slot" data-text="get_slot" tabindex="0"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/optimizer.py#L735-L771">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot(
    var, name
)
</pre> <p>Return a slot named <code translate="no" dir="ltr">name</code> created for <code translate="no" dir="ltr">var</code> by the Optimizer.</p> <p>Some <code translate="no" dir="ltr">Optimizer</code> subclasses use additional variables. For example <code translate="no" dir="ltr">Momentum</code> and <code translate="no" dir="ltr">Adagrad</code> use variables to accumulate updates. This method gives access to these <code translate="no" dir="ltr">Variable</code> objects if for some reason you need them.</p> <p>Use <code translate="no" dir="ltr">get_slot_names()</code> to get the list of slot names created by the <code translate="no" dir="ltr">Optimizer</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var</code> </td> <td> A variable passed to <code translate="no" dir="ltr">minimize()</code> or <code translate="no" dir="ltr">apply_gradients()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A string. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The <code translate="no" dir="ltr">Variable</code> for the slot if it was created, <code translate="no" dir="ltr">None</code> otherwise. </td> </tr> 
</table> <h3 id="get_slot_names" data-text="get_slot_names" tabindex="0"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/optimizer.py#L773-L781">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot_names()
</pre> <p>Return a list of the names of slots created by the <code translate="no" dir="ltr">Optimizer</code>.</p> <p>See <code translate="no" dir="ltr">get_slot()</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of strings. </td> </tr> 
</table> <h3 id="make_session_run_hook" data-text="make_session_run_hook" tabindex="0"><code translate="no" dir="ltr">make_session_run_hook</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/opt/python/training/elastic_average_optimizer.py#L400-L402">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
make_session_run_hook(
    is_chief, task_index
)
</pre> <p>Creates a hook to handle ElasticAverageOptimizerHook ops such as initialization.</p> <h3 id="minimize" data-text="minimize" tabindex="0"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/optimizer.py#L355-L413">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
minimize(
    loss, global_step=None, var_list=None, gate_gradients=GATE_OP,
    aggregation_method=None, colocate_gradients_with_ops=False, name=None,
    grad_loss=None
)
</pre> <p>Add operations to minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply combines calls <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying them call <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> containing the value to minimize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> Optional list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gate_gradients</code> </td> <td> How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation_method</code> </td> <td> Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">colocate_gradients_with_ops</code> </td> <td> If True, try colocating gradients with the corresponding op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">grad_loss</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An Operation that updates the variables in <code translate="no" dir="ltr">var_list</code>. If <code translate="no" dir="ltr">global_step</code> was not <code translate="no" dir="ltr">None</code>, that operation also increments <code translate="no" dir="ltr">global_step</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects. </td> </tr> </table> <h4 id="eager_compatibility" data-text="Eager Compatibility" tabindex="0">Eager Compatibility</h4> <p>When eager execution is enabled, <code translate="no" dir="ltr">loss</code> should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of <code translate="no" dir="ltr">var_list</code> if not None, else with respect to any trainable variables created during the execution of the <code translate="no" dir="ltr">loss</code> function. <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, <code translate="no" dir="ltr">colocate_gradients_with_ops</code> and <code translate="no" dir="ltr">grad_loss</code> are ignored when eager execution is enabled.</p> <h3 id="swapping_saver" data-text="swapping_saver" tabindex="0"><code translate="no" dir="ltr">swapping_saver</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/opt/python/training/elastic_average_optimizer.py#L404-L450">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
swapping_saver(
    var_list=None, name='swapping_saver', **kwargs
)
</pre> <p>Create a saver copy global_center_variable to trainable variables</p> <p>Please call this function after all your variables created with ElasticAverageCustomGetter. For evaluations or inference, use this saver during training. It will save the global_center_variable of the trained parameters under the original parameter names. Args: var_list: List of variables to save, as per <code translate="no" dir="ltr">Saver()</code>. If set to None, save all the trainable_variables that have been created before this call. name: The name of the saver. **kwargs: Keyword arguments of <code translate="no" dir="ltr">Saver()</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <a href="../../train/saver"><code translate="no" dir="ltr">tf.compat.v1.train.Saver</code></a> object. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> global_center_variable is empty, please make sure this is called after model created and ElasticAverageCustomGetter is used when declaring you model </td> </tr> </table> <h3 id="variables" data-text="variables" tabindex="0"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/optimizer.py#L783-L809">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
variables()
</pre> <p>A list of variables which encode the current state of <code translate="no" dir="ltr">Optimizer</code>.</p> <p>Includes slot variables and additional global variables created by the optimizer in the current default graph.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of variables. </td> </tr> 
</table> <h2 id="class_variables" data-text="Class Variables" tabindex="0">Class Variables</h2> <ul> <li>
<code translate="no" dir="ltr">BETA = 0.9</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_GRAPH = 2</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_NONE = 0</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_OP = 1</code> 
</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    © 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/opt/ElasticAverageOptimizer" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/opt/ElasticAverageOptimizer</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
