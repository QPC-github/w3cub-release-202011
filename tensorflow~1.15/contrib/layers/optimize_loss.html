
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.contrib.layers.optimize_loss - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" Given loss and parameters for optimizer, returns a training op. ">
  <meta name="keywords" content="tf, contrib, layers, optimize, loss, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/contrib/layers/optimize_loss.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.contrib.layers.optimize_loss</h1>       <p>Given loss and parameters for optimizer, returns a training op.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.contrib.layers.optimize_loss(
    loss, global_step, learning_rate, optimizer, gradient_noise_scale=None,
    gradient_multipliers=None, clip_gradients=None, learning_rate_decay_fn=None,
    update_ops=None, variables=None, name=None, summaries=None,
    colocate_gradients_with_ops=False, increment_global_step=True
)
</pre>  <p>Various ways of passing optimizers include:</p> <ul> <li>by string specifying the name of the optimizer. See OPTIMIZER_CLS_NAMES for full list. E.g. <code translate="no" dir="ltr">optimize_loss(..., optimizer='Adam')</code>.</li> <li>by function taking learning rate <code translate="no" dir="ltr">Tensor</code> as argument and returning an <code translate="no" dir="ltr">Optimizer</code> instance. E.g. <code translate="no" dir="ltr">optimize_loss(..., optimizer=lambda lr: tf.compat.v1.train.MomentumOptimizer(lr, momentum=0.5))</code>. Alternatively, if <code translate="no" dir="ltr">learning_rate</code> is <code translate="no" dir="ltr">None</code>, the function takes no arguments. E.g. <code translate="no" dir="ltr">optimize_loss(..., learning_rate=None, optimizer=lambda: tf.compat.v1.train.MomentumOptimizer(0.5, momentum=0.5))</code>.</li> <li>by a subclass of <code translate="no" dir="ltr">Optimizer</code> having a single-argument constructor (the argument is the learning rate), such as AdamOptimizer or AdagradOptimizer. E.g. <code translate="no" dir="ltr">optimize_loss(..., optimizer=tf.compat.v1.train.AdagradOptimizer)</code>.</li> <li>by an instance of a subclass of <code translate="no" dir="ltr">Optimizer</code>. E.g., <code translate="no" dir="ltr">optimize_loss(..., optimizer=tf.compat.v1.train.AdagradOptimizer(0.5))</code>.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> Scalar <code translate="no" dir="ltr">Tensor</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Scalar int <code translate="no" dir="ltr">Tensor</code>, step counter to update on each step unless <code translate="no" dir="ltr">increment_global_step</code> is <code translate="no" dir="ltr">False</code>. If not supplied, it will be fetched from the default graph (see <a href="../../train/get_global_step"><code translate="no" dir="ltr">tf.compat.v1.train.get_global_step</code></a> for details). If it has not been created, no step will be incremented with each weight update. <code translate="no" dir="ltr">learning_rate_decay_fn</code> requires <code translate="no" dir="ltr">global_step</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> float or <code translate="no" dir="ltr">Tensor</code>, magnitude of update per each training step. Can be <code translate="no" dir="ltr">None</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">optimizer</code> </td> <td> string, class or optimizer instance, used as trainer. string should be name of optimizer, like 'SGD', 'Adam', 'Adagrad'. Full list in OPTIMIZER_CLS_NAMES constant. class should be sub-class of <code translate="no" dir="ltr">tf.Optimizer</code> that implements <code translate="no" dir="ltr">compute_gradients</code> and <code translate="no" dir="ltr">apply_gradients</code> functions. optimizer instance should be instantiation of <code translate="no" dir="ltr">tf.Optimizer</code> sub-class and have <code translate="no" dir="ltr">compute_gradients</code> and <code translate="no" dir="ltr">apply_gradients</code> functions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gradient_noise_scale</code> </td> <td> float or None, adds 0-mean normal noise scaled by this value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gradient_multipliers</code> </td> <td> dict of variables or variable names to floats. If present, gradients for specified variables will be multiplied by given constant. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clip_gradients</code> </td> <td> float, callable or <code translate="no" dir="ltr">None</code>. If a float is provided, a global clipping is applied to prevent the norm of the gradient from exceeding this value. Alternatively, a callable can be provided, e.g., <code translate="no" dir="ltr">adaptive_clipping_fn()</code>. This callable takes a list of <code translate="no" dir="ltr">(gradients, variables)</code> tuples and returns the same thing with the gradients modified. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">learning_rate_decay_fn</code> </td> <td> function, takes <code translate="no" dir="ltr">learning_rate</code> and <code translate="no" dir="ltr">global_step</code> <code translate="no" dir="ltr">Tensor</code>s, returns <code translate="no" dir="ltr">Tensor</code>. Can be used to implement any learning rate decay functions. For example: <a href="../../train/exponential_decay"><code translate="no" dir="ltr">tf.compat.v1.train.exponential_decay</code></a>. Ignored if <code translate="no" dir="ltr">learning_rate</code> is not supplied. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">update_ops</code> </td> <td> list of update <code translate="no" dir="ltr">Operation</code>s to execute at each step. If <code translate="no" dir="ltr">None</code>, uses elements of UPDATE_OPS collection. The order of execution between <code translate="no" dir="ltr">update_ops</code> and <code translate="no" dir="ltr">loss</code> is non-deterministic. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">variables</code> </td> <td> list of variables to optimize or <code translate="no" dir="ltr">None</code> to use all trainable variables. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name for this operation is used to scope operations and summaries. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">summaries</code> </td> <td> List of internal quantities to visualize on tensorboard. If not set, the loss, the learning rate, and the global norm of the gradients will be reported. The complete list of possible values is in OPTIMIZER_SUMMARIES. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">colocate_gradients_with_ops</code> </td> <td> If True, try colocating gradients with the corresponding op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">increment_global_step</code> </td> <td> Whether to increment <code translate="no" dir="ltr">global_step</code>. If your model calls <code translate="no" dir="ltr">optimize_loss</code> multiple times per training step (e.g. to optimize different parts of the model), use this arg to avoid incrementing <code translate="no" dir="ltr">global_step</code> more times than necessary. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Training op. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if: <ul> <li>
<code translate="no" dir="ltr">loss</code> is an invalid type or shape.</li> <li>
<code translate="no" dir="ltr">global_step</code> is an invalid type or shape.</li> <li>
<code translate="no" dir="ltr">learning_rate</code> is an invalid type or value.</li> <li>
<code translate="no" dir="ltr">optimizer</code> has the wrong type.</li> <li>
<code translate="no" dir="ltr">clip_gradients</code> is neither float nor callable.</li> <li>
<code translate="no" dir="ltr">learning_rate</code> and <code translate="no" dir="ltr">learning_rate_decay_fn</code> are supplied, but no <code translate="no" dir="ltr">global_step</code> is available.</li> <li>
<code translate="no" dir="ltr">gradients</code> is empty. </li>
</ul>
</td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    © 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/layers/optimize_loss" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/layers/optimize_loss</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
