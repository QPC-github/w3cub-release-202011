
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.contrib.training.batch_sequences_with_states - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" Creates batches of segments of sequential input. ">
  <meta name="keywords" content="tf, contrib, training, batch, sequences, with, states, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/contrib/training/batch_sequences_with_states.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.contrib.training.batch_sequences_with_states</h1>       <p>Creates batches of segments of sequential input.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.contrib.training.batch_sequences_with_states(
    input_key, input_sequences, input_context, input_length, initial_states,
    num_unroll, batch_size, num_threads=3, capacity=1000, allow_small_batch=True,
    pad=True, make_keys_unique=False, make_keys_unique_seed=None, name=None
)
</pre>  <p>This method creates a <code translate="no" dir="ltr">SequenceQueueingStateSaver</code> (SQSS) and adds it to the queuerunners. It returns a <code translate="no" dir="ltr">NextQueuedSequenceBatch</code>.</p> <p>It accepts one example at a time identified by a unique <code translate="no" dir="ltr">input_key</code>. <code translate="no" dir="ltr">input_sequence</code> is a dict with values that are tensors with time as first dimension. This time dimension must be the same across those tensors of an example. It can vary across examples. Although it always has to be a multiple of <code translate="no" dir="ltr">num_unroll</code>. Hence, padding may be necessary and it is turned on by default by <code translate="no" dir="ltr">pad=True</code>.</p> <p><code translate="no" dir="ltr">input_length</code> is a Tensor scalar or an int recording the time dimension prior to padding. It should be between 0 and the time dimension. One reason we want to keep track of it is so that we can take it into consideration when computing the loss. If <code translate="no" dir="ltr">pad=True</code> then <code translate="no" dir="ltr">input_length</code> can be <code translate="no" dir="ltr">None</code> and will be inferred.</p> <p>This methods segments <code translate="no" dir="ltr">input_sequence</code> into segments of length <code translate="no" dir="ltr">num_unroll</code>. It batches input sequences from <code translate="no" dir="ltr">batch_size</code> many examples. These mini-batches are available through the <code translate="no" dir="ltr">sequence</code> property of the output. Moreover, for each entry in the batch we can access its original <code translate="no" dir="ltr">input_key</code> in <code translate="no" dir="ltr">key</code> and its input length in <code translate="no" dir="ltr">total_length</code>. <code translate="no" dir="ltr">length</code> records within this segment how many non-padded time steps there are.</p> <p>Static features of an example that do not vary across time can be part of the <code translate="no" dir="ltr">input_context</code>, a dict with Tensor values. This method copies the context for each segment and makes it available in the <code translate="no" dir="ltr">context</code> of the output.</p> <p>This method can maintain and update a state for each example. It accepts some initial_states as a dict with Tensor values. The first mini-batch an example is contained has initial_states as entry of the <code translate="no" dir="ltr">state</code>. If save_state is called then the next segment will have the updated entry of the <code translate="no" dir="ltr">state</code>. See <code translate="no" dir="ltr">NextQueuedSequenceBatch</code> for a complete list of properties and methods.</p> <h4 id="example_usage" data-text="Example usage:" tabindex="0">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">batch_size = 32
num_unroll = 20
num_enqueue_threads = 3
lstm_size = 8
cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units=lstm_size)

key, sequences, context = my_parser(raw_data)
initial_state_values = tf.zeros((state_size,), dtype=tf.float32)
initial_states = {"lstm_state": initial_state_values}
batch = tf.batch_sequences_with_states(
    input_key=key,
    input_sequences=sequences,
    input_context=context,
    input_length=tf.shape(sequences["input"])[0],
    initial_states=initial_states,
    num_unroll=num_unroll,
    batch_size=batch_size,
    num_threads=num_enqueue_threads,
    capacity=batch_size * num_enqueue_threads * 2)

inputs = batch.sequences["input"]
context_label = batch.context["label"]

inputs_by_time = tf.split(value=inputs, num_or_size_splits=num_unroll, axis=1)
assert len(inputs_by_time) == num_unroll

lstm_output, _ = tf.contrib.rnn.static_state_saving_rnn(
  cell,
  inputs_by_time,
  state_saver=batch,
  state_name="lstm_state")

# Start a prefetcher in the background
sess = tf.compat.v1.Session()

tf.compat.v1.train.start_queue_runners(sess=session)

while True:
  # Step through batches, perform training or inference...
  session.run([lstm_output])
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_key</code> </td> <td> A string scalar <code translate="no" dir="ltr">Tensor</code>, the <strong>unique</strong> key for the given input example. This is used to keep track of the split minibatch elements of this input. Batched keys of the current iteration are made accessible via the <code translate="no" dir="ltr">key</code> property. The shape of <code translate="no" dir="ltr">input_key</code> (scalar) must be fully specified. Consider setting <code translate="no" dir="ltr">make_keys_unique</code> to True when iterating over the same input multiple times. <blockquote class="note">
<strong>Note:</strong><span> if <code translate="no" dir="ltr">make_keys_unique=False</code> then <code translate="no" dir="ltr">input_key</code>s must be unique. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_sequences</code> </td> <td> A dict mapping string names to <code translate="no" dir="ltr">Tensor</code> values. The values must all have matching first dimension, called <code translate="no" dir="ltr">value_length</code>. They may vary from input to input. The remainder of the shape (other than the first dimension) must be fully specified. The <code translate="no" dir="ltr">SequenceQueueingStateSaver</code> will split these tensors along this first dimension into minibatch elements of dimension <code translate="no" dir="ltr">num_unrolled</code>. Batched and segmented sequences of the current iteration are made accessible via the <code translate="no" dir="ltr">sequences</code> property.<blockquote class="note">
<strong>Note:</strong><span> if <code translate="no" dir="ltr">pad=False</code>, then <code translate="no" dir="ltr">value_length</code> must always be a multiple of <code translate="no" dir="ltr">num_unroll</code>. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_context</code> </td> <td> A dict mapping string names to <code translate="no" dir="ltr">Tensor</code> values. The values are treated as "global" across all time splits of the given input example, and will be copied across for all minibatch elements accordingly. Batched and copied context of the current iteration are made accessible via the <code translate="no" dir="ltr">context</code> property.<blockquote class="note">
<strong>Note:</strong><span> All input_context values must have fully defined shapes. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_length</code> </td> <td> None or an int32 scalar <code translate="no" dir="ltr">Tensor</code>, the length of the sequence prior to padding. If <code translate="no" dir="ltr">input_length=None</code> and <code translate="no" dir="ltr">pad=True</code> then the length will be inferred and will be equal to <code translate="no" dir="ltr">value_length</code>. If <code translate="no" dir="ltr">pad=False</code> then <code translate="no" dir="ltr">input_length</code> cannot be <code translate="no" dir="ltr">None</code>: <code translate="no" dir="ltr">input_length</code> must be specified. Its shape of <code translate="no" dir="ltr">input_length</code> (scalar) must be fully specified. Its value may be at most <code translate="no" dir="ltr">value_length</code> for any given input (see above for the definition of <code translate="no" dir="ltr">value_length</code>). Batched and total lengths of the current iteration are made accessible via the <code translate="no" dir="ltr">length</code> and <code translate="no" dir="ltr">total_length</code> properties. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initial_states</code> </td> <td> A dict mapping string state names to multi-dimensional values (e.g. constants or tensors). This input defines the set of states that will be kept track of during computing iterations, and which can be accessed via the <code translate="no" dir="ltr">state</code> and <code translate="no" dir="ltr">save_state</code> methods.<blockquote class="note">
<strong>Note:</strong><span> All initial_state values must have fully defined shapes. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_unroll</code> </td> <td> Python integer, how many time steps to unroll at a time. The input sequences of length k are then split into k / num_unroll many segments. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">batch_size</code> </td> <td> int or int32 scalar <code translate="no" dir="ltr">Tensor</code>, how large minibatches should be when accessing the <code translate="no" dir="ltr">state()</code> method and <code translate="no" dir="ltr">context</code>, <code translate="no" dir="ltr">sequences</code>, etc, properties. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_threads</code> </td> <td> The int number of threads enqueuing input examples into a queue. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">capacity</code> </td> <td> The max capacity of the queue in number of examples. Needs to be at least <code translate="no" dir="ltr">batch_size</code>. Defaults to 1000. When iterating over the same input example multiple times reusing their keys the <code translate="no" dir="ltr">capacity</code> must be smaller than the number of examples. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">allow_small_batch</code> </td> <td> If true, the queue will return smaller batches when there aren't enough input examples to fill a whole batch and the end of the input has been reached. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">pad</code> </td> <td> If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">input_sequences</code> will be padded to multiple of <code translate="no" dir="ltr">num_unroll</code>. In that case <code translate="no" dir="ltr">input_length</code> may be <code translate="no" dir="ltr">None</code> and is assumed to be the length of first dimension of values in <code translate="no" dir="ltr">input_sequences</code> (i.e. <code translate="no" dir="ltr">value_length</code>). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">make_keys_unique</code> </td> <td> Whether to append a random integer to the <code translate="no" dir="ltr">input_key</code> in an effort to make it unique. The seed can be set via <code translate="no" dir="ltr">make_keys_unique_seed</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">make_keys_unique_seed</code> </td> <td> If <code translate="no" dir="ltr">make_keys_unique=True</code> this fixes the seed with which a random postfix is generated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> An op name string (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A NextQueuedSequenceBatch with segmented and batched inputs and their states. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if any of the inputs is not an expected type. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if any of the input values is inconsistent, e.g. if not enough shape information is available from inputs to build the state saver. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    © 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/training/batch_sequences_with_states" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/training/batch_sequences_with_states</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
