
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.compat.v1.tpu.batch_parallel - TensorFlow 2.9 - W3cubDocs</title>
  
  <meta name="description" content=" Shards computation along the batch dimension for parallel execution. ">
  <meta name="keywords" content="tf, compat, tpu, batch, parallel, tensorflow, tensorflow~2.9">
  <meta name="HandheldFriendly" content="True">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~2.9/compat/v1/tpu/batch_parallel.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-f16eecbe420d8b2925d31ffbb21d05646497ecbd9515f08ffe69e9bba7332f5657accc7003c7f6c72cb4a132171acf171b359ae3bae4ae5660ddfb1718f88c67.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/tensorflow~2.9.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~2.9/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 2.9</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.compat.v1.tpu.batch_parallel</h1> <devsite-bookmark></devsite-bookmark>       <p>Shards <code translate="no" dir="ltr">computation</code> along the batch dimension for parallel execution.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.tpu.batch_parallel(
    computation: Callable[..., Any],
    inputs: Optional[List[List[Optional[core_types.Tensor]]]] = None,
    num_shards: int = 1,
    infeed_queue: Optional[tpu_feed.InfeedQueue] = None,
    device_assignment: Optional[tf.tpu.experimental.DeviceAssignment] = None,
    name: Optional[Text] = None,
    xla_options: Optional[tf.tpu.XLAOptions] = None
)
</pre>  <p>Convenience wrapper around shard().</p> <p><code translate="no" dir="ltr">inputs</code> must be a list of Tensors or None (equivalent to an empty list). Each input is split into <code translate="no" dir="ltr">num_shards</code> pieces along the 0-th dimension, and computation is applied to each shard in parallel.</p> <p>Tensors are broadcast to all shards if they are lexically captured by <code translate="no" dir="ltr">computation</code>. e.g.,</p> <p>x = tf.constant(7) def computation(): return x + 3 ... = shard(computation, ...)</p> <p>The outputs from all shards are concatenated back together along their 0-th dimension.</p> <p>Inputs and outputs of the computation must be at least rank-1 Tensors.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">computation</code> </td> <td> A Python function that builds a computation to apply to each shard of the input. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">inputs</code> </td> <td> A list of input tensors or None (equivalent to an empty list). The 0-th dimension of each Tensor must have size divisible by <code translate="no" dir="ltr">num_shards</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_shards</code> </td> <td> The number of shards. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">infeed_queue</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, the <code translate="no" dir="ltr">InfeedQueue</code> from which to append a tuple of arguments as inputs to <code translate="no" dir="ltr">computation</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">device_assignment</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, a <code translate="no" dir="ltr">DeviceAssignment</code> describing the mapping between logical cores in the computation with physical cores in the TPU topology. Uses a default device assignment if <code translate="no" dir="ltr">None</code>. The <code translate="no" dir="ltr">DeviceAssignment</code> may be omitted if each shard of the computation uses only one core, and there is either only one shard, or the number of shards is equal to the number of cores in the TPU system. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> (Deprecated) Does nothing. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">xla_options</code> </td> <td> An instance of <a href="https://www.tensorflow.org/api_docs/python/tf/tpu/XLAOptions"><code translate="no" dir="ltr">tpu.XLAOptions</code></a> which indicates the options passed to XLA compiler. Use <code translate="no" dir="ltr">None</code> for default options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of output tensors. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If <code translate="no" dir="ltr">num_shards &lt;= 0</code> </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/tpu/batch_parallel" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/tpu/batch_parallel</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
